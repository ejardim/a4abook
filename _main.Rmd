--- 
title: "Fish stock assessment with R"
subtitle: "The a4a Initiative"
author: "Ernesto Jardim, Colin Millar, Danai Mantopoulou Palouka and Iago Mosqueira"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
url: your book url like https://ejardim.github.io/a4abook/
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  How to do stock assessment in R using the a4a framework
link-citations: yes
github-repo: ejardim/a4abook
output:
    split_bib: false
    pdf_document:
        extra_dependencies: ["a4a"]
---

# Before starting

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(formatR)
#thm = knit_theme$get("bclear") #moe, bclear
#knit_theme$set(thm)
opts_chunk$set(dev='png', dev.args=list(type="cairo"), warning=FALSE)
# lattice theme
library(lattice)
lattice.options(default.args = list(as.table = TRUE))
```

## Installing and loading libraries

To run the methods in this book the reader will need to install the `FLa4a` package [@fla4a] and its dependencies. Some datasets are distributed with the package and as such need to be loaded too.

```{R, eval=FALSE}
# from CRAN
install.packages(c("copula","triangle", "coda", "grid", "gridExtra", "latticeExtra"))
# from FLR
install.packages(c("FLCore", "FLa4a"), repos="http://flr-project.org/R")
```

```{R, message=FALSE, warning=FALSE}
# libraries
library(devtools)
library(FLa4a)
library(XML)
library(reshape2)
library(ggplotFL)
# datasets
data(ple4)
data(ple4.indices)
data(ple4.index)
data(rfLen)
```

```{R}
packageVersion("FLCore")
packageVersion("FLa4a")
```

## How to read this document

The target audience for this document are readers with some experience in R and some background on stock assessment.

The document explains the approach being developed by the Assessment for All Initiative (a4a) for fish stock assessment and scientific advice. It presents a mixture of text and code, where the first explains the concepts behind the methods, while the last shows how these can be run with the software provided. Moreover, having the code allows the reader to copy/paste and replicate the analysis presented here.

The sections and subsections are as independent as possible, so they can be used as a reference document for the `FLa4a`.

## How to get help 

`FLa4a` is build using `R`'s object oriented implementation with S4 classes, and `FLCore`'s [@flr] class structures and methods. In order to access S4 methods and classes documentation the user needs to use specific terminology.

For example, `FLStock` is one of our main components in order to run our stock assessment model. We can check the structure of an `FLStock` object as follows:

```{r}
showClass("FLStock")
```

The object oriented structure of `FLa4a` gives the opportunity to change the behavior of a function according to the object that is applied to. For example we can check the available methods of the function `plot`

```{r}
showMethods("plot")
```

by calling `showMethods` R prints all the possible uses of the `plot` function. We want to see what it does when it is called on an `FLStock` object with no other object. We observe that `plot` takes two arguments, `x` and `y`. So, in the signature of the `getMethod` function we are going to use, we need to define both `x` and `y`.

```{r}
getMethod('plot', signature = list("FLStock","missing"))
```

More information can be found in `R`'s documentation (https://www.r-project.org/).

## Notation

Along this chapter the notation presented in Table \@ref(tab:mathsnotation) will be used. Mathematical descriptions will be kept as simple as possible for readability.

|Type|Symbol|Description| 
|:---|-----:|:----------|
|variables| | |
| |$C$ | catches|
| |$F$ | fishing mortality |
| |$M$ | natural mortality |
| |$R$ | recruitment |
| |$Q$ | vessel or fleet catchability |
| |$w$ | weights |
| |$l$ | likelihood |
| |$I$ | abundance index |
| |$S$ | spawning stock biomass |
| |$CV$ | coefficient of variation |
| |$D$ | residuals or deviances |
| |$N$ | population numbers |
| |$\beta, \gamma$ | parameters |
| |$a$ | stock-recruitment parameter |
| |$b$ | stock-recruitment parameter |
| |$\sigma^2$ | variance of catch |
| |$\tau^2$ | variance of index |
| |$\phi^2$ | variance of predicted recruitment |
| |$\upsilon^2$ | variance of residuals |
|subscripts | | |
| |$a$ | age |
| |$y$ | year |
| |$C$ | catch |
| |$I$ | abundance index |
| |$N$ | normal distribution |
| |$s$ | survey |
| |$SR$ | stock recruitment relationship |
|superscripts and accents | | |
| |$\hat{}$ | observation |
| |$\tilde{}$ | prediction |
| |$c$ | catches |
| |$s$ | abundance index |

Table: (\#tab:mathsnotation) Mathematical notation

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(.packages(), 'bookdown', 'knitr', 'rmarkdown'), 'packages.bib')
```

## Acknowledgements

To be complete with version 1.0.

To write this book we used AI agents (chatGPT) for some bibliographic research and grammar revisions.

## License

This book is released under a Creative Commons license [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/).

The `FLa4a` package is released under the [EUPL 1.1](https://joinup.ec.europa.eu/community/eupl/home).




<!--chapter:end:index.Rmd-->

# Introduction

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
opts_chunk$set(dev='png', dev.args=list(type="cairo"), warning=FALSE)
library(lattice)
lattice.options(default.args = list(as.table = TRUE))
```

## The "Assessment for All" Initiative (a4a)

The European Commission Joint Research Centre's (JRC) "Assessment for All" Initiative (a4a) was launched to simplify and standardize the complex methodologies often employed in fisheries science, a4a focuses on creating flexible, modular frameworks that can accommodate varying data availability, regional needs, and stakeholder objectives.

The JRC started its ‘Assessment for All’ Initiative (a4a), with the aim to develop, test, and distribute methods to assess a large numbers of stocks in an operational time frame, and to build the necessary expertise on stock assessment and advice provision.

According to @EJ_etal_2014, the long-term strategy of a4a is to increase the number of stock assessments by reducing the workload required to run each analysis and by bringing more scientists into fisheries management advice. The first is achieved by developing a working framework with the methods required to run all the analyses a stock assessment needs. Such approach should make the model exploration and selection processes easier, as well as decreasing the burden of moving between software platforms. The second can be achieved by making the analysis more intuitive, thereby attracting more experts to join stock assessment teams.

One major step to achieve the a4a goals was the development of a stock assessment model that could be applied rapidly to a large number of stocks and for a wide range of applications: traditional stock assessment, conditioning of operating models, forecasting, or informing harvest control rules in MSE algorithms.

The modular nature of a4a allows for the integration of data from diverse sources, including biological, environmental, and socioeconomic datasets, ensuring comprehensive assessments. This inclusivity enhances the ability to predict stock dynamics and evaluate the impacts of fishing and environmental changes.

While a4a simplifies traditional assessment approaches, it faces challenges such as ensuring the quality and consistency of input data, especially in regions with limited monitoring infrastructure. To address this, the initiative incorporates uncertainty into its models, leveraging MCMC frameworks and other statistical tools to account for variability in data quality and ecosystem processes.

The a4a framework has been applied in various European fisheries to improve stock assessment practices, only in the Mediterranean Sea has been used for more than 200 stock assessments, as of 2024 in [(GFCM)](https://www.fao.org/gfcm/data/star/en/) and [(STECF)](https://stecf.ec.europa.eu/data-dissemination/medbs_en). Moreover, its use in small pelagic fisheries demonstrated the utility of simple linear models in capturing key population dynamics without the need for data-intensive methods [@EJ_etal_2014].

Some of the key elements of stock assessment are the quantity, quality and aggregation level of the data available. As in many other models the data will condition the type of models that can be used. In `a4a` the minimum set of data, loosely defined as a "moderate data" level, consists of:

- volume of catches in weight (which should include landings and discards);
- length structure of the catches (based on selectivity studies or direct observations);
- natural mortality by length;
- proportion of mature individuals by length;
- age-length key or growth model;
- length-weight relationship;
- index of abundance and its length structure, or index of biomass (the type of index is left open, it could be from a scientifc survey or a commercial CPUE series);

## Multi-stage modelling approach

In ecological and population dynamics modeling, one can choose between integrated models, which estimate correlated parameters together, and two-stage models, which separate estimation into distinct steps. These approaches differ in complexity, data requirements, interpretability, and their ability to address uncertainties. The selection depends largely on the study objectives, available data, and the system's ecological complexity.

Integrated models estimate all parameters within a unified framework, accounting for correlations and interactions between variables such as growth, natural mortality, recruitment, and environmental factors. This approach can provide a realistic depiction of biological systems by preserving dependencies and feedback loops, which are crucial for understanding processes like density dependence or predator-prey interactions [@MCAPAM2023]. Integrated models are particularly advantageous for ecosystem-based management, where interactions among multiple factors need to be captured. However, the complexity of these models makes them computationally intensive and sensitive to data quality.

On the other hand, two-stage models estimate parameters such as growth or natural mortality independently before incorporating them into broader models. This step-wise approach simplifies estimation, reducing computational demands and mitigating issues like parameter confounding. For example, fisheries often use empirical relationships to estimate natural mortality (M) based on growth parameters or life history traits before including M in stock assessment models [@Maceina_etal_2016]. However, this decoupling may overlook dynamic interactions, such as how growth influences mortality, potentially leading to biased or incomplete inferences about ecosystem dynamics [@Jacobsen_etal_2018].

Dealing with uncertainty is a critical aspect of both approaches. Integrated models explicitly quantify and propagate uncertainties across correlated parameters. These models incorporate multiple sources of variability, including observation, process, and structural uncertainties, enhancing the robustness of predictions [@López_etal_2017]. Conversely, two-stage models often treat parameter estimates as fixed values, which can underestimate uncertainty propagation in subsequent analyses. However, by treating first-stage estimates as distributions rather than point estimates, two-stage models can partially address this limitation.

For fisheries science, the choice between these models often depends on management goals and data availability. Integrated models are better suited for forecasting fish abundance or evaluating complex ecological interactions, such as predator-prey dynamics or responses to environmental variability [@Robertson_etal_2022]. Meanwhile, two-stage models are practical for stock assessments, where simplicity and interpretability are prioritized.

Two-stage models are advantageous for practical applications, such as fisheries stock assessments, where simplicity and robustness take precedence over ecological nuance. Empirical estimates of \(M\), derived from life-history traits, provide reliable inputs for subsequent models, avoiding the parameter confounding that often occurs in integrated frameworks.

Despite the intuitive advantages of integrated models, it is not a panacea for poor quality data or model structure uncertainty in stock assessments. There are several disadvantages, mostly related to model misspecification, the complexity of the resulting models, and the associated, often considerable, computational requirements (e.g. the use of remotely sensed environmental information). Consequently, in some situations, the traditional two-stage approach remains a better approach [@maunderPunt2013].

## Stock Assessment Process

The following table breaks down the stock assessment process into three stages: (i) input data preparation, (ii) stock assessment model fitting, and (iii) provision of scientific advice. This breakdown is designed to explain the a4a approach, offering a general framework that outlines the sequence of analyses in the stock assessment process. Each stage includes a plethora of analyses and procedures tailored to the specific stock, considering the available data, time, and resources.

| Stage | Description |
|:----|:----|
| **Input data preparation** | Preparation of catch data, potentially separating landings and discards. Preparation of biological data, including maturity, length-weight relationships, natural mortality, and individual growth. Conversion of length data into age data. |
| **Stock assessment model fit** | Fitting the model to data, inspecting diagnostics such as residuals, retrospective analyses, and hindcasts. Fitting the stock-recruitment model to recruitment and spawning stock biomass (SSB) estimates from the stock assessment model fit or within the model itself. |
| **Scientific advice provision** | Estimation of reference points. Assessment of stock status based on reference points and model estimates of SSB and fishing mortality. Running projections under different scenarios. Providing reports with policy outcome evaluations. |
Table: (\#tab:sastages) Stock assessment process stages

The main purpose of the above table is to clarify a4a's multi-stage approach to stock assessment. For instance, parameters like natural mortality and individual growth are estimated outside the stock assessment model fitting process, unlike integrated analyses. This is done for reasons discussed earlier. Nevertheless, the stock-recruitment relationship can be estimated within the model, as is typical in integrated analyses. This mixed approach seeks to exclude highly correlated processes from the model while incorporating those that can enhance the robustness of the stock assessment model fit.

When data are limited, the stock assessment model requires more structure, but this constraint reduces the information available to manage parameter correlations. Striking this balance is challenging. The a4a approach incorporates stock-recruitment relationships, as these are believed to exhibit lower correlation with other model parameters compared to natural mortality or individual growth.

On the other hand, since natural mortality and individual growth are very important processes acting at a very low level in terms of data processing, there are specific methods to deal with conditioning those processes, in case there's no data or limited data available, and to provide the opportuinity to propagate their uncertainty into stock assessment.

## Stock assessment as a linear model

The submodels formulation uses linear models, which opens the possibility of using the linear modelling tools available in `R`. For example, `mgcv` [@R-mgcv] gam formulas or factorial design formulas using `lm()`.

The 'language' of linear models has been developing within the statistical community for many years, and constitutes an elegant way of defining models without going through the complexity of mathematical representations. This approach makes it also easier to communicate among scientists:

- [@Nelder_1965], notation for randomized block design
- [@Wilkinson_etal_1973], symbolic description for factorial designs
- [@hastie1990generalized], introduced notation for smoothers
- [@chambers1991statistical], further developed for use in S

## Data used in the book

### Plaice in area FAO 27, ICES area IV

[ToDo]

### European hake in FAO 37, GSAs 1,5,6,7

European hake is an important demersal species targeted by Mediterranean fishing fleets in several regions, mainly by bottom trawling, with In GSA 5 (Balearic Islands), bottom trawlers use different fishing tactics depending on the depth, with hake being targeted mainly on the deep shelf and the upper slope. In GSA 6, the fishery is also dominated by trawlers, with a large fleet operating on the shelf and slope and showing relatively stable catches. In GSA 7 (Gulf of Lions), hake is targeted by both French and Spanish vessels using a variety of gear types, including trawlers, gillnets and longlines.

### Red mullet in FAO 27, GSA 1

Red mullets is a key commercial species in GSA 1.They can be found in sandy and muddy areas, and most are caught by bottom trawlers. Small scale fisheries also catch some using nets. The amount of discards reported is very low and considered to be negligible. Trawl fisheries developed along the continental shelf and upper slope. Smaller vessels operate almost exclusively on the continental shelf. Red mullet is intensively exploited during its recruitment from September to November

### Redfish simulated length data

This is a length based dataset simulated with GADGET [@gadget].

<!--chapter:end:02-intro.Rmd-->

# Modelling Individual Growth and Using Stochastic Slicing to Convert Length-based Data Into Age-based Data {#growth}

```{R, message=FALSE, warning=FALSE, echo=FALSE}

# libraries
library(devtools)
library(FLa4a)
library(XML)
library(reshape2)
library(ggplotFL)
# datasets
data(ple4)
data(ple4.indices)
data(ple4.index)
data(rfLen)
```

The `a4a` stock assessment framework is based on age dynamics. Therefore, length information must be processed before running the stock assessment model. The methods in this section provide the analyst flexibility to use a large range of information sources, _e.g._ literature or online databases, to collect information about the species growth model and uncertainty about the model parameters.

The framework allows the analyst to parametrize individual growth, set the assumptions about it and condition the stock assessment model on those decisions. It incentivizes the uptake of estimation uncertainty, as well as exploring several parameterizations and/or growth models to deal with structural uncertainty, finally propagating uncertainty into stock assessment.

Within the `a4a` framework this is handled using the `a4aGr` class and its methods. This class stores information about the growth model and it's parameters, including parameters' uncertainty and the distributions governing it. The class's main method is `l2a` that converts length to ages based on a length based stock object and using the model defined in the `a4aGr` instance.

## a4aGr - The growth class

The conversion of length data to age is performed through the use of a growth model. The implementation is done through the `a4aGr` class .

```{r, show_a4aGr} 
showClass("a4aGr")
```

To construct an `a4aGr` object, the growth model and parameters must be provided. Here we show an example using the von Bertalanffy growth model. To create the `a4aGr` object it's necessary to pass the model equation ($length \sim time$), the inverse model equation ($time \sim length$) and the parameters. Any growth model can be used as long as it's possible to write the model (and the inverse) as an R formula.

```{r, a4aGr_vB_example} 
vbObj <- a4aGr(
	grMod=~linf*(1-exp(-k*(t-t0))),      
	grInvMod=~t0-1/k*log(1-len/linf),      
	params=FLPar(linf=58.5, k=0.086, t0=0.001, units=c("cm","year-1","year"))     
)
```

Check the model and its inverse:

```{r}
lc=20
predict(vbObj, len=lc)
predict(vbObj, t=predict(vbObj, len=lc))
```

The predict method allows the transformation between lengths and ages, and vice-versa, using the growth model.

```{r, predict_araGr_example} 
predict(vbObj, len=5:10+0.5)
predict(vbObj, t=5:10+0.5)
```

## Adding uncertainty to growth parameters with a multivariate normal distribution

Uncertainty in the growth model is introduced through the inclusion of parameter uncertainty. This is done by making use of the parameter variance-covariance matrix (the `vcov` slot of the `a4aGr` class) and setting a distribution for the parameters. The variance-covariance matrix could come from the parameter uncertainty from fitting the growth model parameters, or a meta analysis of correlation between parameters.

Here we set the variance-covariance matrix by scaling a correlation matrix, using a cv of 0.2. Based on 

$$\rho_{x,y}=\frac{\Sigma_{x,y}}{\sigma_x \sigma_y}$$

and 

$$CV_x=\frac{\sigma_x}{\mu_x}$$

```{r, set_vcov_example} 
# Make an empty cor matrix
cm <- diag(c(1,1,1))
# k and linf are negatively correlated while t0 is independent
cm[1,2] <- cm[2,1] <- -0.5
# scale cor to var using CV=0.2
cv <- 0.2
p <- c(linf=60, k=0.09, t0=-0.01)
vc <- matrix(1, ncol=3, nrow=3)
l <- vc
l[1,] <- l[,1] <- p[1]*cv
k <- vc
k[,2] <- k[2,] <- p[2]*cv
t <- vc
t[3,] <- t[,3] <- p[3]*cv
mm <- t*k*l
diag(mm) <- diag(mm)^2
mm <- mm*cm
# check that we have the intended correlation
all.equal(cm, cov2cor(mm))
```

Create the `a4aGr` object as before but now we also include the `vcov` argument for the variance-covariance matrix.

```{r, making_vcov_example} 
vbObj <- a4aGr(
  grMod=~linf*(1-exp(-k*(t-t0))),
  grInvMod=~t0-1/k*log(1-len/linf),
  params=FLPar(
    linf=p["linf"], k=p["k"], t0=p["t0"],
    units=c("cm","year-1","year")),
    vcov=mm
)
```

First we show a simple example where we assume that the parameters are represented using a multivariate normal distribution. Note that the object we have just created has a single iteration of each parameter.

```{r, simulate_vcov_example} 
vbObj@params
dim(vbObj@params)
```

We simulate 250 iterations from the `a4aGr` object by calling `mvrnorm()` using the variance-covariance matrix we created earlier. The object will now have 250 iterations of each parameter, randomly sampled from the multivariate normal distribution.

```{r}
vbNorm <- mvrnorm(250,vbObj)
vbNorm@params
dim(vbNorm@params)
```

We can now convert from length to ages data based on the 250 parameter iterations, which will produce 250 sets of age data. For example, the next code will convert a single length vector using each of the 250 parameter iterations.

```{r}
lvec <- 5:10+0.5
ages <- predict(vbNorm, len=lvec)
dim(ages)
```

The marginal distributions of the parameters can be seen in Figure \@ref(fig:plotnormparams).

```{r, plotnormparams, fig.cap="The marginal distributions of each of the parameters from using a multivariate normal distribution.", echo=FALSE}
par(mfrow=c(1,3))
hist(c(params(vbNorm)["linf",]), main="linf", prob=TRUE, xlab="")
hist(c(params(vbNorm)["k",]), main="k", prob=TRUE, xlab="")
hist(c(params(vbNorm)["t0",]), main="t0", prob=TRUE, xlab="")
```

Pairwise plots show the covariance between each pair of parameters and the shape of their correlation (Figure \@ref(fig:plotnormscatter)).

```{r, plotnormscatter, fig.cap="Scatter plot of the 10000 samples parameter from the multivariate normal distribution.", echo=FALSE}
splom(data.frame(t(params(vbNorm)@.Data)), par.settings=list(plot.symbol=list(pch=19, cex=0.1, col=1)))
```

Using the new generated age vectors one can depict the growth curves for the 250 iterations, which displays individual growth uncertainty (Figure \@ref(fig:plotmvgrowth)).

```{r, plotmvgrowth, fig.cap="Growth curves using parameters simulated from a multivariate normal distribution.", echo=FALSE}
#df0 <- melt(predict(vbNorm, t=0:50+0.5))
bwplot(value~factor(Var1), data=melt(predict(vbNorm, t=0:50+0.5)), par.settings=list(plot.symbol=list(cex=0.2, col="gray50"), box.umbrella=list(col="gray40"), box.rectangle=list(col="gray30")), ylab="length (cm)", xlab="age (years)", scales=list(x=list(rot=90)))
#boxplot(t(predict(vbNorm, t=0:50+0.5)))

#xyplot(value~factor(Var1), groups=iter, type="l", data=melt(predict(vbNorm, t=0:50+0.5)), ylab="length (cm)", xlab="age (years)", scales=list(x=list(rot=90)), col="gray50")

```

## Adding uncertainty to growth parameters with a multivariate triangle distribution
\label{sec:growth_triangle_cop}

One alternative to using a normal distribution is to use a [triangle distribution](http://en.wikipedia.org/wiki/Triangle\_distribution). We use the package `triangle` [@R-triangle]  where this distribution is parametrized using the minimum, maximum and median values. This can be very attractive if the analyst needs to scrape information from the web or literature, and use a meta-analysis to build the parameters' distribution. The triangle distribution has the advantage of setting hard tail limits, avoiding to generate extreme values. Here we show an example of setting a triangle distribution with values taken from Fishbase [@fishbase].

The following shows a method to extract data from fishbase. However, due to potential changes in the way one gets data from fishbase from whithin R, we've downloaded the data beforehand and load it for this example.

```{r, fbscrap, eval=FALSE}
# The web address for the growth parameters for redfish (Sebastes norvegicus)
addr <- "https://fishbase.se/PopDyn/PopGrowthList.php?ID=501"
# Scrape the data
tab <- try(readHTMLTable(addr))
```

```{r,  echo=FALSE}
# Load local copy if no web
load("data/tab.RData")
```

```{r, tri_example}
# Interrogate the data table and get vectors of the values
linf <- as.numeric(as.character(tab$dataTable[,2]))
k <- as.numeric(as.character(tab$dataTable[,4]))
t0 <- as.numeric(as.character(tab$dataTable[,5]))
# Set the min (a), max (b) and median (c) values for the parameter as a list of lists
# Note that t0 has no 'c' (median) value. This makes the distribution symmetrical
triPars <- list(
  linf=list(a=min(linf), b=max(linf), c=median(linf)),
  k=list(a=min(k), b=max(k), c=median(k)),
  t0=list(a=median(t0, na.rm=T)-IQR(t0, na.rm=T)/2, b=median(t0, na.rm=T)+IQR(t0, na.rm=T)/2))

# Draw 250 samples using mvrtriangle
vbTri <- mvrtriangle(250, vbObj, paramMargins=triPars)
```

Note that in this case we're not building a new object with all the parameters' information. We're using the argument `paramMargins` to pass the parameters' information to the method.

The marginals will reflect the uncertainty on the parameter values that were scraped from @fishbase but, as we don't really believe the parameters are multivariate normal, here we adopted a distribution based on a _t_ copula with triangle marginals. The marginal distributions can be seen in Figure \@ref(fig:plottriparams) and the shape of the correlation can be seen in Figure \@ref(fig:plottriscatter).

```{r, plottriparams, echo=FALSE, fig.cap="The marginal distributions of each of the parameters from using a multivariate triangle distribution."}
par(mfrow=c(1,3))
hist(c(params(vbTri)["linf",]), main="linf", prob=TRUE, xlab="")
hist(c(params(vbTri)["k",]), main="k", prob=TRUE, xlab="")
hist(c(params(vbTri)["t0",]), main="t0", prob=TRUE, xlab="")
```

```{r, plottriscatter, echo=FALSE, fig.cap="Scatter plot of the 10000 samples parameter from the multivariate triangle distribution."}
splom(data.frame(t(params(vbTri)@.Data)), par.settings=list(plot.symbol=list(pch=19, cex=0.1, col=1)))
```

We can still use `predict()` to see the growth model uncertainty (Figure \@ref(fig:plottrigrowth)). Comparing with Figure \@ref(fig:plotmvgrowth) one can see that using triangle distribution generates a lot less outliers, or values outside the central range of the growth curve.

```{r, plottrigrowth, echo=FALSE, fig.cap="Growth curves using parameters simulated from a multivariate triangle distribution."}
#df0 <- melt(predict(vbTri, t=0:50+0.5))
bwplot(value~factor(Var1), data=melt(predict(vbTri, t=0:50+0.5)), par.settings=list(plot.symbol=list(cex=0.2, col="gray50"), box.umbrella=list(col="gray40"), box.rectangle=list(col="gray30")), ylab="length (cm)", xlab="age (years)", scales=list(x=list(rot=90)))
#boxplot(t(predict(vbTri, t=0:20+0.5)))

#xyplot(value~factor(Var1), groups=iter, type="l", data=melt(predict(vbTri, t=0:50+0.5)), ylab="length (cm)", xlab="age (years)", scales=list(x=list(rot=90)), col="gray50")

```

Remember that the above examples use a variance-covariance matrix that we essentially made up. An alternative would be to scrape the entire growth parameters dataset from Fishbase and compute the shape of the variance-covariance matrix yourself.

## Adding uncertainty to growth parameters with statistical copulas

A more general approach to adding parameter uncertainty is to make use of statistical copulas [@sklar1959]. @copulahistory describes statistical “copula” as a multivariate cumulative distribution function with uniform margins on the unit interval. @sklar1959 highlighted the fact that any multivariate distribution can be expressed as a function of its margins and a copula. The idea is very actractive, one can simulate any multivariate distribution by setting a multivariate function in the unit interval which describes how the margins relate to each other, and scale up the univariate uniform margin with any continuos univariate distribution.

In our case this is possible with the `mvrcop()` function, borrowed from the package `copula` [@R-copula]. The example below keeps the same parameters and changes only the copula type and family but a lot more can be done. Check the package `copula` for more information.

```{r, copula_triangle_example} 
vbCop <- mvrcop(250, vbObj,
  copula="archmCopula",
  family="clayton",
  param=2,
  margins="triangle",
  paramMargins=triPars)
```

The shape of the correlation as well as the resulting growth curves are shown in Figures \@ref(fig:plotcoptriscatter) and \@ref(fig:plotcoptrigrowth).

```{r, plotcoptriscatter, echo=FALSE, fig.cap="Scatter plot of the 250 samples parameter from the using an archmCopula copula with triangle margins."}
splom(data.frame(t(params(vbCop)@.Data)), par.settings=list(plot.symbol=list(pch=19, cex=0.1, col=1)))
```

```{r, plotcoptrigrowth, fig.cap="Growth curves using parameters simulated from an archmCopula copula with triangle margins.", echo=FALSE}
bwplot(value~factor(Var1), data=melt(predict(vbCop, t=0:50+0.5)), par.settings=list(plot.symbol=list(cex=0.2, col="gray50"), box.umbrella=list(col="gray40"), box.rectangle=list(col="gray30")), ylab="length (cm)", xlab="age (years)", scales=list(x=list(rot=90)))

#xyplot(value~factor(Var1), groups=iter, type="l", data=melt(predict(vbCop, t=0:50+0.5)), ylab="length (cm)", xlab="age (years)", scales=list(x=list(rot=90)), col="gray50")

```

## Converting from length to age based data - the `l2a()` method

After introducing uncertainty in the growth model through the parameters it's time to transform the length-based dataset into an age-based dataset. The method that deals with this process is `l2a()`. The implementation of this method for the `FLQuant` class is the main workhorse. There are two other implementations, for the `FLStock` and `FLIndex` classes, which are mainly wrappers that call the `FLQuant` method several times.

When converting from length-based data to age-based data you need to be aware of how the aggregation of length classes is performed. For example, individuals in length classes 1-2, 2-3, and 3-4 cm may all be considered as being of age 1 (obviously depending on the growth model). How should the values in those length classes be combined?

If the values are abundances then the values should be summed. Summing other types of values, such as mean weight, does not make sense. Instead these values are averaged over the length classes (possibly weighted by the abundance). This is controlled using the `stat` argument which can be either `mean` or `sum` (the default). Fishing mortality is not computed to avoid making wrong assumptions about the meaning of F at length.

We demonstrate the method by converting a catch-at-length `FLQuant` to a catch-at-age `FLQuant`. First we make an `a4aGr` object with a multivariate triangle distribution using parameters extracted from an AI agent. We use 10 iterations as an example, and call `l2a()` by passing in the length-based `FLQuant` and the `a4aGr` object.

```{r, FLQ_l2a, message=FALSE, warning=FALSE}
triPars <- list(
  linf=list(a=55, b=60),
  k=list(a=0.05, b=0.06),
  t0=list(a=-3, b=-2))

# Draw 10 samples using mvrtriangle
vbTriSmall <- mvrtriangle(10, vbObj, paramMargins=triPars)
# slice catch numbers at lengths to ages by summing catches
cth.n <- l2a(catch.n(rfLen.stk), vbTriSmall)
# note there's a lot of 0 catches so we'll set the plus group at 21
cth.n <- setPlusGroup(cth.n, 21)
# there's also negative ages. The simulated data included individuals in lengths that won't show in the catches, like 1 cm. We'll trim those ages
cth.n <- cth.n[ac(0:21)]

# slice catch weights at lengths to ages by averaging catches
cth.wt <- l2a(catch.wt(rfLen.stk), vbTriSmall, stat="mean")
# same process to deal with negative ages
cth.wt <- cth.wt[ac(0:21)]
```

In the previous example, the `FLQuant` object that was sliced (`catch.n(rfLen.stk)`) had only one iteration. This iteration was sliced by each of the iterations in the growth model. It is possible for the `FLQuant` object to have the same number of iterations as the growth model, in which case each iteration of the `FLQuant` and the growth model are used together. It is also possible for the growth model to have only one iteration while the `FLQuant` object has many iterations. The same growth model is then used for each of the `FLQuant` iterations. As with all `FLR` objects, the general rule is _one or n_ iterations.

As well as converting one `FLQuant` at a time, we can convert entire `FLStock` and `FLIndex` objects. In these cases the individual `FLQuant` slots of those classes are converted from length-based to age-based. As mentioned above, the aggregation method depends on the type of values the slots contain. The abundance slots (`*.n`, such as `stock.n`) are summed. The `*.wt`, `m`, `mat`, `harvest.spwn` and `m.spwn` slots of an `FLStock` object are averaged. The `catch.wt` and `sel.pattern` slots of an `FLIndex` object are averaged, while the `index`, `index.var` and `catch.n` slots are summed. 

The method for `FLStock` classes takes an additional argument for the plusgroup.

```{r, FLS_FLI_l2a, message=FALSE}
aStk <- l2a(rfLen.stk, vbTriSmall, plusgroup=21)
aIdx <- l2a(rfTrawl.idx, vbTriSmall)
```

When converting with `l2a()` all lengths above Linf are converted to the maximum age, as there is no information in the growth model about how to deal with individuals larger than Linf. 


<!--chapter:end:03-growth.Rmd-->

# Modelling Natural Mortality

```{R, message=FALSE, warning=FALSE, echo=FALSE}
# libraries
library(FLa4a)
# datasets
data(ple4)
```

Natural mortality ($M$) is a critical parameter in stock assessment models, representing all sources of mortality not related to fishing or harvest ($F$). Combined, these two sources constitute the total mortality ($Z$) that individuals experience, with $Z = F + M$.

However, natural mortality is notoriously difficult to observe and estimate. Only a few methods, such as mark-recapture studies, provide direct estimates of $M$. However, these methods are not applicable to all species and are often costly. As an alternative, life-history theory is commonly used to derive values for $M$ that are consistent with individual growth and reproduction.

There is an extensive body of literature on natural mortality. Works by @pauly1980, @gislason2010, @charnov1993, @maunder2023mrev and @quinn1999, as well as a dedicated special issue in Fisheries Research [@MCAPAM2023], offer valuable insights and serve as excellent starting points. Given the parameter's importance and the difficulty of obtaining direct observations, natural mortality is widely regarded as one of the most significant sources of uncertainty in stock assessments.

Within the a4a framework, natural mortality is treated as an externally fixed parameter in the stock assessment model. Our aim is to develop a system that enables analysts to explore alternative models for $M$ and compare the resulting assessment outcomes. This approach provides a more comprehensive information base to support informed decision-making throughout the stock assessment process.

Within the `a4a` framework, the general method for adding natural mortality in the stock assessment model is to:

1. Create an object of class `a4aM` which holds the natural mortality model and parameters.
2. Add uncertainty to the parameters in the `a4aM` object.
3. Apply the `m()` method to the `a4aM` object to create an age or length based `FLQuant` object of the required dimensions.

The resulting `FLQuant` object can then be directly inserted into an `FLStock` object to be used for the assessment.

In this section we go through each of the steps in detail using a variety of different models.

## `a4aM` - The M class

Natural mortality is implemented in a class named `a4aM`. This class is made up of three objects of the class `FLModelSim`. Each object is a model that represents one effect: an age or length effect, a scaling (level) effect and a time trend, named `shape`, `level` and `trend`, respectively. The impact of the models is multiplicative, i.e. the overal natural mortality is given by `shape` x `level` x `trend`.

```{r, showClass_a4aM}
showClass("a4aM")
```

The `a4aM` constructor requires that the models and parameters are provided. The default method will build each of these models as a constant value of 1.

As a simple example, the usual "0.2" guessestimate could be set up by setting the `level` model to have a single parameter with a fixed value, while the other two models, `shape` and `trend`, have a default value of 1 (meaning that they have no effect).

```{r, m_02}
mod02 <- FLModelSim(model=~a, params=FLPar(a=0.2))
m1 <- a4aM(level=mod02)
m1
```

More interesting natural mortality shapes can be set up using biological knowledge. The following example uses an exponential decay over ages (implying that the resulting `FLQuant`} generated by the `m()` method will be age based). We also use Jensen's second estimator [@Kenchington2014] as a scaling `level` model, which is based on the von Bertalanffy $K$ parameter, $M=1.5K$.

```{r, jensen_second_m}
shape2 <- FLModelSim(model=~exp(-age-0.5))
level2 <- FLModelSim(model=~1.5*k, params=FLPar(k=0.4))
m2 <- a4aM(shape=shape2, level=level2)
m2
``` 

Note that the `shape` model has `age` as a parameter of the model but is not set using the `params` argument.

The `shape` model does not have to be age-based. For example, here we set up a `shape` model using Gislason's second estimator [@Kenchington2014]: $M_l=K(\frac{L_{\inf}}{l})^{1.5}$. We use the default `level` and `trend` models.

```{r, gis_shape}
shape_len <- FLModelSim(model=~K*(linf/len)^1.5, params=FLPar(linf=60, K=0.4))
m_len <- a4aM(shape=shape_len)
```

Another option is to model how an external factor may impact natural mortality. This can be added through the `trend` model. Suppose natural mortality can be modelled with a dependency on the NAO index, due to some mechanism that results in having lower mortality when NAO is negative and higher when it's positive. In this example, the impact is represented by the NAO value on the quarter before spawning, which occurs in the second quarter.

We use this to make a complex natural mortality model with an age based shape model, a level model based on $K$ and a trend model driven by NAO, where mortality increases by 50% if NAO is positive on the first quarter.

```{r, nao_m}
# Get NAO
nao <- read.table("https://www.cpc.ncep.noaa.gov/products/precip/CWlink/pna/norm.nao.monthly.b5001.current.ascii.table", skip=1, fill=TRUE, na.strings="-99.90")
dnms <- list(quant="nao", year=1950:2024, unit="unique", season=1:12, area="unique")
# Build an FLQuant from the NAO data
nao.flq <- FLQuant(unlist(nao[,-1]), dimnames=dnms, units="nao")
# Build covar by calculating mean over the first 3 months
nao <- seasonMeans(trim(nao.flq, year=dimnames(stock.n(ple4))$year))
# Turn into Boolean
nao <- (nao>0)
# Constructor
trend3 <- FLModelSim(model=~1+b*nao, params=FLPar(b=0.5))
shape3 <- FLModelSim(model=~exp(-age-0.5))
level3 <- FLModelSim(model=~1.5*k, params=FLPar(k=0.4))
m3 <- a4aM(shape=shape3, level=level3, trend=trend3)
m3
```

## Adding uncertainty to natural mortality parameters with a multivariate normal distribution

Uncertainty on natural mortality is added through uncertainty on the parameters.

In this section we'll' show how to add multivariate normal uncertainty. We make use of the class `FLModelSim` method `mvrnorm()`, which is a wrapper for the method `mvrnorm()` distributed by the package `MASS` [@mass].

We'll create an `a4aM` object with an exponential shape, a `level` model based on $k$ and temperature, Jensen's third estimator [@Kenchington2014], and a `trend` model driven by the NAO (as above). Afterwards a variance-covariance matrix for the `level` and `trend` models will be included. Finally, create an object with 100 iterations using the `mvrnorm()` method.

Create the object:

```{r, mvrnorm_m}
shape4 <- FLModelSim(model=~exp(-age-0.5))
level4 <- FLModelSim(model=~k^0.66*t^0.57,
    params=FLPar(k=0.4, t=10),
    vcov=array(c(0.002, 0.01,0.01, 1), dim=c(2,2)))
trend4 <- FLModelSim(model=~1+b*nao,
    params=FLPar(b=0.5),
    vcov=matrix(0.02))
m4 <- a4aM(shape=shape4, level=level4, trend=trend4)
# Call mvrnorm()
m4 <- mvrnorm(100, m4)
m4
```

Inspect the level model (for example):

```{r, mvrnorm_m1}
level(m4)
```

Note the variance in the parameters: 

```{r, mvrnorm_m2}
params(trend(m4))
```

Note the shape model has no parameters and no uncertainty:

```{r, mvrnorm_m3}
params(shape(m4))
``` 

In this particular case, the `shape` model will not be randomized because it doesn't have a variance-covariance matrix. Also note that because there is only one parameter in the `trend` model, the randomization will use a univariate normal distribution. The same model could be achieved by using `mnrnorm()` on each model component:

```{r, univariate_m}
m4 <- a4aM(shape=shape4,
    level=mvrnorm(100, level4),
    trend=mvrnorm(100, trend4))
```

an exact match would require to control the random seed so that the draws would be exactly the same.

## Adding uncertainty to natural mortality parameters with statistical copulas

We can also use copulas to add parameter uncertainty to the natural mortality model, similar to the way we use them for the growth model in Section \@ref(growth). As stated above these processes make use of the methods implemented for the `FLModelSim` class.

In the following example we'll use again Gislason's second estimator, $M_l=K(\frac{L_{\inf}}{l})^{1.5}$ and a triangle copula to model parameter uncertainty. The method `mvrtriangle()` is used to create 1000 iterations. 

```{r, gis_copula}
linf <- 60
k <- 0.4
# vcov matrix (make up some values)
mm <- matrix(NA, ncol=2, nrow=2)
# 10% cv
diag(mm) <- c((linf*0.1)^2, (k*0.1)^2)
# 0.2 correlation
mm[upper.tri(mm)] <- mm[lower.tri(mm)] <- c(0.05)
# a good way to check is using cov2cor
cov2cor(mm)
# create object
mgis2 <- FLModelSim(model=~k*(linf/len)^1.5, params=FLPar(linf=linf, k=k), vcov=mm)
# set the lower, upper and centre of the parameters
pars <- list(list(a=55,b=65), list(a=0.3, b=0.6, c=0.35))
mgis2 <- mvrtriangle(1000, mgis2, paramMargins=pars)
mgis2
```

The resulting parameter estimates and marginal distributions can be seen in Figures \@ref(fig:plottrigism) and \@ref(fig:plottrigismhist). By default the method uses an elliptical copula of t family (see `?ellipCopula` for more information).

```{r, plottrigism, echo=FALSE, fig.cap="Pairwise depiction of Gislason's second natural mortality model estimates using a 't' family elliptic copula and triangle distribution margins."}
splom(t(params(mgis2)@.Data), par.settings=list(plot.symbol=list(pch=19, cex=0.1, col=1)))
```

```{r, plottrigismhist, echo=FALSE, fig.cap="Marginal distributions of the parameters for Gislason's second natural mortality model using triangle distributions."}
par(mfrow=c(2,1))
hist(c(params(mgis2)["linf",]), main="Linf", xlab="")
hist(c(params(mgis2)["k",]), main="K", xlab="")
```

We now have a new model that can be used for the `shape` model. You can use the constructor or the set method to add the new model. Note that we have a quite complex method now for M. A length based `shape` model from Gislason's work, Jensen's third model based on temperature `level` and a time `trend` depending on NAO. All of the component models have uncertainty in their parameters.

```{r, making_complicated_m}
m5 <- a4aM(shape=mgis2, level=level4, trend=trend4)
# or
m5 <- m4
shape(m5) <- mgis2
```

## Computing natural mortality time series - the "m" method

Now that we have set up the natural mortality `a4aM` model and added parameter uncertainty to each component, we are ready to generate the `FLQuant` of natural mortality with `m()`. The `m()` method is the workhorse method for computing natural mortality. The method returns a `FLQuant` that can be inserted in an `FLStock` to be used in the assessment method.

The size of the `FLQuant` object is determined by the `min`, `max`, `minyear` and `maxyear` elements of the `range` slot of the `a4aM` object. By default the values of these elements are set to 0, which generates a `FLQuant` with length 1 in the `quant` and `year` dimension. The `range` slot can be set by hand, or by using the `rngquant()` and `rngyear()` methods.

The name of the first dimension of the output `FLQuant` (e.g. 'age' or 'len') is determined by the parameters of the `shape` model. If it is not clear what the name should be then the name is set to 'quant'.

Here we demonstrate `m()` using the simple `a4aM` object we created above that has constant natural mortality.

Start with the simplest model:

```{r, simple_m}
m1
```

Check the range:

```{r, simple_m1}
range(m1)
```

The $M$ `FLQuant` won't have ages or years:

```{r, simple_m2}
m(m1)
```

To have a more useful matrix of values that cover the ages and years in the `FLStock` object, the analyst needs to set the quant and year ranges.

```{r, simple_m3}
# set the quant range
range(m1, c("min","max")) <- c(0,7)
# set the year range
range(m1, c("minyear","maxyear")) <- c(2000, 2010)
range(m1)
```

Create the object with the M estimates by age and year, note the name of the first dimension is 'quant'.

```{r, simple_m4}
m(m1)
```

The next example has an age-based shape (model "m2" from above). As the `shape` model has 'age' as a variable which is not included in the `FLPar` slot it is used as the name of the first dimension of the resulting `FLQuant`.

An important feature of the `m()` method is the use of the `level` model. The outcome of the `level` model will be applied to a range of ages or lengths, set by the `mbar` information in the range slot. In this example the level model is $1.5*K$ and since $K=0.4$, the level predicted by the model will be $0.6$. The `m()` model will use the information in the range, `minmbar` and `maxmbar` to compute the mean level. This mean level will match the value given by the `level` model. The `mbar` range can be changed with the `rngmbar()` method. We illustrate this by making an `FLQuant` with age varying natural mortality.

Check the model and set the ranges:

```{r, m2}
m2
# set the quant range
range(m2, c("min","max")) <- c(0,7)
# set the year range
range(m2, c("minyear","maxyear")) <- c(2000, 2003)
range(m2)
m(m2)
```

Note that the level value is:

```{r, m2_1}
predict(level(m2))
```

Which is the same as:

```{r, m2_2}
m(m2)["0"]
```

This is because the `mbar` range is currently set to "0" and "0" (see above) and the mean natural mortality value over this range is given by the level model.

We can change the `mbar` range:

```{r, m2_3}
range(m2, c("minmbar","maxmbar")) <- c(0,5)
range(m2)
```

Which rescales the the natural mortality at age:

```{r, m2_4}
m(m2)
```

Check that the mortality over the mean range is the same as the level model:

```{r, m2_5}
quantMeans(m(m2)[ac(0:5)])
```

The next example uses a time trend for the `trend` model. We use the `m3` model we made earlier. The `trend` model for this model has a covariate, 'nao'. This needs to be passed to the `m()` method. The year range of the 'nao' covariate should match that of the `range` slot.

Simple, pass in a single nao value (only one year):

```{r, m3_trend}
m(m3, nao=1)
```

Set ages:

```{r, m3_trend1}
range(m3, c("min","max")) <- c(0,7)
m(m3, nao=0)
```

With ages and years - passing in the NAO data as numeric (1,0,1,0)

```{r, m3_trend2}
range(m3, c("minyear","maxyear")) <- c(2000, 2003)
m(m3, nao=as.numeric(nao[,as.character(2000:2003)]))
```

The final example show how `m()` can be used to make an `FLQuant` with uncertainty (see Figure \@ref(fig:uncertainm)). We use the `m4` object from earlier with uncertainty on the `level` and `trend` parameters.

```{r, m4_uncertainty_m}
range(m4, c("min","max")) <- c(0,7)
range(m4, c("minyear","maxyear")) <- c(2000, 2003)
flq <- m(m4, nao=c(nao[,ac(2000:2003)]))
flq
dim(flq)
```

```{r, uncertainm, echo=FALSE, fig.cap="Natural mortality with age and year trend."}
bwplot(data~factor(age)|year, data=flq, par.settings=list(plot.symbol=list(cex=0.2, col="gray50"), box.umbrella=list(col="gray40"), box.rectangle=list(col="gray30")), ylab="M", xlab="age (years)", scales=list(x=list(rot=90)))
```

Notably, the last example created a M model that varies with time, based on an environmental variable, and adds estimation uncertainty, showing the huge flexibility this method can deal with.


<!--chapter:end:04-m.Rmd-->

# Stock assessment framework

```{R, message=FALSE, warning=FALSE, echo=FALSE}
# libraries
library(FLa4a)
# datasets
data(ple4)
data(ple4.indices)
data(ple4.index)
```

## Maths description \label{sec:math}

The stock assessment model is based on two types of observations: catches, $\hat{C}$, and abundance indices, $\hat{I}$. The model predicts catches at age $C_{ay}$ and indices of abundance $I_{ays}$ for each age $a$, year $y$ and survey $s$ in the input dataset. To predict catches and survey indices, the model uses the standard population dynamics model

\begin{equation}
N_{a+1,y+1} = N_{ay} \exp \left( - F_{ay} - M_{ay} \right)
\end{equation}


where $N_{ay}$ is the number of individuals at age $a$ in year $y$, $F_{ay}$ is the fishing mortality at age $a$ in year $y$, and $M_{ay}$ is the natural mortality at age $a$ in year $y$.  Any fish that survived beyond the oldest age $A$ in the model are accumulated in the oldest age group and are assumed to be fished at a common rate $F_{A,y}$.

\begin{align}
N_{A,y+1} = &N_{A-1,y} \exp \left( - F_{A-1,y} - M_{A-1,y} \right) \\
            &+ N_{A,y} \exp \left( - F_{A,y} - M_{A,y} \right)
\end{align}

The numbers $N_{a,y}$ are initiated in the first year, $y=1$ and at the youngest age, $a=1$, and the matrix of numbers at age are filled in according to the population dynamics model stated above (see Figure \@ref(fig:popdyn)).

```{r, popdyn, fig.align="center", fig.cap="Statistical catch at age population dynamics model", echo=FALSE, out.width="40%"}
knitr::include_graphics("figure/sca_matrix.png")
```

Defining $R_y = N_{1,y}$, the numbers at age can be written (ignoring the plus group) as:

\begin{equation}
N_{a,y} = \left\{ \begin{matrix} R_{y-a+1} \exp \left( - \sum^a_{i=1} F_{a-i,y-i} + M_{a-i,y-i} \right) &  y \geq a \\ N_{a-y+1,1} \exp \left( - \sum^{a-y}_{i=1} F_{a-i,y-i} + M_{a-i,y-i} \right) &  y \lt a \end{matrix} \right.
\end{equation}

Catches in numbers by age and year are defined in terms of the three quantities: natural mortality, fishing mortality and recruitment; using a modified form of the well known Baranov catch equation:

\begin{equation}
C_{ay} = \frac{F_{ay}}{F_{ay}+M_{ay}}\left(1 - e^{-(F_{ay}+M_{ay})}\right) R_{y}e^{-\sum (F_{ay} + M_{ay})}
\end{equation}

Survey indices by age and year are defined in terms of the same three quantities with the addition of survey catchability:

\begin{equation}
I_{ays} = Q_{ays} R_{y}e^{-\sum (F_{ay} + M_{ay})}
\end{equation}

It is assumed that the observed catches are normally distributed about the model predictions on the log scale with observation variance $\sigma^2_{ay}$. that is:

\begin{equation}
\log \hat{C}_{ay} \sim \text{Normal} \Big( \log C_{ay}, \sigma^2_{ay}\Big)
\end{equation}

\begin{equation}
\log \hat{I}_{ays} \sim \text{Normal} \Big( \log I_{ays}, \tau^2_{ays} \Big)
\end{equation}

The log-likelihood can now be defined as the sum of the log-likelihood of the observed catches:

\begin{equation}
\ell_C = \sum_{ay} w^{(c)}_{ay}\ \ell_N \Big( \log C_{ay}, \sigma^2_{ay} ;\ \log \hat{C}_{ay} \Big)
\end{equation}

and the log-likelihood of the observed survey indices as:

\begin{equation}
\ell_I = \sum_s \sum_{ay} w^{(s)}_{ays}\ \ell_N \Big( \log I_{ays}, \tau_{ays}^2 ;\ \log \hat{I}_{ays} \Big)
\end{equation}

giving the total log-likelihood

\begin{equation}
\ell = \ell_C + \ell_I
\end{equation}

which is defined in terms of the strictly positive quantites, $M_{ay}$, $F_{ay}$, $Q_{ays}$ and $R_{y}$, and the observation variances $\sigma_{ay}$ and $\tau_{ays}$. As such, the log-likelihood is over-parameterised as there are many more parameters than observations. In order to reduce the number of parameters, $M_{ay}$ is assumed known (as is common).

The remaining parameters are written in terms of a linear combination of covariates $x_{ayk}$, e.g.

\begin{equation}
\log F_{ay} = \sum_k \beta_k x_{ayk}
\end{equation}

where $k$ is the number of parameters to be estimated and is sufficiently small. Using this tecnique the quantities $\log F$, $\log Q$, $\log \sigma$ and $\log \tau$
%$\log \text{initial\,age\,structure}$ % this is not present in the above
(in bold in the equations above) can be described by a reduced number of parameters. The following section has more discussion on the use of linear models in a4a.

The a4a statistical catch-at-age model can addionally allow for a functional relationship that links predicted recruitment $\tilde{R}$ based on spawning stock biomass and modelled recruitment $R$, to be imposed as a fixed variance random effect. [NEEDS REVISION, sentence not clear]

Options for the relationship are the hard coded models Ricker, Beverton Holt, smooth hockeystick or geometric mean. This is implemented by including a third component in the log-likelihood:

\begin{equation}
\ell_{SR} = \sum_y \ell_N \Big( \log \tilde{R}_y(a, b), \phi_y^2 ;\ \log R_y \Big)
\end{equation}


giving the total log-likelihood

\begin{equation}
\ell = \ell_C + \ell_I + \ell_{SR}
\end{equation}


Using the (time varying) Ricker model as an example, predicted recruitment is

\begin{equation}
\tilde{R}_y(a_y,b_y) = a_y S_{y-1} e^{-b_y S_{y-1}}
\end{equation}


where $S$ is spawning stock biomass derived from the model parameters $F$ and $R$, and the fixed quantites $M$ and mean weights by year and age. It is assumed that $R$ is log-normally distributed, or equivalently, normally distributed on the log-scale about the (log) recruitment predicted by the SR model $\tilde{R}$, with known variance $\phi^2$, i.e.

\begin{equation}
\log R_y \sim \text{Normal} \Big( \log \tilde{R}_y, \phi_y^2 \Big)
\end{equation}


which leads to the definition of $\ell_{SR}$ given above. In all cases $a$ and $b$ are strictly positive, and with the quantities $F$, $R$, etc. linear models are used to parameterise $\log a$ and/or $\log b$, where relevant.

By default, recruitment $R$ as apposed to the reruitment predicted from a stock recruitment model $\tilde{R}$, is specified as a linear model with a parameter for each year, i.e.

\begin{equation}
\log R_y = \gamma_y
\end{equation}


This is to allow modelled recruitment $R_y$ to be shrunk towards the stock recruitment model. However, if it is considered appropriate that recruitment can be determined exactly by a relationship with covariates, it is possible, to instead define $\log R$ in terms of a linear model in the same way as $\log F$, $\log Q$, $\log \sigma$ and $\log \tau$.  %But this is pretty much the same as taking a geometric mean, with a model on log a, and making the variance very small.

## Model structure \label{sec:submod}

The `a4a` stock assessment framework allows the user to set up a large number of different models. The mechanics which provide this flexibility are designed around the concept of submodels. The user has to define the model structure using `R` formulas, including `mgcv` [@R-mgcv] gam formulas, for each unknown variable that must be estimated. By using `R` formulas the stock assessment framework gives lot's of flexibility to explore models and combinations of submodels.

There are 5 submodels in operation:

- a model for F-at-age ($F_{ay}$)
- a (list) of model(s) for abundance indices catchability-at-age ($Q_{ays}$),
- a model for recruitment ($R_y$)
- a list of models for the observation variance of catch-at-age and abundance indices ($\{\sigma^2_{ay}, \tau^2_{ays}\}$)
- a model for the initial age structure ($N_{a,y=1}$)

When setting the structure of each submodel the user is in fact building the predictive model and its parameters. The optimization process, done through `ADMB`, estimates the parameters and their variance-covariance matrix, allowing further analysis to be carried out, like simulation, prediction, diagnostics, etc. All the statistical machinery will be at the user's reach.

The framework's workhorse is the method `sca`, which is called over `FLStock` and `FLIndex` or `FLIndices` objects. The following code shows an example of a call to `sca()`.

```{r}
# fit a model with a single index
fit <- sca(ple4, ple4.index,
    fmodel = ~ s(age, k = 5),
    qmodel = list(~ s(age, k = 4)),
    srmodel = ~ 1,
    n1model = ~ s(age, k = 5),
    vmodel = list( ~ 1, ~ 1))
# check output
fit
```

The `sca()` method arguments are shown in Table \@ref(tab:scaargs)

|Argument|Default|Description|
|:---|:---|:----------|
|stock|missing|‘FLStock’ object containing catch and stock information|
|indices|missing|‘FLIndices’ object containing survey indices|
|fmodel|missing|a formula object depicting the model for log fishing mortality at age|
|qmodel|missing|a list of formula objects depicting the models for log survey catchability at age|
|srmodel|missing|a formula object depicting the model for log recruitment|
|n1model|missing|a formula object depicting the model for the population in the first year of the time series|
|vmodel|missing|a list of formula objects depicting the model for the variance of fishing mortality and the indices|
|covar|missing|a list with covariates to be used by the submodels. The formula must have an element with the same name as the list element|
|wkdir|missing|used to set a working directory for the admb optimiser; if wkdir is set, all admb files are saved to this folder, otherwise they are deleted|
|verbose|FALSE|if true, admb fitting information is printed to the screen|
|fit|'assessment'|character with type of fit: 'assessment' (ML estimation, returns covariance matrix), 'MP' (ML estimation, doesn't compute covariance matrix) or 'MCMC' (MCMC estimation)|
|center|TRUE|logical defining if the data should be centered before fitting|
|mcmc|missing|a ‘SCAMCMC’ object with the arguments to run MCMC fits (fit='MCMC)|

Table: (\#tab:scaargs) `sca()` arguments

## Submodel building blocks and fundamental `R` formulas

The elements available to build submodels formulas are 'age' and 'year', which can be used to build models with different structures.

In R's modelling language, a constant model is coded as $\sim 1$, while a slope over time would simply be $\sim year$, a smoother over time $\sim s(year, k=10)$, a model with a coefficient for each year would be $\sim factor(year)$. Transformations of the variables are as usual, e.g. $\sim sqrt(year)$, etc; while combinations of all the above can be done although non-convergence will limit the possibilities.

Using the $F$ submodel as example the following specifies the models described in the previous paragraph, see Figure \@ref(fig:fundforms).

```{r, fundforms, fig.cap="Example of fundamental R formulas used to model the year effect"}
# models
m1 <- ~1
m2 <- ~ year
m3 <- ~ s(year, k=10)
m4 <- ~ factor(year)
m5 <- ~ sqrt(year)

# fits
fit1 <- sca(ple4, ple4.indices, fmodel=m1, fit="MP")
fit2 <- sca(ple4, ple4.indices, fmodel=m2, fit="MP")
fit3 <- sca(ple4, ple4.indices, fmodel=m3, fit="MP")
fit4 <- sca(ple4, ple4.indices, fmodel=m4, fit="MP")
fit5 <- sca(ple4, ple4.indices, fmodel=m5, fit="MP")

# plot
lst <- FLStocks(constant=ple4+fit1,
  linear=ple4+fit2,
  smooth=ple4+fit3,
  factor=ple4+fit4,
  sqrt=ple4+fit5)
lst <- lapply(lst, fbar)
lgnd <- list(points=FALSE, lines=TRUE, space='right')
xyplot(data~year, groups=qname, lst, auto.key=lgnd, type='l', ylab='fishing mortality')
```

The models above and their combinations can be used to model both 'age' and 'year'. The corresponding fits for age are show in the code below and Figure \@ref(fig:fundformsage).

```{r, fundformsage, fig.cap="Example of fundamental R formulas  used to model the age effect"}
# models
m1 <- ~1
m2 <- ~ age
m3 <- ~ s(age, k=3)
m4 <- ~ factor(age)
m5 <- ~ sqrt(age)

# fits
fit1 <- sca(ple4, ple4.indices, fmodel=m1, fit="MP")
fit2 <- sca(ple4, ple4.indices, fmodel=m2, fit="MP")
fit3 <- sca(ple4, ple4.indices, fmodel=m3, fit="MP")
fit4 <- sca(ple4, ple4.indices, fmodel=m4, fit="MP")
fit5 <- sca(ple4, ple4.indices, fmodel=m5, fit="MP")

# plot
lst <- FLStocks(constant=ple4+fit1,
  linear=ple4+fit2,
  smooth=ple4+fit3,
  factor=ple4+fit4,
  sqrt=ple4+fit5)
lst <- lapply(lst, function(x) harvest(x)[,'2000'])
xyplot(data~age, groups=qname, lst, auto.key=lgnd, type='l', ylab='fishing mortality in 2000')
```

## The major effects available for modelling

Although the building blocks for formulas are 'age' and 'year', in fact there are three effects that can be modelled for each submodel: 'age', 'year' and 'cohort'. As examples note the following models for fishing mortality.

```{r}
# the age effect
ageeffect <- ~ factor(age)

# the year effect
yeareffect <- ~ factor(year)

# the cohort
cohorteffect <- ~ factor(year-age)

# the fits
fit1 <- sca(ple4, ple4.indices, fmodel=yeareffect)
fit2 <- sca(ple4, ple4.indices, fmodel=ageeffect)
fit3 <- sca(ple4, ple4.indices, fmodel=cohorteffect)
```

and the graphical representation of the three models in Figures \@ref(fig:majeffy) to \@ref(fig:majeffc).

```{r, majeffy, fig.cap="Major effects: the year effect (~ factor(year))", echo=FALSE}
wireframe(harvest(fit1), main='year effect')
```

```{r, majeffa, fig.cap="Major effects: the age effect (~ factor(age))", echo=FALSE}
wireframe(harvest(fit2), main='age effect')
```

```{r, majeffc, fig.cap="Major effects: the cohort effect (~ factor(year-age))", echo=FALSE}
wireframe(harvest(fit3), main='cohort effect')
```

## Classes Description \label{sec:classes}

The data structure used to store and report the fitting process follows an object-oriented paradigm (e.g. the S4 system in R) and is hierarchically organized. The `type` argument in the `sca` method defines the fitting method - either maximum likelihood or MCMC - and specifies whether the variance-covariance matrix of the parameters is calculated and returned in the case of maximum likelihood. The resulting object belongs to a specific class, depending on the selected option. Figure \@ref(fig:iomod) illustrates the input/output model of the statistical stock assessment method based on catch-at-age data.


```{r, iomod, fig.align="center", fig.cap="The fit process input/output model", echo=FALSE, out.width="120%"}
knitr::include_graphics("figure/scamethod.png")
```

Table \@ref(tab:scatype) provides details about the type of fit approach and computation of variance covariance information.

|Type of Fit|Fit Method|Variance-Covariance Matrix|Output Object Class|
|:--|:--|:--|:--|
|MP|Maximum Likelihood|No|`a4aFit`|
|Assessment|Maximum Likelihood|Yes|`a4aFitSA`|
|MCMC|MCMC|No|`a4aFitMCMC`|
Table: (\#tab:scatype) Fit Types and Associated Classes

Type `MP`, for "Management Procedure," returns an `a4aFit` class object designed for use in MSEs (Management Strategy Evaluations) with full feedback models [@puntmse]. Inverting the Jacobian to compute the variance-covariance matrix is computationally intensive in maximum likelihood models, and as MSE analyses often involve thousands of iterations, using `type="MP"` significantly speeds up the process. This option is advantageous for scenarios requiring multiple model fits. However, the lack of immediate feedback on model convergence is a drawback, as convergence is assessed by inverting the Jacobian. A failed inversion indicates non-convergence.

Type `Assessment`, the default, reports both the parameters and their variance-covariance, and as such requires the invertion of the jacobian. The method takes longer to run but returns a much richer dataset which allows the compution confidence intervals, simulate, etc. When the jacobian can't be inverted this method flags potential non-convergence and returns an empty object.

Type `MCMC` uses the Markov Chain Monte-Carlo approach, in which case it doesn't compute likelihoods or variance-covariance matrices. It returns the full draws of the chain, which allows the computation of credible intervals, simulation kind of studies and so on.

Table \@ref(tab:a4afitclass) describes the composition of the `a4aFit` class.

|Class|Slot|Slot's Class|Description|
|:--|:--|:--|:--|
|a4aFit|call|call|Code used to run the analysis|
| |catch.n|FLQuant|Catch numbers at age and year|
| |clock|numeric|Time to run the analysis|
| |desc|character|Description of the stock and/or analysis|
| |fitSumm|array|Summary statistics of the fit (e.g., number of data points)|
| |harvest|FLQuant|Fishing mortality at age and year|
| |index|FLQuants|Indices of abundance (age/biomass, by year)|
| |name|character|Stock name|
| |range|numeric|Age and year range of the data|
| |stock.n|FLQuant|Population in numbers (age and year)|
Table: (\#tab:a4afitclass) `a4aFit` Class Description

The `a4aFitSA` and `a4aFitMCMC` classes extend `a4aFit`, retaining all its slots while adding a `pars` slot of class `SCAPars`. Table \@ref(tab:a4afitSAclass) outlines these classes.

|Class|Slot|Slot's Class|Description|
|:--|:--|:--|:--|
|a4aFitSA|All `a4aFit`||Inherited from `a4aFit`|
| |pars|SCAPars|Parameter information|
|a4aFitMCMC|All `a4aFit`||Inherited from `a4aFit`|
| |pars|SCAPars|Parameter information|
Table: (\#tab:a4afitSAclass) `a4aFitSA` and `a4aFitMCMC` Class Description

The `SCAPars` class stores details about submodel parameters, such as formulas and distributions, and includes three slots: `stkmodel` for stock model parameters, `qmodel` for catchability parameters, and `vmodel` for variance parameters. Table \@ref(tab:SCAParsclass) describes the `SCAPars` class.

|Class|Slot|Slot's Class|Description|
|:--|:--|:--|:--|
|SCAPars|stkmodel|a4aStkParams|Details of fishing mortality, stock recruitment, and initial stock numbers|
| |qmodel|submodel|Details of catchability parameters|
| |vmodel|submodel|Details of variance parameters|
Table: (\#tab:SCAParsclass) `SCAPars` Class Description

The `stkmodel` slot encompasses parameters for fishing mortality, stock recruitment, and initial stock numbers. Due to potential correlations among these parameters, their variance-covariance matrix is reported collectively. Table \@ref(tab:a4aStkParamsclass) describes the slots of the `a4aStkParams` class.

|Class|Slot|Slot's Class|Description|
|:--|:--|:--|:--|
|a4aStkParams|centering|FLPar|Centering parameters|
| |coefficients|FLPar|Model coefficients|
| |desc|character|Description|
| |distr|character|Distributions|
| |fMod|formula|Fishing mortality model|
| |link|function|Link function|
| |linkinv|function|Inverse link function|
| |m|FLQuant|Mortality parameters|
| |mat|FLQuant|Maturity parameters|
| |n1Mod|formula|Initial stock numbers model|
| |name|character|Stock name|
| |range|numeric|Age and year range|
| |srMod|formula|Stock-recruitment model|
| |units|character|Units of measurement|
| |vcov|array|Variance-covariance matrix|
| |wt|FLQuant|Weights|
Table: (\#tab:a4aStkParamsclass) `a4aStkParams` Class Description

The `qmodel` and `vmodel` slots share the `submodel` class, which describes single submodels. Table \@ref(tab:submodelclass) provides details.

|Class|Slot|Slot's Class|Description|
|:--|:--|:--|:--|
|submodel|centering|FLPar|Centering parameters|
| |coefficients|FLPar|Model coefficients|
| |desc|character|Description|
| |distr|character|Distributions|
| |formula|formula|Submodel formula|
| |link|function|Link function|
| |linkinv|function|Inverse link function|
| |name|character|Stock name|
| |range|numeric|Age and year range|
| |vcov|array|Variance-covariance matrix|
Table: (\#tab:submodelclass) `submodel` Class Description



<!--chapter:end:05-saframework.Rmd-->

---
output:
  pdf_document:
    extra_dependencies: ["float"]
  html_document: default
---
```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(mgcv)
library(ggplot2);theme_set(theme_bw())
library(FLCore)
library(FLa4a)
library(ggplotFL)
library(ggpubr)
drafting <- FALSE
```

# Introduction to Splines

Splines are a specific case of smoothers. A **smoother** is a method or algorithm designed to estimate a smooth function that fits data, capturing underlying trends without overfitting noise. Splines are a powerful tool for modeling complex, non-linear relationships between variables in a flexible and interpretable way. A common way to use splines is to break a function into smooth, continuous polynomial segments, each called a *piece* or *basis function*, joined at specific points called *knots*. This piecewise approach allows us to capture the non-linearity in the data without overfitting.

*Basis functions* consist the main building block of splines.The key concept of "basis functions" is that they transform the input variable (or vector) $\mathbf{x}$ into a set of new variables, which are then used as inputs in the model. This allows the model to remain linear in these transformed variables, even though it can capture complex, non-linear relationships in the original variable.

Splines build their functionality through the core concepts of linear models. Linear models assume that the relationship between data can be described by a straight line, in the case of only one predictor $\mathbf{x}$.
We denote linear model as:

$$\mathbf{y} = \beta_0 + \beta_1\mathbf{x} + \mathbf{\epsilon}$$

Where $\mathbf{y}$ are the observations, the parameters $\beta_0$ and  $\beta_1$ uniquely determine a straight line and $\mathbf{\epsilon}$ is the observation error. Simple in its construction and representation, linear models can be limited when trying to capture the complexity of a real-world data set. The simplest form of a linear model is the mean or average of a data set:

$$\mathbf{y} = \beta_0 + \epsilon$$

In the following graph of non linear data ($\mathbf{y}$), we fit a simple mean value of $\beta_0$ and a straight line model $\beta_0 + \beta_1\mathbf{x}$:

```{r, meanandlinear, fig.cap='Generated data with two linear fits, left a simple mean and right a straight line', fig.pos = 'H', fig.height = 4,echo=FALSE, message=FALSE, warning=FALSE}
set.seed(1234)
x = seq(0,10,0.1) + rnorm(101, 0, 1)
y = 3*sin(x) + rnorm(101, 0, 0.8) +x/2
data <- data.frame(x, y)

model <- lm(y ~ x, data = data)
data$predicted <- predict(model)
eq <- paste("Y =", round(coef(model)[1], 2), "+", round(coef(model)[2], 2), "X")
eq2 <- paste("E(Y) = ", round(mean(data$y),2))

p1 <- ggplot(data, aes(x, y)) +
  geom_point() +
  geom_line(aes(y = predicted), color = "blue")
  # annotate("text", x = 0, y = 10, label = eq, size = 4)

p2 <- ggplot(data, aes(x,y)) +
  geom_point() +
  geom_hline(yintercept = mean(data$y), color = "blue")
  # annotate("text", x = 0, y = 10, label = eq2, size = 4)

ggarrange(p2,p1)

```

In this case our parameters are a $\beta_0 =$ `r round(mean(data$y),2)` for the mean (left) model, while for the straight line model, $\beta_0 =$ `r round(coef(model)[1], 2)` and $\beta_1 =$ `r round(coef(model)[2], 2)`. In Figure \@ref(fig:meanandlinear) we see how by adding to the constant model $\beta_0$ a multiple ($\beta_1$) of the variable $\mathbf{x}$, the model becomes a little more complex but it can now follow the general upward trend compared to the first line, although it fails to follow the peaks and the lows of the data set.

One of the mechanisms that splines use is to split the set of values of the predictor, in our case $\mathbf{x}$, in smaller compartments and fit in those compartments a specified model. The points where the splitting occurs are called *knots*. In Figure \@ref(fig:knots) we take a stepwise approach following the logic behind the use of piecewise polynomials in smoothing splines. First, we split our range of $\mathbf{x}$ values in 4 subsets, by defining our knots, then we take the average of $\mathbf{y}$ data for each of these compartments and in the final step we add a bit of complexity by fitting a straight line model in each of these subsets of our original dataset. Figure X2 demonstrates a naive approach to try and follow better the trends of the data, where in each step we manage to capture a bit more.


```{r, knots, fig.cap='Effect of breaking the data in sections, by knots, and fitting the mean or a linear model to each section.', fig.pos = 'H', echo=FALSE, message=FALSE, warning=FALSE}
data <- data[order(x),]

p3 <- ggplot(data, aes(x, y)) +
  geom_point() +
  geom_vline(xintercept = 2, linetype = 'dashed')+
  geom_vline(xintercept = 6, linetype = 'dashed')+
  geom_vline(xintercept = 10, linetype = 'dashed')+
  scale_x_continuous(breaks = c(2,6,10))

p4 <- ggplot(data, aes(x, y)) +
  geom_point() +
  geom_vline(xintercept = 2, linetype = 'dashed')+
  geom_vline(xintercept = 6, linetype = 'dashed')+
  geom_vline(xintercept = 10, linetype = 'dashed')+
  geom_segment(x = -Inf, xend = 2, y = mean(data[data$x<2,]$y), yend = mean(data[data$x<2,]$y), color = 'blue')+
  geom_segment(x = 2, xend = 6, y = mean(data[data$x>=2 & data$x<6,]$y), yend = mean(data[data$x>=2 & data$x<6,]$y),
               color = 'blue')+
  geom_segment(x = 6, xend = 10, y = mean(data[data$x>=6 & data$x<10,]$y),
               yend = mean(data[data$x>=6 & data$x<10,]$y), color = 'blue')+
  geom_segment(x = 10, xend = Inf, y = mean(data[data$x>=10,]$y), yend = mean(data[data$x>=10,]$y), color = 'blue')+
  scale_x_continuous(breaks = c(2,6,10))

mod1 <- lm(y~x, data = data[data$x<2,])
mod2 <- lm(y~x, data = data[2<=data$x & data$x<6,])
mod3 <- lm(y~x, data = data[6<=data$x & data$x<10,])
mod4 <- lm(y~x, data = data[data$x>=10,])

data$predicted2 <- c(predict(mod1),
                     predict(mod2),
                     predict(mod3),
                     predict(mod4))

data$model  <- c(rep("lm1", length(predict(mod1))),
                 rep("lm2", length(predict(mod2))),
                 rep("lm3", length(predict(mod3))),
                 rep("lm4", length(predict(mod4)))
                 )

p5 <- ggplot(data, aes(x, y, group = model)) +
  geom_point() +
  geom_vline(xintercept = 2, linetype = 'dashed')+
  geom_vline(xintercept = 6, linetype = 'dashed')+
  geom_vline(xintercept = 10, linetype = 'dashed')+
  geom_path(aes(y = predicted2), color = "blue", na.rm = TRUE)+
  scale_x_continuous(breaks = c(2,6,10))

ggarrange(p3,p4,p5, ncol=3)

```


## Generalize to polynomials

We explored the concept of splitting the $\mathbf{x}$ variable space into compartments and developing a solution by locally fitting linear models that better capture the global trajectory of the data. Smooth functions rely on two fundamental mechanisms. The first, as mentioned earlier, involves how the domain of the function is divided for estimation. The second, which we will focus on here, involves using slightly more complex functions than linear ones, such as polynomials.

By combining polynomial pieces and ensuring smoothness at their junctions, i.e. knots, we can create flexible models that adapt to the data. In regression, splines are a powerful tool for fitting complex shapes by introducing non-linear trends while maintaining control over the smoothness of the overall function. This approach allows us to balance flexibility and precision in modeling.

The polynomials are build by transforming the predictor variable or the sets of variables into higher order polynomials (usually 2nd or 3rd grade polynomials). These polynomials need to have matching values at the knots.

Let $S$ our spline function, that is defined in an interval $[a,b]$. We seek to construct $S$ by combining $k-1$ polynomials $P$, where $k$ is the number of knots. Let also $t_{i}, i = 0, ..., k-1$ the positions of the knots in the interval $[a,b]$.

$S$ is going to be defined as:

$$
S(x) =
\begin{cases}
P_1(x) & \text{if } a \leq x < t_1, \\
P_2(x) & \text{if } t_1 \leq x < t_2, \\
\quad\vdots \\
P_{k-1}(x) & \text{if } t_{k-1} \leq x \leq b,
\end{cases}
$$



The above definition is a simplified version of how splines work and can help as an intuitive approach. In reality splines need to satisfy some extra conditions like continuity, i.e. $P_{i-1}(t_{i}) = P_{i}(t_{i})$ on the points of junction, and of the first and second derivative. Depending on the basis functions the conditions may differ.

In Figure \@ref(fig:naivespline) we demonstrate the fitting of cubic regression splines. In this case the highest order of the polynomials $P_i(x)$ are 3rd degree polynomials and it comprises 4 cubic segments, i.e basis functions.

The spline would be then as follows:

$$\mathbf{y} = \beta_0 + \beta_1P_1(\mathbf{x}) + \beta_2P_2(\mathbf{x}) + \beta_3P_3(\mathbf{x}) + \beta_4P_4(\mathbf{x})+\beta_5P_5(\mathbf{x}) $$


```{r, naivespline, fig.cap = "A B-spline with 5 knots consists of 5 cubic polynomials",echo=FALSE, message=FALSE, warning=FALSE, warning=FALSE}
library(splines)
spline <- bs(data$x, df = 5, degree = 2, intercept = FALSE)
cr <- lm(y ~ bs(data$x, df = 5, degree = 2, intercept = FALSE), data = data)
data$cr_fit <- fitted(cr)

sm_cr <- spline

data$knot_range <- cut(
  data$x,
  breaks = c(min(data$x), attr(sm_cr, "knots"), max(data$x)),
  labels = c("25%","50%","75%", "100%"),
  right = TRUE
)

ggplot(data, aes(x, y)) +
  geom_point() +
  geom_line(aes(y = cr_fit), color = "darkred") +
  geom_vline(xintercept = attr(sm_cr, "knots"), linetype = "dashed")
```


The B-spline fit above is constrained at the boundaries, by putting two of the five knots there, resulting in a linear behavior at the ends of the data range. This approach is helpful for data that has an approximately linear trend at the boundaries but exhibits non-linearity in the center.

The knots in this regression spline are placed by quantiles through the variable space, so in the case where the data are evenly spread across the variable space the knots will be placed evenly.

For the spline fitted above, there are $k$ polynomials plus an intercept (not shown) based on the knots (dashed lines). See Figure \@ref(fig:splinebasis) for a depiction of the basis functions.

```{r, splinebasis, fig.cap = "basis functions for a B-spline with 5 knots", echo=FALSE, message = FALSE, warning=FALSE}
X <- spline

df_x <- as.data.frame(X)
names(df_x) <- c("s(x).1", "s(x).2", "s(x).3", "s(x).4","s(x).5")

# multiply the coefficients with the basis splines
df_x$`s(x).1_b1` <- df_x$`s(x).1` * coef(cr)[2] + coef(cr)[1]
df_x$`s(x).2_b2` <- df_x$`s(x).2` * coef(cr)[3] + coef(cr)[1]
df_x$`s(x).3_b3` <- df_x$`s(x).3` * coef(cr)[4] + coef(cr)[1]
df_x$`s(x).4_b4` <- df_x$`s(x).4` * coef(cr)[5] + coef(cr)[1]
df_x$`s(x).5_b5` <- df_x$`s(x).5` * coef(cr)[6] + coef(cr)[1]

# split two dfs
df_x_basis <- df_x[,c(1:5)]
df_x_basis_b <- df_x[,c(6:10)]


df_xm_basis <- reshape2::melt(df_x_basis)
df_xm_basis$x <- data$x

df_xm_basis_b <- reshape2::melt(df_x_basis_b)
df_xm_basis_b$x <- data$x
df_xm_basis_b$fitted <- data$cr_fit
df_xm_basis_b$obs <- data$y

ggplot(data = df_xm_basis, aes(x = x, y = value, group = variable, colour = variable)) +
  geom_line() +
  theme(legend.position = "bottom") +
  geom_vline(xintercept = attr(sm_cr, "knots"), linetype = "dashed") +
  ylab("y")

```

The polynomials transform the initial variable $\mathbf{x}$ and create a *model* matrix $\mathbf{X}$ with $k$ columns and $n$ rows, where $n$ is the number of data points. This new transformation is being then used to fit the model and estimate the $\beta_{0}, ... ,\beta_{k}$ coefficients, $\beta_0$ is required for the intercept. The fitted model results from $\mathbf{X} \mathbf{\beta}$ (Figure \@ref(fig:splinebasisandmodel)).

```{r, splinebasisandmodel, fig.cap = "basis functions for a B-spline with 5 knots", echo=FALSE, message = FALSE, warning=FALSE}
ggplot(data = df_xm_basis_b, aes(x = x, y = value, group = variable, colour = variable)) +
  geom_line() + geom_line(aes(y = fitted), color = "darkgreen", linewidth = 1)+
  geom_point(aes(y = obs), color = "darkgrey", alpha = 0.4)+ theme(legend.position="bottom")+ylab("y")
```

## Thin plate spline

Thin plate splines are particularly useful for smoothing in multiple dimensions. However, they also work well with univariate data, as they offer flexibility and control over smoothness they are the default choice of the `mgcv` package.  Thin plate splines work differently from the splines we have shown so far. They are not composed of a sequence of local polynomials but from basis functions that are smooth across the entire range of the data, and capture increasing amounts of flexibility (Figure \@ref(fig:tps)).

```{r}
# Fit a thin plate spline with gam()
tps_model <- gam(y ~ s(x, k = 5, bs = "tp"), data = data)

# Predict
data$tps_fit <- predict(tps_model)

```

```{r, tps, fig.cap="Thin plate splines fit with k = 5", echo=FALSE, message = FALSE, warning=FALSE}
ggplot(data, aes(x, y)) +
  geom_point(alpha = 0.4) +
  geom_line(aes(y = tps_fit), color = "blue", linewidth = 1)
```

Thin plate splines are ideal when you need a smooth fit without predefined knots. The notion of knots in thin plate splines does not have the same interpretation as for B-splines and other piecewise functions, and infact it is likely not useful to think of knots when using thin plate spines. Instead it is better to think of the number of basis functions used to represent the smooth term (Figure \@ref(fig:tpsbasis)).

```{r, tpsbasis, fig.cap = "basis functions for thin plate splines", echo=FALSE, message = FALSE, warning=FALSE}
X <- model.matrix(tps_model)

df_x <- as.data.frame(X)

# multiply the coefficients with the basis splines
df_x$`(Intercept)_b0` <- df_x$`(Intercept)`*coef(tps_model)[1]
df_x$`s(x).1_b1` <- df_x$`s(x).1`*coef(tps_model)[2]
df_x$`s(x).2_b2` <- df_x$`s(x).2`*coef(tps_model)[3]
df_x$`s(x).3_b3` <- df_x$`s(x).3`*coef(tps_model)[4]
df_x$`s(x).4_b4` <- df_x$`s(x).4`*coef(tps_model)[5]

# split two dfs
df_x_basis <- df_x[,c(1:5)]
df_x_basis_b <- df_x[,c(6:10)]


df_xm_basis <- reshape2::melt(df_x_basis)
df_xm_basis$x <- data$x

df_xm_basis_b <- reshape2::melt(df_x_basis_b)
df_xm_basis_b$x <- data$x
df_xm_basis_b$fitted <- data$tps_fit
df_xm_basis_b$obs <- data$y

ggplot(data = df_xm_basis, aes(x = x, y = value, group = variable, colour = variable)) +
  geom_line()+theme(legend.position="bottom")+ylab("y")

```

As in the example before, the fitted model results from $X\beta$ (Figure \@ref(fig:tpsbasisandmodel)).

```{r, tpsbasisandmodel, fig.cap = "Basis functions for thin plate splines and the fitted model", echo=FALSE, message = FALSE, warning=FALSE}
ggplot(data = df_xm_basis_b, aes(x = x, y = value, group = variable, colour = variable)) +
  geom_line() +
  geom_line(aes(y = fitted), color = "darkgreen", linewidth = 1) +
  geom_point(aes(y = obs), color = "darkgrey", alpha = 0.4) +
  theme(legend.position = "bottom") +
  ylab("y") +
  ylim(-16, 15)
```

Figure \@ref(fig:spliesandtps) shows both models fitted to the dataset, both fits use the same number of knots.

**[Can we say a bit more here?]**

```{r, spliesandtps, fig.cap="Thin plate splines and cubic regression splines fit", echo=FALSE, message = FALSE, warning=FALSE}
ggplot(data, aes(x, y)) +
  geom_point(alpha = 0.4) +
  geom_line(aes(y = tps_fit), color = "blue", linewidth = 1) +
  geom_line(aes(y = cr_fit), color = "darkgreen", linewidth = 1)+
  labs(color = "Spline Type") +
  scale_color_manual(values = c("blue", "darkgreen"))
```

## The `mgcv` package inside `a4a`

```{r , include=FALSE}
data('ple4')
data('ple4.indices')
stk <- ple4
idx <- ple4.indices
```

The `mgcv` package provides various user input options to define the smoother functions used to construct the submodels.

Under the `a4a` framework, the `mgcv` package is used to construct the model matrices of the submodels, which are then passed to `ADMB` where the model fitting takes place.

The default option for the basis functions of the splines is `bs = tp` (thin plate splines) and is considered the optimal option. The user can define other smoothing basis functions using the `bs` argument. The user can refer to the `smooth.terms` of the `mgcv` package for a full description. There are many equivalent basis functions for the splines, and some of them have little or no effect in the context of `a4a`, since they differ only in the penalty term, which is not used in `a4a`.

Examples for different smoothing terms:

```{r, eval = !drafting}
fmod00 <- ~s(age)+s(year, bs = 'tp', k = 10) # thin plate splines
fmod01 <- ~s(age)+s(year, bs = 'cr', k = 10) # regression cubic splines
fmod02 <- ~s(age)+s(year, bs = 'bs', k = 10) # b-splines
fmod03 <- ~s(age)+s(year, bs = 'ps', k = 10) # p-splines
fmod04 <- ~s(age)+s(year, bs = 'ad', k = 10) # Adaptive smoothers

fit00 <- sca(stk, idx, fmodel = fmod00)
fit01 <- sca(stk, idx, fmodel = fmod01)
fit02 <- sca(stk, idx, fmodel = fmod02)
fit03 <- sca(stk, idx, fmodel = fmod03)
fit04 <- sca(stk, idx, fmodel = fmod04)
```

In this example we are using the thin plate regression splines, cubic splines, b-splines, p-splines and adaptive smoothers. Figure \@ref(fig:a4asplines) shows the different fits together, where it's clear the differences are very small.

```{r, a4asplines, eval = !drafting, fig.cap='Multiple fits of thin plate splines, cubic splines, b-splines, p-splines and adaptive smoothers', echo=FALSE, message = FALSE, warning=FALSE}
plot(FLStocks(ThinPlate = stk + fit00,
              CubicRegressionSplines = stk + fit01,
              B_splines = stk + fit02,
              P_splines = stk + fit03,
              Adaptive_smoothers = stk + fit04))+theme(legend.position = 'bottom')
```

Functionality of `mgcv` package provides also the option to work with interactions. Although `s(age, year)` can be defined, it uses a common bivariate spline for the two variables which are very different in scale. It is preferable if interactions are assumed to use a tensor `te(age, year)`. Tensors are mathematical products that help model the individual smoothness of each variable while also capturing their joint interaction. The independent variables in the case of tensors are modeled with different numbers of basis functions allowing different amount of smoothness in each dimension. 

Following the example above, let now $\mathbf{x}$ and $\mathbf{z}$ two independent variables with $S_x$ and $S_z$ their spline functions of dimension $k$ and $m$, respectively. Given the nature of the basis functions, their tensor (interaction and main effects), $Te(\mathbf{x},\mathbf{z})$ can be defined as the sum of all possible multiplications of the elements of $S_x$ and $S_z$.

Again is up to the user to define the basis functions for the tensor smoothers. Figure \@ref(fig:te) shows the differences when using different basis for the tensor product.

```{r, eval = !drafting}
fmod03 <- ~te(age, year, k = c(3,10))
fmod04 <- ~te(age, year, k = c(3,10), bs = 'cr')
fmod05 <- ~te(age, year, bs = 'bs')

fit03 <- sca(stk, idx, fmodel = fmod03)
fit04 <- sca(stk, idx, fmodel = fmod04)
fit05 <- sca(stk, idx, fmodel = fmod05)
```

```{r, te, eval = !drafting, fig.cap='Multiple basis for a tensor and their effects in the fit', echo=FALSE, message = FALSE, warning=FALSE}
plot(FLStocks(TP_tensor = stk + fit03,
              CB_tensor = stk + fit04,
              BS_tensor = stk + fit05))+theme(legend.position = 'bottom')
```

## On the number of knots $k$

$k$ is the dimension of the basis used to represent the smooth term $s$. The default depends on the number of variables that the smooth is a function of. In practice k-1 (or k) sets the upper limit on the degrees of freedom associated with an s smooth (1 degree of freedom is usually lost to the intercept of the smooth). For the smooths the upper limit of the degrees of freedom is given by the product of the k values provided for each marginal smooth less one, for the constraint. The choice of the k is not critical and in general it must be large enough to allow to have enough degrees of freedom capturing the underlying process and small enough to prevent overfitting (Figure \@ref(fig:ks)). A strong pattern in the residuals can be a sign of low $k$.

```{r, eval = !drafting}
fmod00 <- ~s(age)+s(year, k = 5)
fmod01 <- ~s(age)+s(year, k = 10) # cubic splines [Why is this cubic]
fmod02 <- ~s(age)+s(year, k = 20) # b-splines [and this b?]

fit00 <- sca(stk, idx, fmodel = fmod00)
fit01 <- sca(stk, idx, fmodel = fmod01)
fit02 <- sca(stk, idx, fmodel = fmod02)
```

```{r, ks, eval = !drafting, fig.cap='Multiple ks and their effects in the fit', echo=FALSE, message = FALSE, warning=FALSE}
plot(FLStocks('k=5' = stk + fit00,
              'k=10' = stk + fit01,
              'k=20' = stk + fit02))+theme(legend.position = 'bottom')
```

We can check the result of choosing different $k$ values on BIC (Bayesian Information Criteria) and GCV (Generalized Cross Validation score) by running the stock assessment model with different k and looking at the values of those stastics (Figure \@ref(fig:bic)).

```{r, eval = !drafting}
fmodsk <- list()
for(i in 10:20) {
  fmodsk[[paste0(i)]] <- as.formula(paste0("~s(age)+s(year, k=",i,")"))
}

myFits <- scas(FLStocks(stk), list(idx), fmodel = fmodsk)
```

```{r, bic, fig.cap="BIC (Bayesian Information Criteria) and GCV (Generalized Cross Validation score) profiles based on changing the value of k.", eval = !drafting, echo=FALSE, message = FALSE, warning=FALSE}
myFits <- scas(FLStocks(stk), list(idx), fmodel = fmodsk)
plot(myFits)
```

<!--chapter:end:06-splines.Rmd-->

# Fitting

```{r, echo=FALSE}
knitr::opts_chunk$set(warning = FALSE, message=FALSE)
```

The `a4a` stock assessment framework is implemented in `R` through the method `sca()`. The method call requires as a minimum a `FLStock` object and a `FLIndices` or `FLindex` object, in which case the default submodels will be set by the method.

Having described building blocks, basic formulations and effects available to build a submodel's model, it's important to look into specific formulations and relate them to commonly known representations. Note that although a large number of formulations are available for each submodel, the user must carefully decide on the full stock assessment model being build and avoid over-parameterize. Over-parametrization may lead to non-convergence, but may also end up not being very useful for prediction/forecasting, which is one of the main objectives of stock assessment.

```{r}
library(FLa4a)
data(ple4)
data(ple4.indices)
data(ple4.index)
fmod <- ~ s(age, k=8) + s(year, k=30) + te(age, year, k = c(5, 15))
fit <- sca(ple4, ple4.indices, fmodel=fmod)
stk <- ple4 + fit
```

```{r, plt01, fig.cap="Stock summary", echo=FALSE}
plot(stk)
```

Submodels that are not explicitly defined will be set by default using the relevant call to `defaultFmod()`, `defaultQmod`, `defaultSRmod()`, `defaultN1mod` or `defaultVmod()`. These methods will use the length of the time series and number of age groups to define the models. The `show` method for `a4aFit` objects display the models used for the fit.

```{r, echo=FALSE}
fit
```

To set specific submodels the user has to write the relevant `R` formula and include it in the call. The arguments for each submodel are self-explanatory: fishing mortality is 'fmodel', indices' catchability is 'qmodel', stock-recruitment is 'srmodel', observation variance is 'vmodel' and for initial year's abundance is 'n1model'. The following model comes closer to the official stock assessment of North Sea plaice, as such we'll name it `fit0` and keep it for future comparisons.

```{r, fit0}
fmod0 <- ~s(age, k=6)+s(year, k=10)+te(age, year, k=c(3,8))
qmod0 <- list(~s(age, k = 4),
       ~s(age, k = 3),
       ~s(age, k = 3) + year,
       ~s(age, k = 3),
       ~s(age, k = 4),
       ~s(age, k = 6))
srmod0 <- ~ s(year, k=20)
vmod0 <- list(~s(age, k=4), ~1,  ~1, ~1, ~1, ~1, ~1, ~1)
n1mod0 <- ~ s(age, k=3)
fit0 <- sca(ple4, ple4.indices,
       fmodel=fmod0,
       qmodel=qmod0,
       srmodel=srmod0,
       n1model=n1mod0,
       vmodel=vmod0)
stk0 <- ple4 + fit0
```

```{r, plt02, fig.cap="Stock summary - close to official assessment", echo=FALSE}
plot(stk0)
```

As before by calling the fitted object, the submodels' formulas are printed in the console:

```{r, echo=FALSE}
fit0
```

There are a set of methods for `a4a` fit objects which help manipulating `sca()` results, namely `+`, that updates the stock object with the fitted fishing mortalities, population abundance and catch in numbers at age. This method can be applied to `FLStocks` and `a4aFits` objects as well.

The following subsections describe common formulations used to define submodels. Although the formulas are tailored to specific submodels — *e.g.*, a separable model for $F$ — they can, in principle, be applied to any submodel. The `sca` method is agnostic to the model setup and will attempt to fit the model regardless of its specification. However, from a statistical standpoint, convergence may fail if the model is not well specified. From a fisheries modeling perspective, limitations arise in how the model is interpreted. For example, if a scientific survey is modeled with a year effect, the user is implicitly assuming that the survey’s selectivity has changed over time. Consequently, the model may attribute part of the observed trend in the survey data to changes in selectivity rather than to changes in abundance.

## Fishing mortality submodel ($F_{ay}$)

### Separable model

One of the most useful models for fishing mortality is one in which 'age' and 'year' effects are independent, that is, where the shape of the selection pattern does not change over time, but the overall level of fishing mortality do. Commonly called a 'separable model'.

A full separable model in `a4a` is written using the `factor` function which converts age and year effects into categorical values, forcing a different coefficient to be estimated for each level of both effects. This model has $age x year$ number of parameters.

```{r}
fmod1 <- ~ factor(age) + factor(year)
fit1 <- sca(ple4, ple4.indices, fmodel=fmod1, fit="MP")
```

One can reduce the number of parameters and add dependency along both effects, although still keeping independence of each other, by using smoothers rather than `factor`. We're using the North Sea Plaice data, and since it has 10 ages we will use a simple rule of thumb that the spline should have fewer than $\frac{10}{2} = 5$ degrees of freedom, and so we opt for 4 degrees of freedom. We will also do the same for year and model the change in $F$ through time as a smoother with 20 degrees of freedom.

```{r}
fmod2 <- ~ s(age, k=4) + s(year, k=20)
fit2 <- sca(ple4, ple4.indices, fmodel=fmod2, fit="MP")
```

An interesting extension of the separable model is the 'double separable' where a third factor or smoother is added for the cohort effect.

```{r, warning=FALSE}
fmod3 <- ~ s(age, k=4) + s(year, k=20) + s(as.numeric(year-age), k=10)
fit3 <- sca(ple4, ple4.indices, fmodel=fmod3, fit="MP")
```

Figures \@ref(fig:sep00) and \@ref(fig:sep01) depicts the three models selectivities for each year. Each separable model has a single selectivity that changes it's overall scale in each year, while the double separable introduces some variability over time by modeling the cohort factor.

```{r, sep00, fig.cap="Selection pattern of separable models. Each line represents the selection pattern in a specific year. Independent age and year effects (factor), internally dependent age and year (smooth), double separable (double).", echo=FALSE}
flqs <- FLQuants(factor=harvest(fit1), smooth=harvest(fit2), double=harvest(fit3))
pset <- list(strip.background=list(col="gray90"))
xyplot(data~age|qname, groups=year, data=flqs, type="l", col=1, layout=c(3,1), ylab="fishing mortality", par.settings=pset)
```

```{r, sep01, fig.cap="Fishing mortality of separable models. Independent age and year effects (factor), internally dependent age and year (smooth), double separable (double).", echo=FALSE}
wireframe(data~age+year|qname, data=as.data.frame(flqs), layout=c(3,1))
```

### Model with age-year interaction

A non-separable model, where we consider age and year to interact can be modeled by a smooth interaction term with a tensor product of cubic splines, the `te` method (Figure \@ref(fig:te1)), again borrowed from `mgcv` (@R-mgcv).

```{r}
fmod <- ~ te(age, year, k = c(4,20))
fit <- sca(ple4, ple4.indices, fmodel=fmod)
```

```{r, te1, fig.cap="Fishing mortality smoothed non-separable model", echo=FALSE}
wireframe(harvest(fit), zlab="F")
```

In this example fishing mortalities are linked across age and time. What if we want to free up a specific age class because in the residuals we see a consistent pattern. This can happen, for example, if the spatial distribution of juveniles is disconnected to the distribution of adults.  The fishery focuses on the adult fish, and therefore the $F$ on young fish is a function of the distribution of the juveniles and could deserve a specific model. This can be achieved by adding a component for the year effect on age 1 (Figure \@ref(fig:age1)). We'll use `s`'s argument `by` to define the ages that the model will apply to. The `as.numeric` method over `age==1`, will result in a matrix that will be $1$ for ages 1 and $0$ for the other ages, effectively removing those ages from the `s` model. Furthermore, by not removing age 1 from the `te` component we're in effect adding the two estimates for age 1.

```{r}
fmod <- ~ te(age, year, k = c(4,20)) + s(year, k = 5, by = as.numeric(age==1))
fit2 <- sca(ple4, ple4.indices, fmodel=fmod)
```

```{r, age1, fig.cap="Fishing mortality age-year interaction model with extra age 1 smoother.", echo=FALSE}
wireframe(harvest(fit2), zlab="F")
```

### Constant selectivity for contiguous ages or years

To set these models we'll use the method `replace()` to define which ages or years will be modelled together. The following example shows `replace()` in operation. The dependent variables used in the model will be changed and attributed the same age or year, as such during the fit observations of those ages or years with will be seen as replicates. One can think of it as sharing the same mean value, which will be estimated by the model.

```{r}
age <- 1:10
# last age same as previous
replace(age, age>9, 9)
# all ages after age 6
replace(age, age>6, 6)
year <- 1950:2010
replace(year, year>2005, 2005)
```

In the $F$ submodel one can use this method to fix the estimation of $F$ in the plus group to be the same as in the last non-aggregated age.

```{r}
fmod <- ~ s(replace(age, age>9, 9), k=4) + s(year, k=20)
fit <- sca(ple4, ple4.indices, fmod)
```

```{r, ctsselage, fig.cap="F-at-age fixed above age 9", echo=FALSE}
wireframe(harvest(fit), zlab="F")
```

Or estimate the average $F$ in the most recent years, instead of averaging after the assessment to compute the _statu quo_ selection pattern.

```{r}
fmod <- ~ s(age, k=4) + s(replace(year, year>2013, 2013), k=20)
fit <- sca(ple4, ple4.indices, fmod)
```

```{r, ctsselyear, fig.cap="F-at-age fixed for the most recent 5 years", echo=FALSE}
wireframe(data~age+year, data=harvest(fit), screen=c(z=-130, y=0, x=-60), zlab="F")
```

### Time blocks selectivity

To define blocks of data `sca()` uses the method `breakpts()`, which creates a factor from a vector with levels defined by the second argument.

```{r}
year <- 1950:2010
# two levels separated in 2000
breakpts(year, 2000)
# five periods with equal interval
breakpts(year, seq(1949, 2010, length=6))
```

Note `seq()` computes 'left-open' intervals, which means that to include 1950 the sequence must start one year earlier.

These methods can be used to create discrete time series, for which a different selection pattern is allowed in each block. This is called an interaction in statistical modelling parlance, and typically a `*` denotes an interaction term, for smoothers an interaction is achieved using the `by` argument. When this argument is a `factor` a replicate of the smooth is produced for each factor level.

In the next case we'll use the `breakpts()` to split the time series at 1990, although keeping the same shape in both periods, a thin plate spline with 3 knots (Figure \@ref(fig:brk)).

```{r}
fmod <- ~s(age, k = 3, by = breakpts(year, 1990))
fit <- sca(ple4, ple4.indices, fmod)
```

```{r, brk, echo=FALSE, fig.cap="F-at-age in two periods using in both cases a thin plate spline with 3 knots", echo=FALSE}
wireframe(harvest(fit), zlab="F")
```

### Time changing selectivity

In many cases, it may be desirable to allow the selection pattern to evolve over time, from year to year. Again there are several ways to do this, one way is to estimate a mean selection pattern, while also allowing $F$ to vary over time for each age. This is like a separable smoother over year, with 'age blocks' so, looking back at previous examples, we have:

```{r}
fmodel <- ~ s(year, k = 15, by = factor(age)) + s(age, k = 4)
```

This is a type of interaction between age and year, but the only connection (or correlation) across ages is via the smoother on age, however there are still 15 degrees of freedom for each age, so the model 10 x 15 + 4 = 154 degrees of freedom.

To include correlation across ages and years together the tensor product (`te()`) is used, this has the effect of restricting the flexibility of the model for $F$. In the following, there is a smoother in 2 dimensions (age and year) where there is 5 degrees of freedom in the age direction, and 15 in the year dimension, resulting in a total of 5 x 15 = 65 degrees of freedom

```{r}
fmodel <- ~ te(age, year, k = c(5, 15))
```

Often the above formulations provide too much flexibility, and a more complicated specification, but simpler model is preferable:

```{r}
fmodel <- ~ s(age, k = 4) + s(year, k = 15) + te(age, year, k = c(3, 5))
```

in the above model, the main effects for age and year still have similar flexibility to the full tensor model, however, the interaction (or the change in F at age over time) has been restricted, so that the full model now has 4 + 15 + 3 x 5 = 34 degrees of freedom.

### Closed form selection pattern

One can use a closed form for the selection pattern. The only requirement is to be able to write it as a `R` formula, the example below uses a logistic form.

```{r}
fmod <- ~ I(1/(1+exp(-age)))
fit <- sca(ple4, ple4.indices, fmod)
```

```{r, logistic, fig.cap="F-at-age logistic", echo=FALSE}
wireframe(harvest(fit), zlab="F")
```

## Abundance indices catchability submodel ($Q_{ays}$)

The catchability submodel is set up the same way as the $F$ submodel. The only difference is that the submodel is set up as a list of formulas, where each formula relates with one abundance index. There's no limitation in the number of indices or type that can be used for a fit. It's the analyst that has to decide based on her/his expertise and knowledge of the stock and fleet dynamics.

In the following examples we'll use a single index instead of all available indices for plaice in ICES area 4, to simplify the code and examples.

### Catchability submodel for age based indices

The first model shown is simply a dummy effect on age, which means that one coefficient will be estimated for each age. Note this kind of model considers each level of the factor to be independent from the others levels (Figure \@ref(fig:dummyage)).

```{r}
qmod <- list(~factor(age))
fit <- sca(ple4, ple4.index, qmodel=qmod)
```

```{r, dummyage, fig.cap="Catchability age independent model", echo=FALSE}
qhat <- predict(fit)$qmodel[[1]]
wireframe(qhat, zlab="q")
```

If one considers catchability at a specific age to be dependent on catchability on the other ages, similar to a selectivity modelling approach, one option is to use a smoother at age, and let the data 'speak' regarding the shape (Figure \@ref(fig:smoothage)).

```{r}
qmod <- list(~ s(age, k=4))
fit <- sca(ple4, ple4.indices[1], qmodel=qmod)
```

```{r, smoothage, fig.cap="Catchability smoother age model", echo=FALSE}
qhat <- predict(fit)$qmodel[[1]]
wireframe(qhat, zlab="q")
```

Finally, one may want to investigate a trend in catchability with time, very common in indices built from CPUE data. In the example given here we'll use a linear trend in time, set up by a simple linear model (Figure \@ref(fig:qtrend)).

```{r}
qmod <- list( ~ s(age, k=4) + year)
fit <- sca(ple4, ple4.indices[1], qmodel=qmod)
```

```{r, qtrend, fig.cap="Catchability with a linear trend in year", echo=FALSE}
qhat <- predict(fit)$qmodel[[1]]
wireframe(qhat, zlab="q")
```

### Catchability submodel for age aggregated biomass indices

The previous section focused on age-disaggregated indices, which are most often reported as standardized number of individuals, *e.g.* number of individuals caught per hour. Age-aggregated indices (such as CPUE, biomass, DEPM, etc.) may also be used to tune the population's biomass in terms of weight. These indices are linked either to the total biomass or to the weight of a specific group of age classes, defined by the age range set in the object.

In such cases, a different index class must be used: FLIndexBiomass. This class uses a vector named index with an age dimension labeled as 'all'. The qmodel should be specified without age-specific factors, although it may still include a 'year' component and relevant covariates, if needed.

```{r}
# simulating a biomass index (note the name of the first dimension element) using
# the ple4 biomass and an arbritary catchability of 0.001 plus a lognormal error.
dnms <- list(age="all", year=range(ple4)["minyear"]:range(ple4)["maxyear"])
bioidx <- FLIndexBiomass(FLQuant(NA, dimnames=dnms))
index(bioidx) <- stock(ple4)*0.001
index(bioidx) <- index(bioidx)*exp(rnorm(index(bioidx), sd=0.1))
range(bioidx)[c("startf","endf")] <- c(0,0)
# note the name of the first dimension element
index(bioidx)
# fitting the model
fit <- sca(ple4, FLIndices(bioidx), qmodel=list(~1))
```

To estimate a constant selectivity over time one used the model $\sim 1$, resulting in the following estimate:

```{r}
predict(fit)$qmodel[[1]][1,drop=TRUE]
```

The next code shows an example where the biomass index refers to age groups 2 to 4, *e.g.* the CPUE of a fleet that targets these particular ages.

```{r}
# creating the index
dnms <- list(age="all", year=range(ple4)["minyear"]:range(ple4)["maxyear"])
bioidx <- FLIndexBiomass(FLQuant(NA, dimnames=dnms))
# but now use only ages 2:4
index(bioidx) <- tsb(ple4[ac(2:4)])*0.001
index(bioidx) <- index(bioidx)*exp(rnorm(index(bioidx), sd=0.1))
range(bioidx)[c("startf","endf")] <- c(0,0)
# to pass this information to the model one needs to specify an age range
range(bioidx)[c("min","max")] <- c(2,4)
# fitting the model
fit <- sca(ple4, FLIndices(bioidx), qmodel=list(~1))
```

Once more the estimate value is not very far from the simulated one, 0.001.

```{r}
predict(fit)$qmodel[[1]][1,drop=TRUE]
```

### Catchability submodel for single age indices

Similar to age aggregated indices one may have an index that relates only to one age, like a recruitment index. In this case the `FLIndex` object must have in the first dimension the age it refers to. The fit uses the index to tune the population abundance for the specific age. As for biomass indices, the qmodel should be set without age factors, although it can have a 'year' component and covariates if needed.

```{r}
idx <- ple4.index[1]
fit <- sca(ple4, FLIndices(recidx=idx), qmodel=list(~1))
# the estimated catchability is
predict(fit)$qmodel[[1]][1,drop=TRUE]
```

## Stock-recruitment submodel ($R_y$)

The S/R submodel is a special case, in the sense that it can be set up with the same linear tools as the $F$ and $Q$ models, but it can also use some hard coded models. The example shows how to set up a simple dummy model with `factor()`, a smooth model with `s()`, a Ricker model (`ricker()`), a Beverton and Holt model (`bevholt()`), a hockey stick model (`hockey()`), and a geometric mean model (`geomean()`). See Figure \@ref(fig:srmod) for results. As mentioned before, the 'structural' models have a fixed variance, which must be set by defining the coefficient of variation.

```{r}
srmod <- ~ factor(year)
fit <- sca(ple4, ple4.indices, srmodel=srmod)
srmod <- ~ s(year, k=15)
fit1 <- sca(ple4, ple4.indices, srmodel=srmod)
srmod <- ~ ricker(CV=0.1)
fit2 <- sca(ple4, ple4.indices, srmodel=srmod)
srmod <- ~ bevholt(CV=0.1)
fit3 <- sca(ple4, ple4.indices, srmodel=srmod)
srmod <- ~ hockey(CV=0.1)
fit4 <- sca(ple4, ple4.indices, srmodel=srmod)
srmod <- ~ geomean(CV=0.1)
fit5 <- sca(ple4, ple4.indices, srmodel=srmod)
```

```{r, srmod, fig.cap="Recruitment estimates since 1960 by each stock-recruitment model.", echo=FALSE}
flqs <- FLQuants(factor=stock.n(fit)[1], smother=stock.n(fit1)[1], ricker=stock.n(fit2)[1], bevholt=stock.n(fit3)[1], hockey=stock.n(fit4)[1], geomean=stock.n(fit5)[1])
flqs <- lapply(flqs, "[", j=ac(1960:2017))
xyplot(data~year, groups=qname, data=flqs, type="l", auto.key=list(points=FALSE, lines=TRUE, columns=3), ylab="No. recruits")
```

## Observation variance submodel ($\{\sigma^2_{ay}, \tau^2_{ays}\}$)

The variance model allows the user to set up the shape of the observation variances $\sigma^2_{ay}$ and $\tau^2_{ays}$. This is an important subject for fisheries data used as input to stock assessment models.

The defaults assume a U-shape like model for catch-at-age and constant variance for abundance indices. The first relies on the fact that it's common to have more precision on the most represented ages and less precision on the less frequent ages which tend to  be the younger and older individuals. These sizes are less caught by the fleets and as such do not appear as often at the auction markets samples. With regards to the abundance indices, one assumes a scientific survey to have a well designed sampling scheme and protocols which keep observation error at similar levels across ages.

```{r}
# reference model with constant variance for the survey index
vmod <- list(~s(age, k=3), ~1)
fit1 <- sca(ple4, ple4.index, vmodel=vmod)
# to compare - survey index variance modelled has a U-shape smoother
vmod <- list(~s(age, k=3), ~s(age, k=3))
fit2 <- sca(ple4, ple4.index, vmodel=vmod)
```

Variance estimated for the survey is constant at `r round(predict(fit)$vmodel[[2]][1,drop=TRUE],3)` while for catches using the U-shape model, fitted with a smoother, changes with ages (Figure \@ref(fig:vmod)).

```{r, vmod, fig.cap="Abundance index observation variance estimate", echo=FALSE}
wireframe(predict(fit1)$vmodel[[1]], zlab="variance")
```

Observation variance options have an impact in the final estimates of population abundance, which can be seen in Figure \@ref(fig:vmodimpact).

```{r, vmodimpact, fig.cap="Population estimates using two different variance models for the survey", echo=FALSE}
flqs <- FLQuants(constant=stock.n(window(fit1, start=1990)), smoother=stock.n(window(fit2, start=1990)))
xyplot(data~year|factor(age), groups=qname, data=flqs, type="l",
       scales=list(y=list(relation="free", draw=FALSE)),
       auto.key=list(points=FALSE, lines=TRUE, columns=2),
       par.settings=list(superpose.line=list(col=c("red", "blue"), lwd=1.5),
       strip.background=list(col="gray90")), ylab="", layout=c(5,2))
```

## Initial year abundance submodel ($N_{a,y=1}$)

The submodel for the stock number at age in the first year of the time series is set with the usual tools. The model deals with the shape of the population abundance in a single year and as such the year effect shouldn't be included (Figure \@ref(fig:ny1)).

This model has its influence limited to the initial lower triangle of the population matrix, which in assessments with long time series doesn't make much difference. Nevertheless, when modelling stocks with short time series in relation to the number of ages present, it becomes more important and should be given proper attention.

```{r}
# model with smoother
n1mod <- ~s(age, k=4)
fit1 <- sca(ple4, ple4.indices, n1model=n1mod)
# model with factor
n1mod <- ~factor(age)
fit2 <- sca(ple4, ple4.indices, n1model=n1mod)
```

```{r, ny1, fig.cap="Nay=1 models", echo=FALSE}
flqs <- FLQuants(smother=stock.n(fit1)[,1], factor=stock.n(fit2)[,1])
pset <- list(superpose.line=list(col=c("red", "blue")))
lgnd <- list(points=FALSE, lines=TRUE, space='right')
xyplot(data~age, groups=qname, data=flqs, type="l", auto.key=lgnd, par.settings=pset, ylab="")
```

The impact in the overall perspective of the stock status is depicted in Figure \@ref(fig:n1modimpact). Most of the changes happen in the beggining of the time series, although due to the impact on the estimates of other submodels' parameters it can have an impact over the full time series.

```{r, n1modimpact, fig.cap="Population estimates using two different variance models", echo=FALSE}
flqs <- FLQuants(smother=stock.n(fit1), factor=stock.n(fit2))
xyplot(data~year|factor(age), groups=qname, data=flqs, type="l",
       scales=list(y=list(relation="free", draw=FALSE)),
       auto.key=list(points=FALSE, lines=TRUE, columns=2),
       par.settings=list(superpose.line=list(col=c("red", "blue"), lwd=1.5),
       strip.background=list(col="gray90")), ylab="", layout=c(5,2))
```

## More models

More complicated models can be built with these tools. The limitation is going to be the potential overparametrization of the model and the failure to fit if the data isn't informative enough.

For example, Figure \@ref(fig:ageind) shows a model where the age effect is modelled as a smoother throughout years independent from each other, with the exception of ages 9 and 10 which share their parameters.

```{r}
fmod <- ~ factor(age) + s(year, k=10, by = breakpts(age, c(0:8)))
fit <- sca(ple4, ple4.indices, fmod)
```

```{r, ageind, fig.cap="F-at-age as thin plate spline with 3 knots for each age", echo=FALSE}
wireframe(harvest(fit), zlab="F")
```

A quite complex model that implements a cohort effect can be set through the following formula. Figure \@ref(fig:coh) shows the resulting fishing mortality. Note that in this case we end up with a variable $F$ pattern over time, but rather than using 4 * 10 = 40 parameters, it uses, 4 + 10 + 10 = 24.

```{r}
fmodel <- ~ s(age, k = 4) + s(pmax(year - age, 1957), k = 10) + s(year, k = 10)
fit <- sca(ple4, ple4.indices, fmodel=fmodel)
```

```{r, coh, echo=FALSE, fig.cap="F-at-age with a cohort effect.", echo=FALSE}
wireframe(harvest(fit), zlab="F")
```

The following model is applied to the vmodel and it introduces an time trend to reflect the increase in precision in more recent years with improvements in sampling design and increase in sampling effort.

```{r}
vmod <- list(
       ~ s(age, k = 3) + year,
       ~1, ~1, ~1, ~1, ~1, ~1
       )
fit <- sca(ple4, ple4.indices, vmodel=vmod)
```

```{r, vm, echo=FALSE, fig.cap="Catch at age variance model with a year effect.", echo=FALSE}
wireframe(predict(fit)$vmodel[[1]], zlab="variance")
```

This model fits two smoothers to different sets of ages.

```{r}
fmod <- ~s(age, k = 3, by = breakpts(age, 5)) + s(year, k = 10)
fit <- sca(ple4, ple4.indices, fmodel = fmod)
```

```{r, danai01, echo=FALSE, fig.cap="Smoothers fitted to two sets of ages, 1 to 4 and 5 to 10.", echo=FALSE}
wireframe(harvest(fit), zlab="F")
```

<!--## Data weigthing

By default the likelihood components are not weighted and the contribution of each to the maximum likelihood depends on their own likelihood score. However, the user may change these weights by penalizing data points, the $w_{ays}$ in section \@ref(sec:maths). The likelihood score of each data point will be multiplied by the normalized weights ($\sum w_{ays} = 1$). This is done by adding a variance matrix to the `catch.n` and `index.n` slots of the stock and index objects. The values should be given as coefficients of variation on the log scale, so that variance is $\log{({CV}^2 + 1)}$. Figures \@ref(fig:likwgt) and \@ref(fig:likwgtimpact) show the results of the two fits in the population abundance and stock summary.-->

```{r, eval=FALSE, echo=FALSE}
stk <- ple4
idx <- ple4.indices[1]
# cv of observed catches
varslt <- catch.n(stk)
varslt[] <- 0.4
catch.n(stk) <- FLQuantDistr(catch.n(stk), varslt)
# cv of observed indices
varslt <- index(idx[[1]])
varslt[] <- 0.2
index.var(idx[[1]]) <- varslt
# run
fit1 <- sca(stk, idx, fmodel=fmod0, qmodel=qmod0, srmodel=srmod0, vmodel=vmod0, n1model=n1mod0)
flqs <- FLQuants(nowgt=stock.n(fit0), extwgt=stock.n(fit1))
```

```{r, likwgt, fig.cap="Stock summary of distinct likelihood weightings", echo=FALSE, eval=FALSE}
xyplot(data~year|factor(age), groups=qname, data=flqs, type="l", scales=scl, auto.key=lgnd, par.settings=pset, ylab="")
```

```{r, likwgtimpact, fig.cap="Population estimates using two different variance models", eho=FALSE, eval=FALSE}
flsts <- FLStocks(nowgt=ple4+fit0, wgt=ple4 + fit1)
plot(flsts)
```

<!--Note that by using a smaller CV for the index, one is increasing the contribution of the survey and penalizing catch at age, in relative terms. The ratio between likelihood scores of both fits show this effect with catch at age increasing by 2.3 while the index increases almost 8 fold.-->

```{r, eval=FALSE, echo=FALSE}
fit0 <- sca(ple4, ple4.indices[1], fmodel=fmod0, qmodel=qmod0, srmodel=srmod0, vmodel=vmod0, n1model=n1mod0)
(fitSumm(fit1)/fitSumm(fit0))[c(2,8,9),]
```

## Working with covariates

In linear model one can use covariates to explain part of the variance observed on the data that the 'core' model does not explain. The same can be done in the `a4a` framework. The example below uses the North Atlantic Oscillation (NAO) index to model recruitment.

```{r}
nao <- read.table("https://www.cpc.ncep.noaa.gov/products/precip/CWlink/pna/norm.nao.monthly.b5001.current.ascii.table", skip=1, fill=TRUE, na.strings="-99.90")
dnms <- list(quant="nao", year=1950:2024, unit="unique", season=1:12, area="unique")
nao <- FLQuant(unlist(nao[,-1]), dimnames=dnms, units="nao")
nao <- seasonMeans(trim(nao, year=dimnames(stock.n(ple4))$year))
```

First by simply assuming that the NAO index drives recruitment (Figure \@ref(fig:naor)).

```{r}
srmod <- ~ s(nao, k=10)
fit2 <- sca(ple4, ple4.indices[1], qmodel=list(~s(age, k=4)), srmodel=srmod, covar=FLQuants(nao=nao))
```

```{r, naor, echo=FALSE, fig.cap="Recruitment model with covariates. Using the NAO index as a recruitment index.", echo=FALSE}
flqs <- FLQuants(simple=stock.n(fit)[1], covar=stock.n(fit2)[1])
xyplot(data~year, groups=qname, data=flqs, type="l",
       auto.key=list(points=FALSE, lines=TRUE, columns=2),
       par.settings=list(superpose.line=list(col=c("red", "blue"), lwd=1.5),
       strip.background=list(col="gray90")), ylab="")
```

In a second model we're using the NAO index not to model recruitment directly but to model one of the parameters of the S/R function (Figure \@ref(fig:naor2)).

```{r}
srmod <- ~ ricker(a=~nao, CV=0.25)
fit3 <- sca(ple4, ple4.indices[1], qmodel=list(~s(age, k=4)), srmodel=srmod, covar=FLQuants(nao=nao))
```

```{r, naor2, echo=FALSE, fig.cap="Recruitment model with covariates. Using the NAO index as a covariate for the stock-recruitment model parameters.", echo=FALSE}
flqs <- FLQuants(simple=stock.n(fit)[1], covar=stock.n(fit3)[1])
xyplot(data~year, groups=qname, data=flqs, type="l",
       auto.key=list(points=FALSE, lines=TRUE, columns=2),
       par.settings=list(superpose.line=list(col=c("red", "blue"), lwd=1.5),
       strip.background=list(col="gray90")), ylab="")
```

Note that covariates can be added to any submodel using the linear model capabilities of `R`.

## Assessing `ADMB` files

The framework gives access to all files produced to run the `ADMB` fitting routine through the argument `wkdir`. When set up, all the `ADMB` files will be left in the directory. Note that the `ADMB` tpl file is distributed with the `FLa4a`. One can get it from your `R` library, under the folder `[myRlib]/FLa4a/admb/`.

```{r, eval=FALSE}
fit1 <- sca(ple4, ple4.indices, wkdir="fit1run")
```

## Missing observations in the catch matrix or index

Missing observations are encoded as `NA`, and usually occur if there was no sampling for a year, or, since we model observations on the log scale, if the observation was zero. The `a4a` framework can deal with missing observations in the catches and indices.

The example below shows how to set up a model with missing observations in the catch matrix, to demonstrate the effect of missing observations, using the default model settings.

```{r missing obs}
fit <- sca(ple4, ple4.indices)
ple4_missing <- ple4
catch.n(ple4_missing)[ac(1:2), "2013"] <- NA
fit_missing <- sca(ple4_missing, ple4.indices)
```

In effect, the information on $F$ and $Q$ for the missing observations is inferred from the structural assumptions of the model. If a separable $F$ model is used, the value of $F$ at a given age is derived from its relationship with $F$ at other ages in the same year, as well as from the temporal relationship of $F$ across years for ages with available data. The same principle applies to any other submodel.

The impact of missing observations is illustrated in Figure \@ref(fig:obsmissing), which shows box plots of the predicted catch at age, incorporating estimation error. When observations are missing, the resulting estimates for those ages are both different and more uncertain. Additionally, the estimates for nearby years are affected, although the influence of the missing data diminishes with time—estimates from years further away converge toward those obtained using the full dataset.

```{r, obsmissing, echo = FALSE, fig.cap="Stock estimates with missing observations."}
pred <- ple4 + simulate(fit, 1000)
pred_missing <- ple4 + simulate(fit_missing, 1000)
flqs <- FLQuants(base = catch.n(pred)[ac(1:2), ac(2011:2015)], missing = catch.n(pred_missing)[ac(1:2), ac(2011:2015)])
bwplot(data ~ qname | factor(year) + factor(age), data = as.data.frame(flqs), scales = "free", auto.key = T, ylab = "Catch at age", layout=c(5,2), par.settings=list(box.rectangle = list(col = "black"), box.umbrella = list(col = "black"), plot.symbol = list(col = "black")))
```

This is a simple example, but the same principle applies to more complex models. However, if there are many missing observations, the model cannot be too flexible; otherwise, it won’t be able to reliably estimate the missing data.

In any case, one can always add more structure to the model to help address missing information. A common approach is to include a stock-recruitment relationship, which links the spawning stock biomass to recruitment. The example above would definitely benefit from this approach, as the missing information pertains to the first age group. See Figure \@ref(fit:obsmissing2), estimates are much more similar, although estimates from the fit to the missing data dataset show more uncertainty, as expected.

```{r}
# bevholt s/r CV was tweaked to give best results for the example
fit2 <- sca(ple4, ple4.indices, srmodel=~bevholt(CV=0.16))
fit_missing2 <- sca(ple4_missing, ple4.indices, srmodel=~bevholt(CV=0.16))
```

```{r, obsmissing2, echo = FALSE, fig.cap="Stock estimates with missing observations."}
pred <- ple4 + simulate(fit2, 1000)
pred_missing <- ple4 + simulate(fit_missing2, 1000)
flqs2 <- FLQuants(base = catch.n(pred)[ac(1:2), ac(2011:2015)], missing = catch.n(pred_missing)[ac(1:2), ac(2011:2015)])
bwplot(data ~ qname | factor(year) + factor(age), data = as.data.frame(flqs2), scales = "free", auto.key = T, ylab = "Catch at age", layout=c(5,2), par.settings=list(box.rectangle = list(col = "black"), box.umbrella = list(col = "black"), plot.symbol = list(col = "black")))
```

Another point to note, is that if observations are systematically missing, for example due to the actual observation being below a detection limit, or zero, then the model may overestimate the true catch at age. This is a common problem in stock assessment models, and is not unique to the `a4a` framework. Proposed solutions to this issue are to replace zeros with a small number, or half of the smallest observed value.

<!--chapter:end:07-fitting.Rmd-->

# Diagnostics \label{sec:diagn}

Statistical model diagnostics are essential to identify potential issues in the model, such as violations of assumptions, outliers, and influential data points. Without proper diagnostics, fitted models may provide misleading conclusions due to violated assumptions or undetected anomalies. For instance, residual analysis is a widely used diagnostic method that assesses the discrepancy between observed and fitted values, helping to validate underlying model assumptions [@hickey2019]

The `a4a` framework implements several analysis of residuals, visualizations and statistics, that can be used to evaluate the fit quality and chose across multiple fits.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(FLa4a)
data(ple4)
data(ple4.indices)
```

For demonstration purposes we'll use the plaice in ICES area 4 stock with a plus group at age 9, a single index "BTS-Combined (all)", and trimmed time series to the most recent 15 years.

```{r}
# use single indices and set plus group at 9
idx <- ple4.indices[c("BTS-Combined (all)")]
idx[[1]] <- idx[[1]][1:9]
stk <- setPlusGroup(ple4, 9)
iy <- 2003
# fit
fmod <- ~ s(age, k = 8) + s(year, k = 29) + te(age, year, k = c(6, 15))
qmod <- list(~factor(age))
n1mod <- ~s(age, k=5)
vmod <- list(~s(age, k=3), ~s(age, k=3))
fit <- sca(stk, idx, fmodel=fmod, qmodel=qmod, n1model=n1mod, vmodel=vmod)
```

## Residuals

Residuals are a ubiquos metrics to check quality of a fit. For `sca()` fits there are out-of-the-box methods to compute in the log scale, raw residuals (aka deviances), standardized residuals and pearson residuals. A set of plots to inspect residuals and evaluate fit quality and assumptions are implemented.

Consider $x_{ay}$ to be either a catch-at-age matrix ($C_{ay}$) or one abundance index ($I_{ay}$\footnote{For simplicity of notation we'll avoid the subscript $s$ in $I$, since we're referring to individual indices}) and $d_{ay}$ to represent residuals.

Raw residuals are compute by $d_{ay} = \log{x_{ay}} - \log{\tilde{x}_{ay}}$ and have distribution $N(0,\upsilon^2_{a})$. Standardized residuals will be compute with $d^s_{ay} = \frac{d_{ay}}{\hat{\upsilon}^2_{a}}$ where $\hat{\upsilon}^2_{a} = (n-1)^{-1} \sum_y(d_{ay})^2$. Pearson residuals scale raw residuals by the estimates of $\sigma^2$ or $\tau^2$, as such $d^p_{ay} = \frac{d_{ay}}{\tilde{\upsilon}^2_{a}}$ where $\tilde{\upsilon}^2_{a} = \tilde{\sigma}^2_{a}$ for catches, or $\tilde{\upsilon}^2_{a} = \tilde{\tau}^2_{a}$ for each index of abundance.

The `residuals()` method computes these residuals and generate a object which can be plotted using a set of out-of-the-box methods. The argument `type` will allow the user to chose which residuals will be computed. By default the method computes standardized residuals.

```{r}
# residuals
d_s <- residuals(fit, stk, idx)
# shorten time series
d_s <- window(d_s, start=iy)
```

Figure \@ref(fig:res) shows a scatterplot of standardized residuals with a smoother to guide (or mis-guide ...) your visual analysis. Note that the standardization should produce normal residuals with variance=1, which means that most residual values should roughly be between $-2$ and $2$.

```{r, res, fig.cap="Standardized residuals for abundance indices and catch numbers (catch.n). Each panel is coded by age class, dots represent standardized residuals and lines a simple smoother."}
plot(d_s)
```

When plotting residuals, by default the auxiliar line is a smoother. However it's possible to use other type of lines by setting the argument `auxline` in `plot` (check `panel.xyplot` help page for more information), of which the relevant ones for residuals are shown in Table \@ref(tab:auxargs). If type has more than one element, an attempt is made to combine the effect of each of the components.

|Argument|Description|
|:---|:----------|
|l|lines|
|h|draws vertical (or horizontal if horizontal = TRUE) line segments from the points to the origin|
|s|like "l" in the sense that they join consecutive points, but instead of being joined by a straight line, points are connected by a vertical and a horizontal segment forming a ‘step’, with the vertical segment coming first|
|S|same as s but the horizontal segment coming first|
|g| adds a reference grid|
|r|adds a linear regression line|
|smooth|adds a loess fit|
|spline|adds a cubic smoothing spline fit|
|a|draws line segments joining the average y value for each distinct x value|
Table: (\#tab:auxargs) Values for the argument `auxline` of residual plots

Figure \@ref(fig:resaux) shows a regression line over the residuals instead of the loess smooother with a grid on the background.

```{r, resaux, fig.cap="Standardized residuals for abundance indices and catch numbers (catch.n). Each panel is coded by age class, dots represent standardized residuals and lines a regression fit."}
plot(d_s, auxline=c("r","g"))
```

Pearson residuals can be computed and plotted the same way as standardized residuals by setting `type='pearson'` (Figure \@ref(fig:resp)).

```{r, resp, fig.cap="Pearson residuals for abundance indices and catch numbers (catch.n). Each panel is coded by age class, dots represent standardized residuals and lines a simple smoother."}
d_p <- residuals(fit, stk, idx, type='pearson')
# shorten time series
d_p <- window(d_p, start=iy)
plot(d_p)
```

Finally, the raw residuals are computed by setting `type='deviances'` and plotted the same way as before (Figure \@ref(fig:resr)). These residuals are usefull to identify which data points are not well modelled, showing a large dispersion of the residuals and requiring more attention from the analyst.

```{r, resr, fig.cap="Raw residuals for abundance indices and catch numbers (catch.n). Each panel is coded by age class, dots represent standardized residuals and lines a simple smoother."}
d_r <- residuals(fit, stk, idx, type='deviances')
# shorten time series
d_r <- window(d_r, start=iy)
plot(d_r)
```

The above plots can be done by age while grouping by year, instead of by year grouping by ages, the default, in which case it can help distinguish non-modeled structural year effects. The `plot` argument `by` needs to be set as `age`.

```{r, resy, fig.cap="Standardized residuals for abundance indices and catch numbers (catch.n). Each panel is coded by year, dots represent standardized residuals and lines a simple smoother."}
plot(d_s, by='age', auxline=c("h", "g"))
```

The common bubble plot (`bubble()`) are shown in Figure \@ref(fig:bub). It shows the same information as Figure \@ref(fig:res) but in a multivariate perspective.

```{r, bub, fig.cap="Bubbles plot of standardized residuals for abundance indices and for catch numbers (catch.n)."}
bubbles(d_s)
```

Figure \@ref(fig:qq) shows a quantile-quantile plot to assess how well standardized residuals match a normal distribution.

```{r, qq, fig.cap="Quantile-quantile plot of standardized residuals for abundance indices and catch numbers (catch.n). Each panel is coded by age class, dots represent standardized residuals and lines the normal distribution quantiles."}
qqmath(d_s)
```

## Predictive skill

An important feature of stock assessment model fits is the capacity to predict, since one of the most important analysis done with these fits is forecasting future fishing opportunities under pre-defined conditions. The `a4a` framework implements a visualization of the fit's predictive skill for both catch-at-age and abundance indices. These are generated by the method `plot()` with the fit object and a `FLStock` (Figure \@ref(fig:selplt)) or `FLIndices` (Figure \@ref(fig:idxplt)) object as arguments. As before the objects will be shortened to the most recent 15 years for demonstration purposes.

```{r, echo=FALSE}
fit <- window(fit, start=iy)
stk <- window(stk, start=iy)
idx <- window(idx, start=iy)
```


```{r, selplt, fig.cap="Predict and observed catch-at-age"}
plot(fit, stk)
```

```{r, idxplt, fig.cap="Predict and observed abundance-at-age"}
plot(fit, idx)
```

## Aggregated catch in weight

Although a statistical catch-at-age model assumes errors in catch-at-age and, as such, errors in the total catch in weight, there's still interest to evaluate how close the model estimates are of the observed catch in weight, even if reported catch in weight is one of the less reliable pieces of information available for stock assessment. The implementation of this diagnopstics is done through the method `computeCatchDiagnostics()`, which can be visualized with `plot()` (Figure \@ref(fig:catchdiag)).

```{r, catchdiag, fig.cap="Diagnostics for age aggregated catch in weight", fig.height=10, fig.asp=1, out.width = '100%', warning=FALSE}
fmod <- ~ factor(age) + factor(year) + te(age, year, k = c(5, 15))
fit <- sca(ple4, ple4.indices, fmodel=fmod)
c_d <- computeCatchDiagnostics(fit, ple4)
plot(c_d)
```

The `plot` method takes 2 important arguments in this case, `type` and `probs`. Thre first allows the analyst to choose between `all`, the plot in Figure \@ref(fig:catchdiag), and `prediction` (Figure \@ref(fig:cpred)), which reports prediction error, median estimates and observations. The latter, a vector of 2 values, refers to the confidence intervals to be computed.

```{r, cpred, fig.cap="Prediction of aggregated catch in weight"}
plot(c_d, type="prediction", probs=c(0.025, 0.975))
```

## Fit summary, information and cross-validation metrics

To get information about the likelihood fit the method `fitSumm()` can be used to report number of parameters (`npar`), negative log-likelkihood (`nlogl`), `ADMB` maximum gradient par (`maxgrad`), number of observations (`nobs`), generalized cross validation score (`gcv`), convergence flag (`convergence`) and acceptance rate (`accrate`) relevant for MCMC fits only.

The GCV is implemented as described by @Wood2017, where the author explains that minimizing the GCV score helps balance the trade-off between model fit and smoothness, effectively preventing overfitting by penalizing excessive complexity. For more information on the other metrics check @admb123manual and @monnahan2014admbmcmc.

The second part refers to the likelihood value for each component. The first component is catch-at-age, components after the first are for indices and the last component is for the recruitment model, if set.

```{r}
fit <- sca(ple4, ple4.indices, srmodel=~bevholt(CV=0.2))
fitSumm(fit)
```

Information criteria based metrics are reported with the methods `AIC` and `BIC`, check @ding2023information for a primer on these metrics. According to @ding2023information the AIC can lead to the selection of more complex models that may fit the data better, especially in smaller sample sizes, while the BIC incorporates a stronger penalty for model complexity. In both cases, when comparing models, the lower the score the better the model fits according to these information criteria.

```{r}
AIC(fit)
BIC(fit)
```

<!--## The package a4adiags

The package `a4adiags` contains some additional diagnostics based on the `reference`. Runs test checks weather the residuals are randomly distributed. A "run" is a sequence of the same sign residuals. Few runs indicate a trend or a correlation in the residuals while too many runs may suggest overfitting.

The primary output of a runstest is a p-value where: a high p value $(p\leq 0.05)$ suggests that the residuals are randomly distributed, a low p value indicates a non-random pattern in the residuals.

```{r, echo=FALSE, eval=FALSE}
library(a4adiags)
theme_set(theme_bw())
fit <- sca(mut09, mut09.idx, fmod = ~factor(age) + s(year, k = 8))
res <- residuals(fit, mut09, mut09.idx)
```

```{r, idxrunstest, fig.cap="Runstest for the abundance index", echo=FALSE, eval=FALSE}
plotRunstest(fit, mut09.idx, combine = F) + theme_bw() + facet_wrap(~age)
```

```{r, catchrunstest, fig.cap="Runstest for the catch by age", echo=FALSE, eval=FALSE}
plotRunstest(catch.n(mut09), catch.n(mut09 + fit), combine = F) + theme_bw() + facet_wrap(~age)
```

Green shading indicates no evidence $(p <  0.05)$ and red shading evidence $(p  >0.05)$ to reject the hypothesis of a randomly distributed time-series of residuals, respectively. The shaded (green/red) area spans three residual standard deviations to either side from zero, and the red points outside of the shading violate the '$3\sigma$ limit' for that series.-->

## Retrospective analysis

Retrospective analysis involves sequentially removing the most recent year of data, refitting the model, and comparing key metric estimates, *e.g.* the estimate of fishing mortality in year $y_{-1}$ across different fits, using a statistic like Mohn's rho [@mohn1999retrospective]. The rationale is that a well-fitted, stable model should yield consistent estimates despite changes in the data.

This method originated from Virtual Population Analysis (VPA) [@pope1972], which estimates fishing mortality and abundance by working backward from the most recent years. In contrast, retrospective analysis is less directly applicable to statistical catch-at-age models, as these models typically start from the beginning of the time series and the youngest age class, working forward through time. As noted by @cadrin2025misinterpreting, there is a risk of circular reasoning when this method is used to both diagnose and validate stock assessment models. Nevertheless, many experts still rely on retrospective analysis to evaluate model performance.

```{r}
fit0 <- sca(ple4, ple4.indices)
n <- 5
nret <- as.list(1:n)
stks <- FLStocks(lapply(nret, function(x){window(ple4, end=(range(ple4)["maxyear"]-x))}))
idxs <- lapply(nret, function(x){window(ple4.indices, end=(range(ple4)["maxyear"]-x))})
fits <- scas(stks, idxs, fmodel=list(fmodel(fit0)))
stks <- stks + fits
stks[[6]] <- ple4 + simulate(fit0, 250)
```

Note fmodel doesn't change:

```{r, echo=FALSE}
lapply(fits, fmodel)
```

The retrospective plot shown below presents the current fit with uncertainty and each retrospective fit on top. If the retrospective fit is not within the confidence interval of the current fit the analyst can argue that the estimate is different and as such reflecting a "poor" fit.

```{r, retro, fig.cap="Retrospective analysis of the plaice in ICES area IV stock. Fixed F model."}
plot(window(stks, start=2005))
```

One could use specific submodels and pass them to the fitting function `scas`, including with some adjustments to take into account the data reduction. In the next example the fishing mortality model is set reducing the smoothness taking into account the length of the dataset. Not considering the adjustment of the model to the new dataset may result in comparisons across models which are very different due to the relationship between information contained in the data and the number of parameters in the model. This issue is more relevant for stocks with shorter time series.

```{r}
n <- 5
nret <- as.list(1:n)
stks <- FLStocks(lapply(nret, function(x){window(ple4, end=(range(ple4)["maxyear"]-x))}))
idxs <- lapply(nret, function(x){window(ple4.indices, end=(range(ple4)["maxyear"]-x))})
# each model will have smootheness scaled to length of time series
fmod <- lapply(stks, defaultFmod)
fits <- scas(stks, idxs, fmodel=fmod)
stks <- stks + fits
stks[[6]] <- ple4 + simulate(fit0, 250)
```

Note fmodel changes:

```{r, echo=FALSE}
lapply(fits, fmodel)
```

And the retrospective plot

```{r, retro2, fig.cap="Retrospective analysis of the plaice in ICES area IV stock. Updating F model."}
plot(window(stks, start=2005))
```

## Hindcast

A hindcast is a method used in modeling and simulation where historical data is used to test and validate predictive models. In a hindcast, known outcomes from the past are compared with the model's predictions to assess the model's accuracy and performance. The primary goal of hindcasting is to improve the reliability and accuracy of predictive models by identifying discrepancies between predicted and actual outcomes and adjusting model parameters accordingly [@hc2002]. The term retroactive forecasting is used by @hc2002 to denote the form of hindcasting in which forecasts are made for past years (e.g. 2006–2010) using data prior to those years (perhaps 1970–2005). The terminology ex-post is used in business forecasting, referring to predictions for historical periods for which verification data are already available at the time of forecast.

For  this exercise we'll use the package `a4adiags` hindcast method, which follows the suggestions by @cookbook2021 and @KELL2016119. The hindacast is carried out by sequentially removing the most recent year in the data, similar to a retrospective analysis, refit the stock assessment model and project one year ahead. The Mean Absolute Scale Error (MASE) [@cookbook2021] is used to assess the predictive skill, a score higher than 1 indicates that the model forecasts have less skill than a random walk.

```{r, warning=FALSE, message=FALSE}
library(a4adiags)
theme_set(theme_bw())
nyears <- 5
# set number of year for average biology and selectivity
nsq <- 3
hc <- a4ahcxval(ple4, ple4.indices, nyears = nyears, nsq = nsq)
```

Figure \@ref(fig:hc) depicts the hincast results for the abudance indices used in the assessment. The MASE value is included in the strip above the plot. In this case one can see that 3 out of the 5 surveys are not better predictors than a random walk.

```{r, hc, echo=FALSE, fig.cap="Survey predictions of year ahead indices in hindcast process. The MASE is presented in the strip about the index and is related to the predictive skill of the index.", warning=FALSE, message=FALSE}
plotXval2(hc$indices)
```



<!--chapter:end:08-diagnostics.Rmd-->

# Uncertainty \label{sec:predsim}

Uncertainty is a fundamental aspect of scientific advice, serving as a reflection of the inherent limitations within the knowledge base used to construct evidence and support scientific opinions. It highlights the gaps, variability, and potential biases present in data, methods, and modeling assumptions that underlie scientific conclusions.

In fisheries science, a field that has evolved primarily to provide evidence-based advice for the sustainable exploitation of marine resources, acknowledging and addressing uncertainty is of major importance. Given the dynamic, complex, and partially observable nature of aquatic ecosystems, the need to systematically characterize and communicate uncertainty is paramount to ensuring robust and credible assessments [@privitera2019].

In this section we'll address two important elements of quantifying uncertainty in stock assessment results, prediction error and propagation of uncertainty across modelling stages.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(FLa4a)
library(ggplot2)
data(hke1567)
data(hke1567.idx)
```

The two workhorses for this topic are `predict()` and `simulate()` which are implemented to work with `sca` fits of type `"assessment"`. Note `fit = "MP"` doesn't compute the variance-covariance matrix of the parameters, which is essential for simulating.

This chapter is based on the following model:

```{r, predsim_fit0}
nsim <- 250
fmod <- ~s(age, k = 4) +
    s(year, k = 8) +
    s(year, k = 8, by = as.numeric(age == 0)) +
    s(year, k = 8, by = as.numeric(age == 4))
qmod <- list(~I(1/(1 + exp(-age))))
fit0 <- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod)
stk0 <- hke1567 + fit0
```

## The `simulate` and `predict` methods

### `predict()`

The predict method computes the quantities of interest using the estimated coefficients and the design matrix of the model, defined via the formulas in the submodels. The method uses a fitted model object, created by a call to `sca`, and returns a list with one element for each submodel, where each element is a `FLQuants` object.

```{r, predsim_fit_pred}
fit.pred <- predict(fit0)
lapply(fit.pred, names)
```

The `stkmodel` element reports `harvest`, `rec` and `ny1`. The `qmodel` reports one `FLQuant` for each index, and the `vmodel` element returns one `FLQuant` for catch (in fact `catch.n`) and one for each index. This allows easy access to the parameterised parts of the model, for example the initial population structure, `ny1`, can be accessed via `fit.pred$stkmodel$ny1`.

```{r, echo=FALSE}
fit.pred$stkmodel$ny1
```

If the fitted object has iterations, as after using the `simulate` method, which will be discussed in the next section, predict will be applied to each iter, generating distributions of the above mentioned quantities (Figure \@ref(fig:simny1)).

```{r, simny1, fig.cap='Simulations from the model prediction of initial age structure', fig.pos = 'H', fig.height = 4, echo=FALSE, message=FALSE, warning=FALSE}
fit.sim <- simulate(fit0, nsim = 250)
sim.pred <- predict(fit.sim)
fit_sim_ny1 <- sim.pred$stkmodel$ny1

# reduce to quantiles
fit_sim_ny1 <- quantile(fit_sim_ny1, prob = c(0.025, 0.50, 0.975))

# reshape
dat <- reshape(
  as.data.frame(fit_sim_ny1, drop = TRUE),
  timevar = "iter", idvar = c("age"), direction = "wide"
)

# plot
ggplot(data = dat, aes(x = age, y = `data.50%`)) +
  geom_ribbon(aes(ymin = `data.2.5%`, ymax = `data.97.5%`),
    fill = "red", alpha = .15
  ) +
  geom_point() +
  geom_line() +
  ylab("Estimated initial age structure (numbers)") +
  scale_x_continuous(breaks = pretty(dat$age))
```

### `simulate()`

As the name implies `simulate` is used to generate simulations of the fit. It operates over objects of class `a4aFitSA` using the method `mvrnorm()` provided by the R package MASS [@mass]. The method generates random draws from a multivariate normal distribution with mean given by the coefficients of the model, and variance matrix given by the estimated covariance matrix of the coefficients (in practice this is a submatrix of the inverse of the hessian matrix). The method approximates the joint distribution of the model parameters as a multivariate normal in the log space, which is inline with the assumption made by `ADMB` when fitting the model. This approach is called 'parametric bootstrap', and it's a common method for generating uncertainty in the parameters of a model.

`simulate()` operates at the submodel level, *e.g.* `simulate(fit0@pars@qmodel, nsim=250)`, when called over a `a4aFitSA` object the method simply runs `simulate()` over each of the submodels. In this case it returns an object of the same class with model parameters replaced by `nsim` simulated parameters and updated slots `stock.n`, `catch.n` and `harvest`. Figure \@ref(fig:predsimhist) depicts the distribution of a parameter, the observation error of the first survey index.

```{r, predsimhist, fig.cap="Histogram of 250 draws from the approximate distribution of the estimate of survey observation error.", echo=FALSE}
hist(
  exp(coef(fit.sim)$vmodel[[2]]),
  main = "250 draws of a model parameter",
  nclass = 10,
  xlab = "Survey index observation error"
)
```

In some simulation studies one needs to make sure the random draws are the same, which in `R` is obtained by explicitly setting the random seed with the method `set.seed()`. The same is achieved in this case as the example below shows.

```{r}
set.seed(1234)
fit.sim1 <- simulate(fit0, nsim = 250)
set.seed(1234)
fit.sim2 <- simulate(fit0, nsim = 250)
all.equal(fit.sim1, fit.sim2)
```

If the whole stock is of interest, for example, to inspect model predictions of $SSB$, the user should use the `+` method with a fitted object including iterations. In such case the `stock.n`, `catch.n` and `harvest` slots of the stock object will be updated and the usual metrics can be computed and extracted, *e.g.* `ssb(stk.pred)`. Figure \@ref(fig:sim2) depicts the stock summary plot after adding estimation uncertainty through `simulate`.

```{r, sim2, fig.cap="Stock summary of the simulated and fitted data"}
stk.pred <- hke1567 + fit.sim
plot(FLStocks(simulated=stk.pred, fitted=stk0))
```

## Prediction error

To address this topic we'll borrow from state-space modelling to define prediction error as the combination of observation error and estimation error (REF). Although `sca` is not a state-space model, some of the inherent concepts of statistical catch at age models are smilar to state-space models. `sca` estimates parameters to model underlying unobserved processes, *e.g.* recruitment, catchability or fishing mortaliy, based on indirect observations of those processes, number of fish caught by the fleet and changes in abundance at sea.

Another similar approach can be found in statistical learning model. @hastie2009 decomposes prediction error into irreducible error, square bias and variance. Where the first term refers to variance that cannot be avoided, in our case estimated by residual variance. The second term, bias is ignored as the true value isn't known and the fits are assumed to be unbiased. The last term is the variance of the prediction due to the training dataset samples, which we're approximating with parameter estimation variance.

<!--EJ NOTE: I think this paper supports the same idea [@harville85], although the maths are a bit (too...) much for me.-->

<!--In econometrics @greene explains the need to sum random noise and estimation error to build prediction intervals.

@book{greene2018econometric,
  title={Econometric Analysis},
  author={Greene, William H.},
  year={2018},
  edition={8th},
  publisher={Pearson Education},
  isbn={9780134461366}
}-->

However, `sca`'s residuals are estimates of observation error (measurement noise) and model error (misspecification, omitted variables), which means it will be an overestimate of observation error.

Nevertheless, in this section we'll assume prediction error to be the sum of residual variance and estimation variance, and will describe how to compute both elements and bringing them together in a `FLStock` object.

```{r}
# compute deviances
res0 <- residuals(fit0, hke1567, hke1567.idx, type="deviances")
# build object with estimation uncertainty
stk.eu <- hke1567 + simulate(fit0, nsim)
# build object with residual uncertainty
stk.ru <- propagate(stk0, nsim) + res0
# build object with prediction uncertainty
stk.pu <- hke1567 + simulate(fit0, nsim) + res0
```

```{r, preduncert, fig.cap="Prediction (pu), residual (ru) and estimation (eu) uncertainty", echo=FALSE}
plot(FLStocks(pu=stk.pu, eu=stk.eu, ru=stk.ru)) + facet_grid(qname~stock, scales="free") + theme(legend.position = "top")
```

## Confidence interval coverage

## Propagate uncertainty into stock assessment

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(FLa4a)
stk00 <- readRDS("data/MUT1_stk.rds")
idx00 <- readRDS("data/MUT1_idx.rds")
```

In a multistage stock assessment process as described in this book, it's important to be able to propagate uncertainty across the different stages. This section describes methods to propagate uncertainty across stages and compares their outcomes in terms of stock assessment outputs.

The idea is to add uncertainty as one moves from one stage to the next. If a stock has uncertainty on it's growth parameters, or natural mortality, or any other quantity estimated or set during the input data preparation, the model fit uncertainty will be added to it by generating iterations in the input data which are then used to fit the stock assessment model. The suggested workflow is:

1. Add uncertainty in growth or M parameters.
2. Draw from the parameters distribution.
3. Compute metrics for stock assessment.
    1. If there's uncertainty in growth parameters use slicing to created iterations of metrics by age, e.g. catch at age and index at age.
    2. If there's uncertainty in M parameters draw from the distribution and generate iterations of the M matrix.
    3. If both draw from growth and M parameters, potentially having into account correlation between those parameters, and generate iterations of age based metrics and M.
4. Fit the stock assessment model to each iteration
5. Simulate from each fit
6. Aggregate results in single `FLStock` object.

In this section we give an example of how uncertainty in natural mortality, set up using the `m()` method and the class `a4aM` (see chapter XX), is propagated through the stock assessment. We'll use the stock of Red Mullet in the Mediterranean GSA 1 (see Introduction for details) and 3 methods to add estimation uncertainty (step 5 above):

1. Take one draw of the fit
2. Take n draws of the fit and summarize with the median
3. Take n draws of the fit and combine all

These outcomes will be compared with a fit across M iterations without any sampling from the fit.

Using `a4a` methods we'll model natural mortality using a negative exponential model by age, Jensen's estimator for the level and no time trend. We include multivariate normal uncertainty using the `mvrnorm()` method and create 250 iterations.

```{r}
nits <- 250

shape <- FLModelSim(model=~exp(-age-0.5))
level <- FLModelSim(model=~k^0.66*t^0.57, params = FLPar(k=0.4, t=10),
                     vcov=matrix(c(0.002, 0.01,0.01, 1), ncol=2))
#trend <- FLModelSim(model=~b, params=FLPar(b=0.5), vcov=matrix(0.02))

m4 <- a4aM(shape=shape, level=level)
m4 <- mvrnorm(nits, m4)
range(m4)[] <- range(stk00)[]
range(m4)[c("minmbar","maxmbar")]<-c(1,1)
flq <- m(m4)[]
quant(flq) <- "age"
stk0 <- propagate(stk00, nits)
m(stk0) <- flq
```

The M matrix for this stock is shown in Figure\@ref(fig:m)).

```{r, m, fig.cap="Natural mortality generated from M model's parameter uncertainty", echo=FALSE, message=FALSE, warning=FALSE}
bwplot(data~factor(age), data=m(stk0))
```

We fit the same model to the new stock object which has uncertainty in the natural mortality and add estimation uncertainty following the methods described above.


```{r}
# create objects to store the results
stk01 <- stk0
stk02 <- stk0
stk03 <- propagate(stk00, nits*nits)

# run without estimation unceratainty
stk04 <- stk00 + sca(stk0, idx00)

for(i in 1:nits){
    stk <- iter(stk0, i)
    fit <- sca(stk, idx00)
    # Method 1
    iter(stk01, i) <- stk + simulate(fit, 1)
    # Method 2
    iter(stk02, i) <- qapply(stk + simulate(fit, nits), iterMedians)
    # Method 3
    iter(stk03, (nits*(i-1)+1):(nits*i)) <- stk + simulate(fit, nits)
}

```


```{r, mprop, fig.cap="Stock summary. Stock metrics computed over fits including uncertainty in M and estimation uncertainty"}
plot(FLStocks("M"=stk04, "M + 1 estimation sample"=stk01, "M + estimation median"=stk02, "M + n estimation samples"=stk03))
```



<!--chapter:end:09-uncertainty.Rmd-->

# The statistical catch-at-age stock assessment framework with Markov Chain Monte Carlo (MCMC) \label{sec:mcmc}

The previous methods were demonstrated using maximum likelihood estimation (MLE). However, `ADMB` also supports Markov Chain Monte Carlo (MCMC) methods, which provide significant advantages, particularly when working with complex models that involve many parameters. The key difference is that while MLE finds a single best estimate of parameters by maximizing the likelihood function, MCMC offers a broader perspective by generating an entire distribution of possible values. This approach is more informative because it does not just give the most likely estimate but also helps us understand the uncertainty surrounding it. With MCMC, researchers can incorporate prior knowledge and obtain results that are often more realistic and reliable [@gelman2013bayesian]. This is especially useful when dealing with complicated models where traditional likelihood-based methods struggle, as MCMC allows for efficient exploration of possible solutions without requiring an exact mathematical formulation [@gilks1995markov; @robert2005monte].

One of the biggest advantages of MCMC is its flexibility when working with models that have irregular behavior, such as those with multiple peaks or abrupt changes in likelihood. Standard MLE methods assume that the likelihood function behaves smoothly, but this is rarely true in real-world applications. In fisheries, ecology, and other applied sciences, models often have parameters that interact in complex ways, creating likelihood surfaces with ridges and multiple solutions. In these cases, MLE can easily get stuck in a local peak, failing to find the best possible estimate or underestimating the real uncertainty in the system [@neal1993probabilistic]. Since MCMC uses a probabilistic sampling approach instead of strict optimization, it moves freely across the entire space of possible values, making it more robust and adaptable to challenging problems [@robert2005monte].

Traditional MLE-based uncertainty estimation relies on the Hessian matrix, which essentially measures how quickly the likelihood function changes as parameters vary. This method assumes that the shape of the likelihood function is quadratic and roughly the same everywhere. However, this assumption is often unrealistic, especially in models with many parameters or correlations between them, as is common in fisheries stock assessment models

In fields like fisheries science, where models often involve multiple correlated parameters, MCMC provides a much more flexible and realistic way to estimate uncertainty. Unlike MLE, which assumes uncertainty follows a simple symmetrical pattern, MCMC can handle more complex distributions, giving a better representation of real-world variability. This is especially important when estimating key fisheries management indicators, such as spawning stock biomass ($SSB$) or fishing mortality ($F$), which influence critical policy decisions. Because MCMC does not impose strict mathematical assumptions about the shape of uncertainty, it produces estimates that are more reflective of real-world conditions, ultimately leading to more informed and reliable management strategies.

When running MCMC `ADMB` uses automatic differentiation to improve sampling efficiency and speed [@Fournier2012]. It supports various sampling algorithms, including Metropolis-Hastings and Hamiltonian Monte Carlo, which help navigate high-dimensional parameter spaces and complex likelihood structures more effectively. This makes `ADMB` particularly useful in applied sciences like fisheries and ecology, where uncertainty estimation is crucial for decision-making. Additionally, `ADMB` provides built-in diagnostics to assess MCMC convergence and reliability, ensuring that posterior distributions are well-explored and results are robust [@gelman2013bayesian].

<!--To evaluate the quality of MCMC sampling, `ADMB` offers several key diagnostics. Autocorrelation analysis to detect dependencies between successive samples; effective sample size (ESS) measures the number of independent samples in the chain. The Gelman-Rubin diagnostic (\(\hat{R}\)) helps assess whether multiple chains have converged to the same distribution, with values close to 1 indicating good convergence. Trace plots visually inspect parameter behavior over iterations, revealing trends or poor mixing. Additionally, `ADMB` monitors the acceptance rate to ensure efficient sampling and provides posterior density estimates to check if the distribution has been properly explored. These tools help users refine their MCMC runs, adjusting sampling length or proposal distributions to improve performance and ensure reliable uncertainty estimates.-->

The manual "A Guide for Bayesian Analysis in AD Model Builder" [@monnahan2014admbmcmc] describes and explain a larger group of arguments that can be set when running MCMC with `ADMB`, which the method `sca()` uses.

## The MCMC method for `sca`

This section shows how the `sca()` method interfaces with `ADMB` to use MCMC. For this section we'll use the hake assessment in mediterranean areas (gsa) 1, 5, 6 and 7.

We'll start buy fitting the MLE model and afterwards call MCMC methods. The outcomes of the MCMC fit need to be inspected to make sure the chain converged and the results are robust. A set of diagnostics are available to do this work.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
opts_chunk$set(dev='png', dev.args=list(type="cairo"))
```

```{r}
# load libraries and data
library(FLa4a)
library(ggplotFL)
data(hke1567)
data(hke1567.idx)
nsim <- 250
# MLE estimate
fmod <- ~s(age, k = 4) +
    s(year, k = 8) +
    s(year, k = 8, by = as.numeric(age == 0)) +
    s(year, k = 8, by = as.numeric(age == 4))
qmod <- list(~I(1/(1 + exp(-age))))
fit <- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod)
fit <- simulate(fit, nsim)
```

To run the MCMC method, one needs to configure a set of arguments, which is done by creating a `SCAMCMC` object. Table \@ref(tab:mcargs) describes the arguments available to run the MCMC method, extracted from Monnahan [@monnahan2019]. For more details on the MCMC configuration in `ADMB` visit the `ADMB` website.

|Argument|Default value| Description|
|---:|:---|:----------|
|mcmc | 10000 |Run N MCMC iterations|
|mcsave | 100 | Save every N th MCMC iteration|
|mcscale | | Rescale step size for first N iterations|
|mcmult | | Rescale the covariance matrix|
|mcrb | | Reduce high parameter correlations|
|mcprobe | 0.05 | Use a fat-tailed proposal distribution|
|mcdiag| FALSE| Use a diagonal covariance matrix|
|mcnoscale| FALSE | Do not scale the algorithm during|
|mcu| FALSE | Use a uniform distribution as proposal distribution|
|hybrid| FALSE | Use the hybrid method|
|hynstep | | Mean number of steps for the leapfrog method|
|hyeps | | The stepsize for the leapfrog method [X numeric and > 0]|

Table: (\#tab:mcargs) `ADMB` MCMC arguments

```{r}
# mcmc
mc <- SCAMCMC()
```

Defaults for now are ok, so lets fit the model. Note that the argument `fit` must be set to `MCMC` and the argument `mcmc` takes the `SCAMCMC` object.

```{r}
# fit the model
fitmc00 <- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = "MCMC", mcmc=mc)
# check acceptance rate
fitSumm(fitmc00)
```

As usual `fitSumm` store relevant information about the model fit. In the case of MCMC fit the information stored is the number of model paramters (`nopar`), the number of observations (`nobs`) and the acceptance rate (`accrate`).

```{r, fig.cap="Stock assessment summaries of maximum likelihood (mle) and monte carlo (mc) fits."}
plot(FLStocks(mle=hke1567 + fit, mc=hke1567 + fitmc00))
```

## Diagnostics with CODA

In essence, the diagnostics are used to give the analyst confidence that the posterior distribution of the parameters is unbiased, as much as possible with symmetric non correlated distributions of each parameter over which one can make inference.

There's a large body of literature about MCMC convergence. In this section we'll focus on the out-of-the-box methods for metropolis hastings algorithm available to the stock assessment scientist: trace plots, autocorrelation and cross correlation analysis, geweke diagnostic, Gelman and Rubin's convergence diagnostic, cumulative means, distribution density and acceptance rate. These tools should be used together to evaluate proper mixing and convergence.

`ADMB` has an hybrid algorithm based on Hamiltonian dynamic which will not be addressed here. The reader is invited to consult @monnahan2019 for more information.

We use the package `CODA` [@coda] to run the diagnostics on MCMC fits. One needs to convert the `sca` output into a `mcmc` `CODA` object over which several diagnostics can be ran. The `mcmc` object is a matrix with the parameters (row = iters, cols= pars).

```{r}
library(coda)
```

For demonstration purposes we'll create a chain with 1000 samples (`mcmc=1000`) and save every iter (`mcsave=1`), which will create a highly correlated and unstable chain, and update the initial MCMC fit to also have 1000 samples (`mcmc=100000`, `mcsave=100`). The latter will have lower correlation due to the higher thinning.

```{r}
# update initial fit, control random seed
mc <- SCAMCMC(mcmc=100000, mcsave=100, mcseed=10)
fitmc01 <- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = "MCMC", mcmc=mc)
fitmc01.mc <- FLa4a::as.mcmc(fitmc01)
# highly correlated fit, control random seed
mc <- SCAMCMC(mcmc=1000, mcsave=1, mcseed=10)
fitmc02 <- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = "MCMC", mcmc=mc)
fitmc02.mc <- FLa4a::as.mcmc(fitmc02)
```

### Traceplots

Trace plots (`traceplot()`) show the sampled values of a parameter over iterations. A plot that looks like a random, stable "cloud" of points with no trends or drifts, with rapid fluctuactions, is a signal of convergence, meaning the chain mixes well and is stationary. If the trace plot shows a strong trend or periodicity, drifts, or long autocorrelated stretches, it means the chain hasn't converged. Figure \@ref(fig:chain01) cleary depicts this difference between the two runs.

```{r, chain01, fig.cap="MCMC chain's trace for the first parameter. High correlation chain in blue, low correlation chain in red."}
# use mcmc.list() to create a list with both runs so they're plot together
traceplot(mcmc.list(mc01=fitmc01.mc[,1], mc02=fitmc02.mc[,2]), lwd=1.5, col=c(2,4), lty=1)
```

Ploting the chains for the parameter clearly shows autocorrelation for the first parameter in the blue chain. It also shows an initial phase where the chain seems to be stuck in a single value. This initial phase, when the parameter seems to be stuck in a fixed position, is called the "burn-in" phase. These iterations can be dropped with the `burnin()`, although it doesn't sort out autocorrelation (Figure \@ref(fig:chain01b)).

```{r, chain01b, fig.cap="MCMC chain with high autocorrelation after removing the initial 250 samples (burnin period)."}
traceplot(FLa4a::as.mcmc(burnin(fitmc02, 250))[,1], lwd=1.5, col=4, lty=1)
```

### Autocorrelation and crosscorrelation analysis

Autocorrelation analysis (`acf()` and `acfplot()`) is useful to assess stationarity, a stationary chain should have low autocorrelation, meaning that each sample is approximately independent. On the opposite, high autocorrelation indicates slow mixing and possible non-stationarity. Furthermore, in a well mixed chain autocorrelation drops quickly to near zero, while a poor mixing will display high autocorrelation reducing efficiency of the search.

The autocorrelation plot will show correlation along the chain for each parameter at different lags. Figure \@ref(fig:acf01hc) shows there's a strong autocorrelation for the first parameter which we'd like to avoid, while Figure \@ref(fit:acf01) shows a much lower correlation.

```{r, acf01hc, fig.cap="Autocorrelation plot of the first parameter in the MCMC chain", fig.show="hold"}
acfplot(fitmc02.mc[,1], lwd=3, main="High correlation chain", ylim=c(-1, 1))
```

```{r, acf01, fig.cap="Autocorrelation plot of the first parameter in the MCMC chain", fig.show="hold"}
acfplot(fitmc01.mc[,1], lwd=3, main="Low correlation chain", ylim=c(-1, 1))
```

Crosscorrelation (`crosscorr()` and `crosscorr.plot()`) inspects the pairwise correlation of all parameters, which is a useful tool to assess the efficiency of the sampling process and the independence of the generated samples. If cross-correlations are high, it often means that transitions between states are slow, leading to an increased running time and requiring a larger number of samples to achieve effective independent samples. Conversely, low cross-correlation implies that parameters are explored more independently, leading to faster convergence and better mixing.

```{r, ccr01, fig.cap="Crosscorrelation plots", fig.show="hold", out.width="50%"}
crosscorr.plot(fitmc01.mc, main="Low correlation chain")
crosscorr.plot(fitmc02.mc, main="High correlation chain")
```

### Geweke diagnostic

The geweke diagnostic (`geweke.diag()` and `geweke.plot()`) computes the Geweke-Brooks Z-score [@Geweke1992], which indicates if the first and following parts of a sample from a Markov chain are drawn from the same distribution as the last part of the chain, usualy the last 50% of the samples.

It's useful to decide if the first few iterations should be discarded and provides information about the stability of the chain. Figure \@ref(fig:gew01) shows the geweke plot for the two MCMC runs.

```{r, gew01, fig.cap="Geweke plot of the first parameter in the MCMC chains", fig.show="hold", out.width="50%"}
geweke.plot(fitmc01.mc[,1], main="Low correlation chain")
geweke.plot(fitmc02.mc[,1], main="High correlation chain")
```

The panel on the left shows a much more regular chain, where the different blocks of data show similar distributions. The panel on the right clearly shows the z-score statistic changing out of the confidence intervals until about 400 samples are discarded, which points to the need to drop a set of initial samples.

The geweke diagnostic is also a good way to look at mixing by comparing the mean and variance of the first part of the chain to the last part. Good mixing will show no significant difference between early and late samples. Poor mixing will show large differences, indicating the chain has not explored the posterior fully.

### Cumulative means

Inspecting the cumulative mean along the chain is another good way to check for the stability of the chain. When the mixing is good the mean stabilizes quickly, and vice-versa if not.

```{r, cmean01, fig.cap="Cumulative mean plots of the first parameter in the MCMC chains", fig.show="hold", out.width="50%"}
cm01 <- fitmc01.mc[,1]
cm01 <- cumsum(cm01) / seq_along(cm01)
cm02 <- fitmc02.mc[,1]
cm02 <- cumsum(cm02) / seq_along(cm02)
plot(cm01, type="l", xlab="samples", ylab="mean", main="Low correlation chain", ylim=c(-0.52, -0.42))
plot(cm02, type="l", xlab="samples", ylab="mean", main="High correlation chain", ylim=c(-0.52, -0.42))
```

### Distribution density

An important element of MCMC is to produce symetric posterior distributions, for one it's a sign that the chain explored the space of the parameter, for other it makes inference about the parameters a lot more robust. If the distributions are skewed or multimodal, estimating the expected value and variance becomes a lot more complicated. As such having symetric distributions is preferred and should be checked before computing statistics of interest.

Figure \@ref(fig:dens01) shows the density plots (`densplot()`) for both runs, where it shows the symetric distribution of the uncorrelated chain (left panel) and the bimodal distribution of the correlated chain.

```{r, dens01, fig.cap="Density plots of the first parameter in the MCMC chains", fig.show="hold", out.width="50%"}
densplot(fitmc01.mc[,1], main="Low correlation chain")
densplot(fitmc02.mc[,1], main="High correlation chain")
```

### Gelman-Rubin statistic

The Gelman-Rubin statistic ($\hat{R}$) [@gelman1992inference] can be used to check if multiple chains have reached a stable state and are properly exploring the target distribution. It compares how much variation exists within each chain to the variation between different chains. If all chains are sampling from the same distribution, these variations should be similar, and $\hat{R}$ will be close to 1, otherwise, if it's greater than 1.1 it suggests that the chains have not yet converged.

To compute $\hat{R}$, multiple chains are run with different starting points. The algorithm measures how spread out the samples are within each chain and compares it to how much the chains differ from each other. If the chains have not mixed well, they will appear too different from each other, and $\hat{R}$ will be large. If the chains have mixed properly, they will have a similar spread, and the statistic will be close to 1.

To run another chain one makes use of the `mcseed` argument to make sure the 2 chains start from different places. The Gelman-Rubin statistics is computed by the `gelman.diag()` method and depicted with `gelman.plot()`. It's easy to see the difference between the two fits. While the low corrrelation fit shows values close to 1 for most parameters, the high correlation fit shows a number of large values.

```{r}
# low correlation
mc <- SCAMCMC(mcmc=100000, mcsave=100, mcseed=30)
fitmc01b <- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = "MCMC", mcmc=mc)
fitmc01b.mc <- FLa4a::as.mcmc(fitmc01b)
# highly correlated fit
mc <- SCAMCMC(mcmc=1000, mcsave=1, mcseed=30)
fitmc02b <- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = "MCMC", mcmc=mc)
fitmc02b.mc <- FLa4a::as.mcmc(fitmc02b)
# create lists for comparison
mclst01 <- mcmc.list(a=fitmc01.mc, b=fitmc01b.mc)
mclst02 <- mcmc.list(a=fitmc02.mc, b=fitmc02b.mc)
```

```{r, echo=FALSE}
gelman.diag(mclst01)
```

```{r, echo=FALSE}
gelman.diag(mclst02)
```

```{r, gelm01, fig.cap="Gelman-Rubin's diagnostic plots for the first parameter.", fig.show="hold", out.width="50%"}
mclst01 <- mcmc.list(a=fitmc01.mc[,1], b=fitmc01b.mc[,1])
mclst02 <- mcmc.list(a=fitmc02.mc[,1], b=fitmc02b.mc[,1])
gelman.plot(mclst01, main="Low correlation chain", ylim=c(1, 5))
gelman.plot(mclst02, main="High correlation chain", ylim=c(1, 5))
```

### Acceptance rate

The acceptance rate in Markov Chain Monte Carlo (MCMC) methods plays a crucial role in balancing exploration and efficiency when sampling from a posterior distribution. It represents the proportion of proposed states that are accepted in the Markov chain and directly influences mixing, convergence, and the quality of inference.

A low acceptance rate (e.g., <20%) means that most proposed moves are rejected, leading to slow exploration of the posterior distribution. This can result in poor mixing and high autocorrelation between samples [@gelman2013bayesian]. A high acceptance rate (e.g., >80%) suggests that the proposals are too conservative, leading to small moves and highly correlated samples.

@monnahan2019 suggests that the optimal acceptance rate varies by model size, among other things, but is roughly 40%, although models with more parameters should have a lower optimal acceptance rate. @gelman97 complementary suggest that for Random Walk Metropolis-Hastings (RWMH) in high-dimensional spaces an optimal acceptance rate is about 25%.

The acceptance rate is reported out of the MCMC fit and can be accessed with `fitSumm()`. Inspecting the acceptance rate for the models we're using shows a higher acceptance rate for the high correlation model, although both are above the recommended optimal for high dimensional models, like the models used in stock assessment.

```{r}
data.frame(hessian_scale=c(fitSumm(fitmc01)),
    hessian_noscale=c(fitSumm(fitmc02)),
    row.names=rownames(fitSumm(fitmc01)))

```

## `ADMB`'s arguments to tune the MCMC algorithm

This section is based on @monnahan2019 and describes a set of arguments and methods which the stock assessment analyst can use to tune the MCMC algorithm and be more confident on its convergence and follow up inference.

### Thinning rate

For the Metropolis-Hastings algorithm, the most important tuning option available to the user is the saving rate (the inverse of the thinning rate). This is the rate at which parameters are saved, such that thinning is effectively discarding draws. This tuning option is critical since this algorithm generates auto-correlated parameters by design. The user controls the thinning rate with the argument `mcsave`.

If N = 1 every single draw is saved (none are thinned out), which generates high autocorrelation, suggesting the need to thin more (save fewer). This is the case of the `fitmc02` fit. In `fitmc01` `mcsave` was increased to 100, by increasing the total samples by 100 and saving every 100th. This helps reduce the autocorrelation and produces independent draws from the posterior of interest.

### mcscale and mcnoscale

`ADMB` "scales” the covariance matrix up or down, depending on the current acceptance rate, during the first part of the chain. Scaling the covariance matrix down produces proposed sets closer to the current set, and vice versa for scaling up. By default, it scales during the first 500 iterations before thinning, but the user can specify this with `mcscale` or turn off scaling with `mcnoscale`. `ADMB` rescales the covariance matrix every 200 iterations until the acceptance rate is between 0.15 and 0.4, or the scaling period is exceeded. Draws from this tuning phase should be discarded as part of the burn-in.

The code below illustrates the effect in the acceptance rate of not scaling the hessian, the acceptance rate drops significantly, which means poor mixing and higher autocorrelation.

```{r}
# no scale
mc <- SCAMCMC(mcmc=100000, mcsave=100, mcseed=10, mcnoscale=TRUE)
fitmc01ns <- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = "MCMC", mcmc=mc)
fitmc01ns.mc <- FLa4a::as.mcmc(fitmc01ns)
data.frame(hessian_scale=c(fitSumm(fitmc01)),
    hessian_noscale=c(fitSumm(fitmc01ns)),
    row.names=rownames(fitSumm(fitmc01)))
```

In the next analysis we show the autocorrelation statistics for lags of 0, 1, 5, 10 and 50. When the hessian is not scaled the autocorrelation of lag 1 increased from 0.33 to 0.43.

```{r}
data.frame(hessian_scale=c(autocorr.diag(fitmc01.mc[,1])),
    hessian_noscale=c(autocorr.diag(fitmc01ns.mc[,1])),
    row.names=c(0, 1, 5, 10, 50))
```

### mcprobe

For some models, there may be concern of being “stuck” in a local minimum and simply never proposing a value far enough away to escape it and find other regions of high density. `ADMB` has a built-in algorithm which modifies the default proposal distribution so it occasionally proposes very distant parameters (i.e. “probes”). The modified proposal distribution is a mixture distribution of Normal and Cauchy distributions.

The `mcprobe` argument controls how the two distributions are mixed, with larger values being more Cauchy (fatter tails, larger jumps). The range of valid inputs is 0.00001 to 0.499, and if no value is supplied a default of 0.05 is used.

```{r}
# more Cauchy
mc <- SCAMCMC(mcmc=1000, mcsave=1, mcseed=10, mcprobe=0.45)
fitmc02p <- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = "MCMC", mcmc=mc)
fitmc02p.mc <- FLa4a::as.mcmc(fitmc02p)
data.frame(probe_0.05=c(fitSumm(fitmc02)),
    probe_0.45=c(fitSumm(fitmc02p)),
    row.names=rownames(fitSumm(fitmc02)))
```

In the next analysis we show the autocorrelation statistics for lags of 0, 1, 5, 10 and 50. When the hessian is not scaled the autocorrelation of lag 1 increased from 0.33 to 0.37.

```{r}
data.frame(probe_0.05=c(autocorr.diag(fitmc02.mc[,1])), probe_0.45=c(autocorr.diag(fitmc02p.mc[,1])))
```

### mcrb

The `mcrb` option (which stands for “rescaled bounded”) alters the covariance matrix used to propose new parameter sets in the Metropolis-Hastings algorithm. Its intended use is to create a more efficient MCMC sampler so the analyses run faster. This option reduces the estimated correlation between parameters. The value must be integer between 1 and 9, inclusive, with lower values leading to a bigger reduction in correlation.

The option will be most effective under circumstances where the correlation between parameters at the posterior mode is higher than other regions of the parameter space. In this case, the algorithm may make efficient proposals near the posterior mode, but inefficient proposals in other parts of the parameter space. By reducing the correlation using `mcrb` the proposal function may be more efficient on average across the entire parameter space and require less thinning (and hence run faster).

```{r}
# reduce correlation
mc <- SCAMCMC(mcmc=100000, mcsave=100, mcseed=10, mcrb=9)
fitmc01r <- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = "MCMC", mcmc=mc)
fitmc01r.mc <- FLa4a::as.mcmc(fitmc01r)
data.frame(mcrb_no=c(fitSumm(fitmc01)),
    mcrb_high=c(fitSumm(fitmc01r)),
    row.names=rownames(fitSumm(fitmc01)))
```

In the next analysis we show the cross-correlation plots (Figure \@ref(fig:mcrp)).

```{r, mcrp, fig.cap="Cross correlation for run without mcrp and mcrp of 9.", out.width="50%"}
crosscorr.plot(fitmc01.mc, main="No mcrp")
crosscorr.plot(fitmc01r.mc, main="mcrp = 9")
```


<!--chapter:end:10-mcmc.Rmd-->

# Reference Points

One of the primary objectives of stock assessment is the estimation of reference points. These serve as benchmarks for evaluating the outputs of assessment models and determining the status of a fish stock. Reference points are critical for effective fisheries management, supporting decision makers setting future levels of exploitation.

The most common classification of stock status is bidimensional, comparing exploitation levels and biomass sizes against target reference points. This framework allows for the assessment of whether a stock is overfished or experiencing overfishing:

1. Exploitation Levels: Typically represented by fishing mortality ($F$), overfishing occurs when $F$ exceeds the target reference point.
   
2. Biomass Size: Commonly measured by spawning stock biomass ($SSB$), stocks are deemed overfished if the $SSB$ falls below the reference point.

These assessments often utilize tools like the Kobe plot, which visually represents stock status in relation to these metrics (REF).

In addition to target reference points, limit reference points (LRPs) are commonly included in stock assessments. These represent thresholds that should not be crossed, as they signal a high risk of stock collapse or significant uncertainty in population dynamics. Effective management aims to maintain fishing pressure and biomass levels away from these limits to ensure long-term sustainability and reduce the risk of adverse outcomes.


In 1982, the United Nations Convention on the Law of the Sea (UNCLOS, @unclos1982), in Article 61, required coastal states to determine and maintain, or restore, populations of harvested species at levels capable of producing the maximum sustainable yield (MSY). MSY is defined as the largest yield (catch) that can be taken from a specific fish stock over an indefinite period without causing stock depletion. Since then MSY and its proxies, such as $B_{MSY}$ (biomass at MSY) and $F_{MSY}$ (fishing mortality at MSY), remain widely used. These reference points are related with the stock's productivity, which results of a complex interaction between recruitment, individual growth and mortality processes.

Recruitment refers to the addition of new individuals to a fish population, specifically those that become vulnerable to fishing. It includes the process of spawning, which depends on the reproductive potential of adults, and the survival of larvae until they enter the fishery - a process largely influenced by environmental conditions. Individual growth determines how long it takes for a fish to gain weight, increase in length, and eventually reach maturity and reproduce. Mortality is typically divided into natural mortality and fishing mortality. Natural mortality includes all causes of death unrelated to fishing, such as predation by other species, and is influenced by both environmental factors and biological interactions. Fishing mortality, on the other hand, is primarily driven by human activity and depends on decisions related to fishing practices, including the effort exerted by the fleet, the selectivity of the gear used, and the availability of fish. For example, the productivity of a stock can differ significantly depending on whether a fleet fishes in an area with many juvenile fish using small-mesh nets, or in an area where juvenile fish are scarce and large-mesh nets are used. These processes are interconnected and influenced by a complex mix of environmental conditions and biological characteristics.

For this section we'll be using the package `FLBRP` [@flbrp] from the FLR family of packages and its documentation for parts of the text.

```{r}
library(FLBRP)
library(FLa4a)
data(ple4)
data(ple4.indices)
fit0 <- sca(ple4, ple4.indices)
stk0 <- ple4 + fit0
```

To proceed with the computation of reference points we must start by creating an `FLBRP` object and afterwards run the fitting process with `brp()`. The FLBRP class has information on selection pattern, mass at age, and biological parameters (see `?FLBRP` for a full description of this class).

Slots named `*.obs` will contain the related time series present in the original `FLStock` object, while other slots will contain averages across the year dimension over the last $n$ years, where $n$ is controlled by the arguments `biol.nyears`, `fbar.nyears` and `sel.nyears` (Table \@ref(tab:brpargs))

| Argument | Default value | Description |
|---:|:---|:---------|
|fbar | seq(0, 4, length.out = 101)| Vector of Fs to find minimum or maximum value of metric, e.g. MSY |
| nyears | 3 | Number of years to average across to estimate input quantities for reference points estimation |
| biol.nyears | nyears | Number of years to average across to estimate biological quantities, e.g. maturity|
| fbar.nyears | nyears | Number of years to average across to scale F-at-age to 1 and compute selection pattern|
| sel.nyears | nyears | Number of years to average across to compute landings and discards in number of individual per age |
| na.rm | TRUE | Remove NA if existing|
| mean | "arithmetic" | Statistic to average quantities above, alternative is "geometric" for the geometric mean|

Table: (\#tab:brpargs) Arguments available to create `FLBRP` object.

By default `FLBRP` creates a harvest slot with 100 computations of fishing mortality at age scaled from $\bar{F}=0$ up to $F_{crash}$ or $\bar{F}=4$, if the former isn't possible to compute, which is later used to find the reference points.

## Yield per recruit reference points

In the case where no stock recruitment relationship exists, or was fitted, `brp()` will return yield per recruit reference points. By default it computes biomasses in the absence of fishing, also know as virgin biomass, $F_{MAX}$, $F_{0.1}$ and 40% Spawning per recruit reference points.

```{r}
brp0 <- FLBRP(stk0)
brp0 <- brp(brp0)
summary(brp0)
```

The selection pattern and other quantities can be depicted by calling `plot()` on the specific `FLBRP` object's slot.

```{r, selection pattern, echo=FALSE}
xyplot(data~age,data=catch.sel(brp0),type=c('l', 'p'))
```

```{r, relevant quantities for reference points, echo=FALSE}
xyplot(data~age|qname, data=FLQuants(sel=catch.sel(brp0),
 	dsel=discards.sel(brp0), swt=stock.wt(brp0),
	cwt =catch.wt(brp0), mat= mat(brp0), m = m(brp0)),
	type="l",scale="free")
```

To extract a table with all reference points one uses the method `refpts()`. Note in this case $F_{msy}$ is the same as $F_{max}$, since the assumed stock recruitment is mean recruitment. 

```{r}
refpts(brp0)
refpts(brp0)[c('msy', ('fmax')), ]
```

The depiction of the reference points with the method `plot()` shows recruitment as constant over all levels of biomass and set to $1$.

```{r, reference points, echo=FALSE}
plot(brp0)
```

## Stock recruitment relationship based reference points

An important way to improve reference points is to include stock recruitment dynamics. Yield per recruit, as in the previous section, ignores these dynamics and assumes recruitment will be the same no matter $SSB$'s size. To inform `brp()` to take stock recruitment dynamics into account, the stock recruitment model must be fitted and the resulting `FLSR` object passed to the `FLBRP` call when creating the `FLBRP` object.

There's two ways of fitting stock recruitment models: (i) after fitting the stock assessment model by using its outputs, `SSB` and recruitment, as data to fit the model; (ii) inside the stock assessment model together with all other quantities. There's pros and cons on both approaches, we're not going to dwell on those now though.

### Stock recruitment after fitting the stock assessment model  

In the following example we'll use a Beverton and Holt stock recruitment relationship. There are several other relationships that can be used, see `?bevholt` for more details.

```{r, message=FALSE, warning=FALSE, message=FALSE}
sr0 <- as.FLSR(stk0, model=bevholt)
sr0 <- fmle(sr0, control = list(trace = 0))
plot(sr0)
```

We now need to provide the `FLSR` object, `sr0`, to the `FLBRP()` and refit the reference points.

```{r}
brp0 <- FLBRP(stk0, sr=sr0)
model(brp0)
params(brp0)
brp0 <- brp(brp0)
```

The new reference points can now be extracted using `refpts()` with the `FLBRP` object as the main argument, and depict the relationships with `plot()`. Note this time by setting the flag `obs` to `TRUE` the plot will include the estimates of $SSB$ and $R$.

```{r}
refpts(brp0)
```

Note $MSY$ based reference points are no longer the same as $F_{MAX}$, and recruitment is no longer constant over all $SSB$ levels.

```{r, echo=FALSE}
plot(brp0, obs=TRUE)
```

### Stock recruitment during stock assessment model fit

An alternative option, using `sca()`, its to fit the stock recruitment model together with the stock assessment model fit, and create the `FLSR` object from the fit object (Figure \@ref(fig:a4asr)).

```{r}
# fit with Beverton and Holt model
fit1 <- sca(ple4, ple4.indices, srmodel = ~ bevholt(CV = 0.5))
# create FLSR object
a4aflsr <- as(fit1, "FLSR")
```

```{r, a4asr, fig.cap="Stock recruitment model estimated by `sca()`"}
plot(a4aflsr, obs = TRUE)
```

Create the `FLBRP` object with the new `FLSR` object with the stock recruitment model fitted with `sca()` and fit a new set of reference points (Figure \@ref(fig:a4arp)). Note the reference points are slightly different. The stock recruitment parameters estimated with `sca()` take into account all the other parameters and as such are not exactly the same, ultimately resulting in a distinct set of reference points.

```{r}
# create FLBRP object
a4abrp <- FLBRP(stk0, a4aflsr)
a4abrp <- brp(a4abrp)
```

```{r, a4arp, fig.cap="Reference points estimated with `sca()` stock recruitment fit"}
plot(a4aflsr, obs = TRUE)
```

## Economics reference points

We can add economic data to the `FLBRP` object to calculate economic based reference points, like maximum economic yield (MEY). We need to provide information about price, variable costs and fixed costs. The first in value at age per weight of fish, the others in value per unit of fishing mortality.

```{r}
# price
price(brp0) <- c(rep(1,3),rep(1.5,2),rep(2,5))
price(brp0)@units <- "1000 euro per ton"

# variable costs per F 
vcost(brp0) <- 100000
vcost(brp0)@units <- "1000 euro per F"

# fixed costs per F 
fcost(brp0) <- 50000
fcost(brp0)@units <- "1000 euro per F"

# reference points
brp0 <- brp(brp0)
refpts(brp0)
```

The reference points table is now complete with values for `revenue`, `costs` and `profit`, as well as estimtes for $MEY$ based reference points. The point where `profits` are maximized, instead of the point where `catch` is maximized as in the case of `MSY` (Figure \@ref(fig:ecorp)).

```{r ecorp, fig.cap="Reference points including economic reference points"}
plot(brp0)
```

## Computing user specific reference points

The user may want to calculate specific "reference points" given F levels. The example below shows how it can be done, having in mind that by specifying F levels the user may be computing arbitrary references.

```{r}
custom_refs <- FLPar(Ftrgt1 = 0.33, Ftrgt2 = 0.44)
brp1 <- brp0 + custom_refs
refpts(brp1)
```

<!--Or create an empty FLPar with specified reference points and recalculate everything:

```{r}
#brp2 <- FLPar(NA,dimnames=list(refpt=c("virgin","f0.1","fmax","spr.30","spr.35","spr.45"), quantity=c("harvest","yield","rec","ssb","biomass","revenue","cost","profit"), iter=1))
brp2 <- brp1

brp2@refpts <- FLPar(NA, dimnames=list(refpt = c("virgin", "f0.1", "fmax", "spr.30", "spr.35","spr.45"), quantity=c("harvest", "yield", "rec", "ssb", "biomass", "revenue", "cost", "profit"), iter=1))

brp2 <- brp(brp2)
refpts(brp2)
```
Another simple way, although it onl;y works for $SPR$ based reference points, is to include other `spr.##` points in the `refpts` table.
-->

One specific case of user tailored reference points is to compute $F_{MSY}$ ranges according to @HILBORN2010193 and @Rindorf_etal_2016 ideas. For this case there's already the method `msyRanges()`, which takes as argument a fitted `FLBRP` object and delivers a `FLPar` object, similar to `refpts` with the lower and upper boundary of F according to the specified range multiplier, e.g. if 0.05 the ranges will reflect $(1-0.05) \times F_{MSY}$ and $(1+0.05) \times F_{MSY}$.

```{r}
rp.rngs <- msyRange(brp0, range=0.05)
rp.rngs
```


<!--chapter:end:11-refpts.Rmd-->

# Projections and harvest control rules

@massad2005 provide a useful distinction between forecasting and projection in scientific prediction, stating that prediction in general science can be categorized into two main components: forecasting and projection. According to their definition, a forecast is an attempt to predict what will happen, whereas a projection describes what would happen given certain hypotheses.

This distinction is also reflected by the Organisation for Economic Co-operation and Development (OECD) [@OECD], which notes that “forecasting” and “prediction” are often used interchangeably when estimating the future value of a given variable. The term projection is generally used in two interrelated senses: (1) as a future value of a time series computed based on specific assumptions about environmental changes, and (2) in probability theory, as the conditional expectation of a variable [@OECD].

Applying this conceptual framework to fisheries science, the process of advising on future fishing opportunities is best viewed as a projection exercise. Predicting future outcomes is always conditioned on assumptions about future conditions—e.g., recruitment—or used to test hypotheses—e.g., regarding levels of extraction.

In the multi-stage stock assessment process described in this book, projections are not part of the stock assessment modeling itself. The assessment concludes when analysts compare estimates of biomass and fishing mortality with reference points, determining whether a stock is overfished or subject to overfishing [@hilborn2013quantitative]. Projections typically follow this assessment and incorporate estimates or assumptions about population dynamics—such as growth, reproduction, and natural mortality—to predict future catches, biomass, and abundance under specific conditions and quantified uncertainty. This final stage aims at providing scientific advice to decision-makers about potential consequences of the decisions they need to make.

For this section we'll be using the package `FLasher` [@flasher; @flasher2016] from the FLR family of packages.

```{r, warning=FALSE, message=FALSE}
library(FLa4a)
library(FLasher)
library(FLBRP)
#library(ggplotFL)
data(ple4)
data(ple4.indices)
```

## Simple workflow

The basic workflow to project with `FLasher` is:

1. extend the `FLStock` object to store the predictions using the method `fwdWindow()`
2. set the targets for the projection with method `fwdControl()`
3. project with the method `fwd()` using a `FLStock` and a `FLSR`

We'll start by fitting a model including, a stock recruitment model which will be used to forecast recruitment. We'll also set the number of iterations we'll be working with and the time period we want to project.

```{r, warning=FALSE, message=FALSE}
# fit model
fmod <- ~ s(age, k = 8) + s(year, k = 25) + te(age, year, k = c(6, 15))
fit00 <- sca(ple4, ple4.indices, srmodel=~geomean(), fmodel=fmod)
stk00 <- ple4 + fit00
# create stock recruitment model object
sr00 <- as(fit00, "FLSR")
# set projection
# number of iterations
nsim <- 250
# most recent year in the data
maxy <- range(ple4)["maxyear"]
# number of years to project
projy <- 5
# last year for projections
endpy <- maxy + projy
# initial year for projections
inipy <- maxy + 1
# extend stock object to store projection's results
stk00 <- fwdWindow(stk00, end = endpy)
# set the control for the projections, in this case a fixed f of 0.3
trg00 <- fwdControl(year = inipy:endpy, quant = "f", value = 0.3)
# project
stk01 <- fwd(stk00, control=trg00, sr=sr00)
```

```{r, echo=FALSE, fig.cap="Projection of stock for 5 years with fixed fishing mortality and recruitment"}
plot(stk01)
```

A natural addition to this forecast is to add uncertainty. We'll do that by generating uncertainty in population numbers, catch numbers and fishing mortality, using `simulate()`, and add stock recruitment uncertainty using the residuals of the fit.

```{r, warning=FALSE, message=FALSE}
stk00 <- ple4 + simulate(fit00, nsim)
stk00 <- fwdWindow(stk00, end = endpy)
res00 <- residuals(sr00)
rec00 <- window(rec(stk00), 2018, 2022)
rec00 <- rlnorm(rec00, mean(res00), sqrt(var(res00)))
stk02 <- fwd(stk00, control=trg00, sr=sr00, residuals=rec00)
```

```{r, echo=FALSE, fig.cap="Stochastic projection of stock for 5 years with fixed fishing mortality and recruitment"}
plot(stk02)
```

An alternative to the above workflow is to fit the stock recruitment model after the stock assessment model, using the output of the assessment as input to the stock recruitment fit. In which case stock recruitment estimation uncertainty can be added by fitting the stock recruitment model over stock assessment uncertainty, so that there will be stock-recruitment fit to each iteration generated from the stock assessment model.

```{r, warning=FALSE, message=FALSE}
fit00 <- sca(ple4, ple4.indices) 
stk00 <- ple4 + simulate(fit00, nsim)
sr00 <- as.FLSR(stk00, model="geomean")
sr00 <- fmle(sr00, control = list(trace = 0))
stk00 <- fwdWindow(stk00, end = endpy)
res00 <- residuals(sr00)
rec00 <- window(rec(stk00), inipy, endpy)
rec00 <- rlnorm(rec00, c(yearMeans(res00)), sqrt(c(yearVars(res00))))
stk03 <- fwd(stk00, control=trg00, sr=sr00, residuals=rec00)
```

```{r, echo=FALSE, fig.cap="Stochastic projection of stock for 5 years with fixed fishing mortality and recruitment"}
plot(stk03)
```

These two methods don't give very different result when the stock recruitment model is not having a large impact in the other parameters. However the second method is much slower due to all the fits needed to have the empirical distribution of the stock recruitment model parameters.

```{r, echo=FALSE, fig.cap="Stochastic projection of stock for 5 years with fixed fishing mortality and recruitment. 01: projection without uncertainty, stock recruitment model fit within the stock assessment model; 02: projection with uncertainty, stock recruitment model fit within the stock assessment model; 03: projection with uncertainty, stock recruitment model fit after the stock assessment model"}
plot(window(FLStocks('01' = stk01, '02' = stk02, '03' = stk03), start = 2000))
```

## Initial condition assumptions

When projecting the stock forward one needs to make a number of assumptions about initial conditions, the starting point from where projections will be made. The method `fwdWindow` has a set of options that allows the analyst to decide about those assumptions:

| Argument | Default value | Description |
|---:|:-|:---------|
|`wt`| 3 | Number of years to average over to get the future mean weights at age|
|`mat`| 3 |Number of years to average over to get the future proportion mature at age|
|`m`| 3 |Number of years to average over to get the future natural mortality at age|
|`spwn`| 3 | Number of years to average over to get the future fraction of mortality before spawning|
|`discards.ratio`| 3 | Number of years to average over to get the future mean proportion of discards at age|
|`catch.sel`| 3 |Number of years to average over to get the future selection patern (fishing mortality at age which will be scaled based on canges in $\bar{F}$)|

Table: (\#tab:initcond) Initial conditions

One can also define if those assumptions will be based on the mean value over the time period set, or randomly sampled from historical values, through setting the argument `fun` to `mean` or `sample`, respectively.

For the next examples we'll use the approach of fitting the stock recruitment within the assessment together with other parameters. We'll set to 20 the number of years to compute mean weights at age, to 10 the number of years to average across and estimate the selection pattern in terms of fishing mortality at age. Finally, we'll use a 10 year period to compute the average discard ratio.

```{r, warning=FALSE, message=FALSE}
fmod <- ~ s(age, k = 8) + s(year, k = 25) + te(age, year, k = c(6, 15))
fit00 <- sca(ple4, ple4.indices, srmodel=~geomean(), fmodel=fmod)
sr00 <- as(fit00, "FLSR")
stk00 <- ple4 + fit00
stk00 <- fwdWindow(stk00, end = endpy, years = list(wt = 20, catch.sel = 10, discards.ratio = 10), fun = list(wt = "sample"))
trg00 <- fwdControl(year = inipy:endpy, quant = "f", value = 0.3)
stk04 <- fwd(stk00, control=trg00, sr=sr00)
```

```{r, echo=FALSE, fig.cap="Stochastic projections of stock for 5 years with fixed fishing mortality and recruitment. Two scenarios with different assumptions about initial conditions"}
plot(FLStocks(default=stk01, alternative=stk04))
```

## Scenarios

There's a wide range of scenarios that can be of interest to project in order to give advice to policy makers, or to better understand the fitted stock assessment model. For example, projecting the stock in the absence of fishing for a few generations, gives good insights about the dynamics of the population being modelled.

```{r, warning=FALSE, message=FALSE}
fmod <- ~ s(age, k = 8) + s(year, k = 25) + te(age, year, k = c(6, 15))
fit00 <- sca(ple4, ple4.indices, srmodel=~geomean(), fmodel=fmod)
sr00 <- as(fit00, "FLSR")
stk00 <- ple4 + simulate(fit00, nsim)
# set projection
projy <- 25
endpy <- maxy + projy
inipy <- maxy + 1
stk00 <- fwdWindow(stk00, end = endpy)
trg00 <- fwdControl(year = inipy:endpy, quant = "f", value = 0)
# recruitment uncertainty
res00 <- residuals(sr00)
rec00 <- window(rec(stk00), inipy, endpy)
rec00 <- rlnorm(rec00, mean(res00), sqrt(var(res00)))
# project
stk05 <- fwd(stk00, control=trg00, sr=sr00, residuals=rec00)
```

```{r, echo=FALSE, fig.cap="Stochastic projection of stock for 25 years in the absence of fishing"}
plot(stk05)
```

These scenarios are defined by the target quantities one's trying to achieve. Table \@ref(tab:trgqts) `FLasher` there are the following target quantities:

| Target | Description |
|---:|:---------|
|ssb | The Spawning Stock Biomass at spawning time |
|ssb_spawn| The Spawning Stock Biomass at spawning time |
|ssb_end| The Spawning Stock Biomass at the end of the time period |
|ssb_flash| The Spawning Stock Biomass at the beginning of the following year, kept for retro compatibility with package `FLash` [@flash] |
|biomass_spawn| Total Stock Biomass at spawning time|
|biomass_end| Total Stock Biomass at the end of the time period |
|f | Fishing mortality over the time period|
|fbar| Fishing mortality over the time period|
|catch| The total catch over the time period|
|landings| The total landings over the time period|
|discards| The total discards over the time period|
|effort| The total effort over the time period|
|revenue| The total revenue over the time period|
Table: (\#tab:trgqts) Target quantities and their description

When projecting the stock under the conditions defined by the scenario one can mix several quantities. For example it may be interesting to project an initial situation of growing the stock followed by a higher exploitation to evaluate how catches would behave.

```{r, warning=FALSE, message=FALSE}
trg00 <- fwdControl(year = inipy:endpy, quant = c(rep("ssb_end", 15), rep("f", 10)), value = c(rep(2000000, 15), rep(0.3, 10)))
stk06 <- fwd(stk00, control=trg00, sr=sr00, residuals=rec00)
```

```{r, echo=FALSE, fig.cap="Stochastic projection of stock for 25 years with fixed SSB for 15 years followed by fixed fishing mortality for 10 years and constant recruitment"}
plot(stk06)
```

## Relative scenarios

Another scenario that is very useful when advising decision makers is to have objectives which are relative to previous performances. For example one could increase spawning stock biomass by 10% each year. This is done buy using the argument `relYear` and setting `value` in relative terms, 1.1.

```{r, warning=FALSE, message=FALSE}
fit00 <- sca(ple4, ple4.indices, srmodel=~geomean())
sr00 <- as(fit00, "FLSR")
stk00 <- ple4 + simulate(fit00, nsim)
# set projection
projy <- 5
endpy <- maxy + projy
inipy <- maxy + 1
stk00 <- fwdWindow(stk00, end = endpy)
trg00 <- fwdControl(year = inipy:endpy, quant = "ssb_end", value = 1.1, relYear = inipy:endpy-1)
# recruitment uncertainty
res00 <- residuals(sr00)
rec00 <- window(rec(stk00), inipy, endpy)
rec00 <- rlnorm(rec00, mean(res00), sqrt(var(res00)))
# project
stk07 <- fwd(stk00, control=trg00, sr=sr00, residuals=rec00)
```

Similar scenarios can be set for all quantities and any years to use as reference. The next example sets a scenario where $SSB$ levels are set in relation to the most recent estimate out of the assessmeent.

```{r, warning=FALSE, message=FALSE}
trg00 <- fwdControl(year = inipy:endpy, quant = "ssb_end", value = 1.1, relYear = maxy)
stk08 <- fwd(stk00, control=trg00, sr=sr00, residuals=rec00)
```

```{r, echo=FALSE, fig.cap="Stochastic projection of stock for 25 years with fixed SSB for 15 years followed by fixed fishing mortality for 10 years and constant recruitment. scn01 = 10% SSB growth relative to previous year; scn02 = 10% higher SSB relative to most recent estimate"}
plot(window(FLStocks(scn01 = stk07, scn02 = stk08), start = 2000))
```

## Limits

An important element when projecting the stock forward is to keep the performance of the fishery within some boundaries. A common one requested by the industry is to keep catches within some stability. `fwd()` can include those constraints using the `min` and `max` arguments. The next example sets the minimum future catches to half of mean historical catches.

```{r, warning=FALSE, message=FALSE}
minc <- 0.2*mean(catch(stk00), na.rm=TRUE)
trg00 <- fwdControl(year = inipy:endpy, quant = rep(c("ssb_end", "catch"), projy), value = rep(c(1500000, NA), projy), min=rep(c(NA, minc), projy))
stk09 <- fwd(stk00, control=trg00, sr=sr00, residuals=rec00)
```

```{r, echo=FALSE, fig.cap="Stochastic projection of stock for 25 years with SSB target of 1500000t and catch limit of 50% historical catches"}
plot(window(stk09, start = 2000))
```

## Harvest Control Rules (HCR)

Harvest Control Rules (HCR) can be complex and of many shapes (REF). We'll keep our examples simple to demonstrate the mechanism of coding HCR.

HCR are decision algorithms that can be used to codify the decision making process, allowing for longer term stability of management decisions in fisheries.

The HCR we're going to explore is based on a target and a limit. The target is applied to the management objective and represents the intent of management. The limit is applied to the process we want to use as trigger for protective actions. For example, the objective of the management system is to extract the highest catches possible for a very long time (aka equilibrium). However, due to natural variability and scientific uncertainty, it can happen that the stock's biomass decreases below what's expected, in which case decision makers want to make sure the stock stays healthy and productive. This situation can be translated into a target of fishing mortality at the level that extracts the Maximum Sustainable Yield, and a biomass limit of *e.g.* half the biomass that would produce the referred catches, everything being in a stable equilibrium. If $SSB$ falls below the limit then fishing mortality is set at 80% of the target.

Such HCR could be written as

$if \quad SSB_y > 0.5 \times B_{MSY} \quad then \quad F_{y+1} = F_{MSY} \quad or \quad else \quad F_{y+1} = 0.8 \times F_{MSY}$

and coded like

```{r, warning=FALSE, message=FALSE}
# fit model
fit00 <- sca(ple4, ple4.indices, srmodel=~geomean())
stk00 <- ple4 + fit00
# create stock recruitment model object
sr00 <- as(fit00, "FLSR")
# estimate reference points
brp00 <- FLBRP(stk00, sr=sr00)
brp00 <- brp(brp00)
ftrg <- refpts(brp00)['msy','harvest']
blim <- 0.5*refpts(brp00)['msy','biomass']
# set projection
# most recent year in the data
maxy <- range(ple4)["maxyear"]
# number of years to project
projy <- 2
# last year for projections
endpy <- maxy + projy
# initial year for projections
inipy <- maxy + 1
# extend stock object to store projection's results
stk00 <- fwdWindow(stk00, end = endpy)
# set the controls for the projections
trg00 <- fwdControl(year = inipy:endpy, quant = "f", value = ftrg)
trg01 <- fwdControl(year = inipy:endpy, quant = "f", value = 0.8*ftrg)
# project
if(ssb(stk00)[,ac(maxy)] > blim){
        stk10 <- fwd(stk00, control=trg00, sr=sr00)
    } else {
        stk10 <- fwd(stk00, control=trg01, sr=sr00)
}
```

```{r, echo=FALSE, fig.cap="Projection of stock for 2 years following a HCR with a target of FMSY and limit of 50% SSBMSY"}
plot(window(stk10, start = 2000))
```

<!--chapter:end:12-fwd.Rmd-->

# Annex - stock assessment workflow

The following sections describes a potential workflow for fitting a `a4a` stock assessment model. The idea is to explore the age and year effects in isolation and adjust the model's smoothness to model those effects.

The procedure is heavily supported by residuals' analysis. In a well specified model residuals should show a random pattern, without any trend or very high values (outliers).

## The "mean" model

To start the analysis we'll fit a "mean" model, where all submodels will be set to an overall average, by using the $\sim 1$ formula. This will be our reference model to see how adding age and year effects will show up in the diagnostic tools, in particular in the residuals.

```{r, message=FALSE, warning=FALSE}
library(FLa4a)
data(hke1567)
data(hke1567.idx)
fit01 <- sca(hke1567, hke1567.idx,
    fmod=~1,
    qmod=list(~1),
    srmod=~1,
    vmod=list(~1, ~1),
    n1mod=~1)
res01 <- residuals(fit01, hke1567, hke1567.idx)
```

The common residuals plot clearly shows a time trend for each age (Figure \@ref(fig:meanresbyage)) for both datasets. Furthermore, inspecting how catch ate age residuals are positioned across ages, by comparing the level of residuals for each age, one can see the pattern of lower than 0 residuals in age 0, reversing the signal for ages 1 and 2, close to 0 in ages 3 and 4, and again below 0 in age 5.


```{r, meanresbyyear, fig.cap="Mean fit residuals by year)"}
plot(res01)
```

This pattern becomes more apparent when plotting the residuals by age across years.

```{r, meanresbyage, fig.cap="Mean fit residuals by age"}
plot(res01, auxline="l", by="age")
```

## The age effects

The following models will introduce age effects in the fishing mortality submodel and catchability submodel. In the fishing mortality submodel we'll introduce a `factor` which means that there will be as many parameters as ages minus 1 and each parameters will be independent of each other.

```{r}
fit02 <- sca(hke1567, hke1567.idx,
    fmod=~factor(age),
    qmod=list(~1),
    srmod=~1,
    vmod=list(~1, ~1),
    n1mod=~1)
res02 <- residuals(fit02, hke1567, hke1567.idx)
```

The residuals plot now shows catch at age residuals less stagered, reflecting the modelling of the age effect.

```{r, fageresbyyear, fig.cap="Fishing mortality model with age effect residuals by year"}
plot(res02)
```

The residuals plot by age shows the same outcome.

```{r, fageresbyage, fig.cap="Fishing mortality model with age effect residuals by age"}
plot(res02, auxline="l", by="age")
```

We'll now proceed adding an age effect to the catchability model while removing the catch at age effect.

```{r}
fit03 <- sca(hke1567, hke1567.idx,
    fmod=~1,
    qmod=list(~factor(age)),
    srmod=~1,
    vmod=list(~1, ~1),
    n1mod=~1)
res03 <- residuals(fit03, hke1567, hke1567.idx)
```

```{r, qageresbyyear, fig.cap="Index catchability model with age effect residuals by year"}
plot(res03)
```

The residuals plot by age shows the same outcome.

```{r, qageresbyage, fig.cap="Index catchability model with age effect residuals by age"}
plot(res03, auxline="l", by="age")
```

Finally both effects are brought together.

```{r}
fit04 <- sca(hke1567, hke1567.idx,
    fmod=~factor(age),
    qmod=list(~factor(age)),
    srmod=~1,
    vmod=list(~1, ~1),
    n1mod=~1)
res04 <- residuals(fit04, hke1567, hke1567.idx)
```

```{r, fqageresbyyear, fig.cap="Fishing mortality and index catchability models with age effect residuals by year"}
plot(res04)
```

The residuals plot by age shows the same outcome.

```{r, fqageresbyage, fig.cap="Fishing mortality and index catchability models with age effect residuals by age"}
plot(res04, auxline="l", by="age")
```

## Year effect on fishing mortality

This model will introduce an year effect in the fishing mortality submodel on top of the F age effect added before. Inspecting the last set of residuals (Figure \@ref(fig:fqageresbyage)) one can easily see the pattern across years with more positive residuals in the beggining of the time series and more negative in the most recent years. As for age we're using a `factor` for years. The new model's residuals won't show such a pronounced effect anymore (Figures \@ref(fig:fyearresbyyear) and \@ref(fig:fyearresbyage)).

```{r}
fit05 <- sca(hke1567, hke1567.idx,
    fmod=~factor(age) + factor(year),
    qmod=list(~1),
    srmod=~1,
    vmod=list(~1, ~1),
    n1mod=~1)
res05 <- residuals(fit05, hke1567, hke1567.idx)
```

```{r, fyearresbyyear, fig.cap="Fishing mortality model with year effect residuals by year"}
plot(res05)
```

The residuals plot by age shows the same outcome.

```{r, fyearresbyage, fig.cap="Fishing mortality model with year effect residuals by age"}
plot(res05, auxline="l", by="age")
```

We can see now that the residuals show a lot less patterns than before. There's still some issues, the survey catchability seems to have an year trend. However the model is not fully specified yet, stock recruitment is modelled as constant over time, the initial population abundance is also modelled as a constant as well as the variance models.

## Year effect on catchability

It's uncommon to include year trends on the abundance index catchability model. Such decision needs to be considered carefully as the trend in the index, in the case of a well design scientific survey, should result from a change in abundance. Modelling that trend would attribute such change to the survey design and remove it from the abundance. If the survey index is based on a commercial CPUE it becomes more likely that changes in selectivity or fishing behaviour could show up in the index as changes in abundance. Although the common process of standardizing CPUEs should deal with technical issues.

In the case of adding year effects to teh catchability submode the same formulas can be used, to include period breaks, trends, etc.

## The initial year population abundance model, aka N1

This model sets the n-at-age in the first year of the time series, which is needed due to the lack of previous data to reconstruct those cohorts. It will affect the population numbers in the lower triangle of the initial population matrix and catches.

The following model will introduce an age effect in the population abundance in the first year of the time series.

```{r}
fit06 <- sca(hke1567, hke1567.idx,
    fmod=~factor(age) + factor(year),
    qmod=list(~factor(age)),
    srmod=~1,
    vmod=list(~1, ~1),
    n1mod=~factor(age))
res06 <- residuals(fit06, hke1567, hke1567.idx)
```

The best way to inspect the effect of this model is to zoom into the initial years of the time series.

```{r, echo=FALSE}
r06 <- window(res06, end="2010")
class(r06) <- "a4aFitResiduals"
r05 <- window(res05, end="2010")
class(r05) <- "a4aFitResiduals"
```

Figure \@ref(fig:n1resbyage05) zooms into the previous model, which used an intercept only model for N1, while Figure \@ref(fig:n1resbyage06).

```{r, n1resbyage05, fig.cap="N1 fitted as an intercept only model: 2007 - 2010 residuals by age", echo=FALSE}
plot(r05, auxline="l", by="age")
```

```{r, n1resbyage06, fig.cap=, fig.cap="N1 fitted with an age effect model: 2007 - 2010 residuals by age", echo=FALSE}
plot(r06, auxline="l", by="age")
```

Comparing the two plots it can be seen the effect of modeling abundance in the initial year. Residuals for both catch at age and catchability improved considerably. The following years also improve to different levels.

## The stock recruitment submodel

In this example we'll simply add a model to allow recruitment to vary over time and we'll see how to track potential improvements in the residuals.

```{r}
fit07 <- sca(hke1567, hke1567.idx,
    fmod=~factor(age) + factor(year),
    qmod=list(~factor(age)),
    srmod=~factor(year),
    vmod=list(~1, ~1),
    n1mod=~factor(age))
res07 <- residuals(fit07, hke1567, hke1567.idx)
```

The residuals plot by year are very useful to see the effect of adding a varying stock recruitment model. The year trends present in previous models are not absent. Recruitment variability when left unmodelled was being picked up by trends in the survey catchability and catch at age. And due to the cohort dynamics underlying the catch at age model, where propagating into other ages' estimates.

```{r, srresbyyear, fig.cap="Stock-recruitment model with year effect residuals by year"}
plot(res07)
```

## The variance submodel

Finally, we're testing the variance submodel, specifically the catch at age variance model. We won't dig into the catchability variance model though. It's common to accept that a scientific survey following a well designed sampling protocol will have equal variance across ages since no preferential areas should be sampled sampled.

The variance model will use a smoother with `k=3`. The expectation is that the variance model will have a U-shape, since yourger and older ages are usually less caught and as such estomates of those ages will have larger variances than fully exploited ages.

```{r}
fit08 <- sca(hke1567, hke1567.idx,
    fmod=~factor(age) + s(year, k=10),
    qmod=list(~factor(age)),
    srmod=~s(year, k=10),
    vmod=list(~s(age, k=3), ~1),
    n1mod=~factor(age))
```

We'll use the pearson residuals for this analysis since those are standardized by the predicted variances of the model and not the residual variance itself, like the more common standardized residuals. Figure \@ref(fig:vresbyyear08) shows an improve set of residuals when compared to Figure \@ref(fig:vresbyyear07) which add an intercept only model for the variance model.

```{r}
res07 <- residuals(fit07, hke1567, hke1567.idx, type="pearson")
res08 <- residuals(fit08, hke1567, hke1567.idx, type="pearson")
```

```{r, vresbyyear07, fig.cap="Variance model with intercept only age effect pearson residuals"}
plot(res07)
```

```{r, vresbyyear08, fig.cap="Variance model with age effect pearson residuals"}
plot(res08)
```

To see what's happening with the variance model one can use predict to plot the different models fitted.

```{r, vagepredbyage, fig.cap="Variance models for catch at age", echo=FALSE}
flqs <- FLQuants(mod08=predict(fit08)$vmodel$catch[,"2022"],
    mod07=predict(fit07)$vmodel$catch[,"2022"])
xyplot(data~age, data=flqs, group=qname, type="l", auto.key=T)
```

To see the effect these models have on the estimated quantities one can look at the variance of the estimates:

```{r, vage, fig.cap="Estimates of population abundance with different variance models", echo=FALSE}
flqs <- FLQuants(mod08=catch.n(hke1567+simulate(fit08, nsim=500))[,"2022"], mod07=catch.n(hke1567+simulate(fit07, nsim=500))[,"2022"])
bwplot(data~qname|factor(age),  data=as.data.frame(flqs), scales="free", auto.key=T)
```

## Final comments

The sequence presented here can be changed and applied in any order the user is interested or prefers to.

The approach of not allowing year effects in surveys and variance model can be modified if the user prefers to do so.

The (ab)use of `factor` is for demonstration purposes only. The user is incentivised to explore other model forms, in particular smoothers.








<!--chapter:end:30-annexworkflow.Rmd-->

# References

<!--chapter:end:99-References.Rmd-->

