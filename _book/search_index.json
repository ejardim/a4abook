[["the-statistical-catch-at-age-stock-assessment-framework-with-markov-chain-monte-carlo-mcmc.html", "14 The statistical catch-at-age stock assessment framework with Markov Chain Monte Carlo (MCMC) 14.1 The MCMC method for sca 14.2 Diagnostics with CODA", " 14 The statistical catch-at-age stock assessment framework with Markov Chain Monte Carlo (MCMC) The previous methods were demonstrated using maximum likelihood estimation (MLE). However, ADMB also supports Markov Chain Monte Carlo (MCMC) methods, which provide significant advantages, particularly when working with complex models that involve many parameters. The key difference is that while MLE finds a single best estimate of parameters by maximizing the likelihood function, MCMC offers a broader perspective by generating an entire distribution of possible values. This approach is more informative because it does not just give the most likely estimate but also helps us understand the uncertainty surrounding it. With MCMC, researchers can incorporate prior knowledge and obtain results that are often more realistic and reliable (Gelman et al., 2013). This is especially useful when dealing with complicated models where traditional likelihood-based methods struggle, as MCMC allows for efficient exploration of possible solutions without requiring an exact mathematical formulation (Gilks, Richardson, &amp; Spiegelhalter, 1996; Robert &amp; Casella, 2004). One of the biggest advantages of MCMC is its flexibility when working with models that have irregular behavior, such as those with multiple peaks or abrupt changes in likelihood. Standard MLE methods assume that the likelihood function behaves smoothly, like a well-shaped bowl, but this is rarely true in real-world applications. In fisheries, ecology, and other applied sciences, models often have parameters that interact in complex ways, creating likelihood surfaces with ridges and multiple solutions. In these cases, MLE can easily get stuck in a local peak, failing to find the best possible estimate or underestimating the real uncertainty in the system (Neal, 1993). Since MCMC uses a probabilistic sampling approach instead of strict optimization, it moves freely across the entire space of possible values, making it more robust and adaptable to challenging problems (Robert &amp; Casella, 2013). Traditional MLE-based uncertainty estimation relies on the Hessian matrix, which essentially measures how quickly the likelihood function changes as parameters vary. This method assumes that the shape of the likelihood function is roughly the same everywhere—meaning that a quadratic (bowl-like) approximation is valid (Pawitan, 2001). However, this assumption is often unrealistic, especially in models with many parameters or correlations between them, as is common in fisheries stock assessment models. Furthermore, MLE uncertainty estimates require a large sample size for them to be accurate, which is not always available in real-world applications (Wasserman, 2004; van der Vaart, 1998). If these assumptions do not hold, MLE can give misleading confidence intervals, making decision-making riskier. Additionally, MLE assumes that the model is correctly specified—meaning that it accurately represents the real system being studied. If the model is misspecified or overly simplified, the Hessian-based uncertainty estimates may be highly unreliable, requiring alternative approaches like robust standard errors or resampling methods (White, 1982). In fields like fisheries science, where models often involve multiple correlated parameters, MCMC provides a much more flexible and realistic way to estimate uncertainty. Unlike MLE, which assumes uncertainty follows a simple symmetrical pattern, MCMC can handle more complex distributions, giving a better representation of real-world variability. This is especially important when estimating key fisheries management indicators, such as spawning stock biomass (\\(SSB\\)) or fishing mortality (\\(F\\)), which influence critical policy decisions. Because MCMC does not impose strict mathematical assumptions about the shape of uncertainty, it produces estimates that are more reflective of real-world conditions, ultimately leading to more informed and reliable management strategies. ADMB’s approach to Markov Chain Monte Carlo (MCMC) enhances Bayesian analysis by efficiently exploring parameter uncertainty in complex models. Unlike standard MCMC tools, ADMB leverages automatic differentiation to improve sampling efficiency and speed (Fournier et al., 2012). It supports various sampling algorithms, including Metropolis-Hastings and Hamiltonian Monte Carlo, which help navigate high-dimensional parameter spaces and complex likelihood structures more effectively. This makes ADMB particularly useful in applied sciences like fisheries and ecology, where uncertainty estimation is crucial for decision-making. Additionally, ADMB provides built-in diagnostics to assess MCMC convergence and reliability, ensuring that posterior distributions are well-explored and results are robust (Gelman et al., 2013). To evaluate the quality of MCMC sampling, ADMB offers several key diagnostics. Autocorrelation analysis detects dependencies between successive samples, while the effective sample size (ESS) measures the number of independent samples in the chain. The Gelman-Rubin diagnostic (\\(\\hat{R}\\)) helps assess whether multiple chains have converged to the same distribution, with values close to 1 indicating good convergence. Trace plots visually inspect parameter behavior over iterations, revealing trends or poor mixing. Additionally, ADMB monitors the acceptance rate to ensure efficient sampling and provides posterior density estimates to check if the distribution has been properly explored. These tools help users refine their MCMC runs, adjusting sampling length or proposal distributions to improve performance and ensure reliable uncertainty estimates. The manual “A Guide for Bayesian Analysis in AD Model Builder” by Cole C. Monnahan, Melissa L. Muradian and Peter T. Kuriyam describe and explain a larger group of arguments that can be set when running MCMC with ADMB, which the a4a uses. 14.0.1 References: Brooks, S., Gelman, A., Jones, G., &amp; Meng, X.-L. (2011). Handbook of Markov Chain Monte Carlo. CRC Press. Gamerman, D., &amp; Lopes, H. F. (2006). Markov Chain Monte Carlo: Stochastic Simulation for Bayesian Inference. CRC Press. Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., &amp; Rubin, D. B. (2014). Bayesian Data Analysis (3rd ed.). CRC Press. Neal, R. M. (1993). Probabilistic inference using Markov Chain Monte Carlo methods. Department of Computer Science, University of Toronto. Robert, C., &amp; Casella, G. (2013). Monte Carlo Statistical Methods (2nd ed.). Springer. Casella, G., &amp; Berger, R. L. (2002). Statistical Inference (2nd ed.). Duxbury. Lehmann, E. L., &amp; Casella, G. (1998). Theory of Point Estimation (2nd ed.). Springer. Pawitan, Y. (2001). In All Likelihood: Statistical Modelling and Inference Using Likelihood. Oxford University Press. van der Vaart, A. W. (1998). Asymptotic Statistics. Cambridge University Press. Wasserman, L. (2004). All of Statistics: A Concise Course in Statistical Inference. Springer. White, H. (1982). “Maximum Likelihood Estimation of Misspecified Models.” Econometrica, 50(1), 1-25. Fournier, D. A., Skaug, H. J., Ancheta, J., Ianelli, J., Magnusson, A., Maunder, M. N., Nielsen, A., &amp; Sibert, J. (2012). AD Model Builder: Using automatic differentiation for statistical inference of highly parameterized complex nonlinear models. Optimization Methods and Software, 27(2), 233–249. Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., &amp; Rubin, D. B. (2013). Bayesian Data Analysis (3rd ed.). CRC Press. 14.1 The MCMC method for sca This section shows how the sca methods interface with ADMB to use the MCMC fits. For this section we’ll use the hake assessment in mediterranean areas (gsa) 1, 5, 6 and 7. We’ll start buy fitting the MLE model and afterwards call the MCMC methods. The outcomes of the MCMC fit need to be inspected to make sure the chain converged and the results are robust. A set of diagnostics are available to do this work. [TO CHECK] For many Bayesian software platforms, the MCMC algorithms are started at user-specified or arbitrary places. ADMB has the advantage that it can robustly estimate the posterior mode and the covariance at that point. This information is very valuable in initializing the MCMC chain. Specifically, an MCMC chain starts from the posterior mode and uses the estimated covariance matrix in its proposed jumps (see the algorithm sections below). As such, ADMB chains typically do not need a long period to reach areas of high density. However, we caution the user to always check the MCMC output as other issues may lead to a chain that needs a longer burn-in. (REF TO Monnahan) # load libraries and data library(FLa4a) library(ggplotFL) data(hke1567) data(hke1567.idx) nsim &lt;- 250 # MLE estimate fmod &lt;- ~s(age, k = 4) + s(year, k = 8) + s(year, k = 8, by = as.numeric(age == 0)) + s(year, k = 8, by = as.numeric(age == 4)) qmod &lt;- list(~I(1/(1 + exp(-age)))) fit &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod) fit &lt;- simulate(fit, nsim) To run the MCMC method, one needs to configure a set of arguments, which is done by creating a SCAMCMC object. Table 14.1 describes the arguments available to run the MCMC method, extracted from Monnahan [ref]. For more details on the MCMC configuration in ADMB visit the ADMB website. Table 14.1: ADMB MCMC arguments Argument Description mcmc N Run N MCMC iterations mcsave N Save every N th MCMC iteration mcscale N Rescale step size for first N iterations mcmult N Rescale the covariance matrix mcrb N Reduce high parameter correlations mcprobe X Use a fat-tailed proposal distribution mcdiag Use a diagonal covariance matrix mcnoscale Do not scale the algorithm during mcu Use a uniform distribution as proposal distribution hybrid Use the hybrid method hynstep N Mean number of steps for the leapfrog method hyeps X The stepsize for the leapfrog method [X numeric and &gt; 0] # mcmc mc &lt;- SCAMCMC() # check the default pars mc ## An object of class &quot;SCAMCMC&quot; ## Slot &quot;mcmc&quot;: ## [1] 10000 ## ## Slot &quot;mcsave&quot;: ## [1] 100 ## ## Slot &quot;mcscale&quot;: ## [1] NaN ## ## Slot &quot;mcmult&quot;: ## [1] NaN ## ## Slot &quot;mcrb&quot;: ## [1] NaN ## ## Slot &quot;mcprobe&quot;: ## [1] NaN ## ## Slot &quot;mcseed&quot;: ## [1] NaN ## ## Slot &quot;mcdiag&quot;: ## [1] FALSE ## ## Slot &quot;mcnoscale&quot;: ## [1] FALSE ## ## Slot &quot;mcu&quot;: ## [1] FALSE ## ## Slot &quot;hybrid&quot;: ## [1] FALSE ## ## Slot &quot;hynstep&quot;: ## [1] NaN ## ## Slot &quot;hyeps&quot;: ## [1] NaN Defaults for now are ok, so lets fit the model. Note that the argument fit must be set to MCMC and the argument mcmc takes the SCAMCMC object. # fit the model fitmc00 &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) # check acceptance rate fitSumm(fitmc00) ## iters ## 1 ## nopar 52.0000 ## nlogl NA ## maxgrad NA ## nobs 176.0000 ## gcv NA ## convergence NA ## accrate 0.3271 As usual fitSumm store relevant information about the model fit. In the case of MCMC fit the information stored is the number of model paramters (nopar), the number of observations (nobs) and the acceptance rate (accrate). plot(hke1567 + fitmc00) 14.2 Diagnostics with CODA In essence, the diagnostics are used to give the analyst confidence that the posterior distribution of the parameters is unbiased, as much as possible with simetric non correlated distributions of each parameter, over which one can make inference. There’s a large body of literature about MCMC convergence. In this section we’ll focus on the out-of-the-box methods available to the stock assessment scientist: trace plots, autocorrelation analysis, geweke diagnostic, acceptance rate, distribution density and cumulative means. We use the package CODA to run the diagnostics on MCMC fits. One needs to convert the sca output into a mcmc CODA object over which several diagostics can be ran. The mcmc object is a matrix with the parameters (row = iters, cols= pars). library(coda) For demonstration purposes we’ll create a chain with 1000 samples (mcmc=1000) and save every iter (mcsave=1), which will create a highly correlated and unstable chain, and update the initial MCMC fit to also have 1000 samples (mcmc=100000, mcsave=100). # update initial fit mc &lt;- SCAMCMC(mcmc=100000, mcsave=100) fitmc01 &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) fitmc01.mc &lt;- FLa4a::as.mcmc(fitmc01) # highly correlated fit mc &lt;- SCAMCMC(mcmc=1000, mcsave=1) fitmc02 &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) fitmc02.mc &lt;- FLa4a::as.mcmc(fitmc02) 14.2.1 Traceplots Trace plots show the sampled values of a parameter over iterations. A plot that looks like a random, stable “cloud” of points with no trends or drifts, with rapid fluctuactions, is a signal of convergence, meaning the chain mixes well and is stationary. If the trace plot shows a strong trend or periodicity, drifts, or long autocorrelated stretches, it means the chain hasn’t converged. Figure 14.1 cleary depicts this difference between the two runs. traceplot(mcmc.list(mc01=fitmc01.mc[,1], mc02=fitmc02.mc[,2]), lwd=2, col=c(2,4), lty=1) Figure 14.1: MCMC chains trace. Correlated chain in blue, uncorrelated chain in red. Ploting the chains for the parameter clearly shows autocorrelation for the first parameter in the blue chain. It also shows an initial phase where the chain seems to be stuck in a single value. This initial phase, when the parameter seems to be stuck in a fixed position, is called the “burn-in” phase. These iterations can be dropped with the burnin method (Figure 14.2), although it doesn’t sort out the autocorrelation or the parameter density. traceplot(FLa4a::as.mcmc(burnin(fitmc02, 250))[,1], lwd=2, col=4, lty=1) Figure 14.2: MCMC chain with high autocorrelation after removing the initial 250 samples (burnin period). 14.2.2 Autocorrelation analysis Autocorrelation analysis is useful to assess stationarity, a stationary chain should have low autocorrelation, meaning that each sample is approximately independent. On the opposite, high autocorrelation indicates slow mixing and possible non-stationarity. The autocorrelation plot produced by the acf function, will show correlation along the chain for each parameter at different lags. Figure 14.3 shows there’s a strong autocorrelation for the first parameter in the blue chain, which we’d like to avoid. acfplot(mcmc.list(mc01=fitmc01.mc[,1], mc02=fitmc02.mc[,2]), type=&quot;p&quot;, pch=19, col=c(2,4)) Figure 14.3: Autocorrelation plot of the first parameter in the MCMC chain. Correlated chain in blue, uncorrelated chain in red. Autocorrelation analysis is also useful for this diagnostics. In a good mixed chain autocorrelation drops quickly to near zero, while a poor mixing will display high autocorrelation, meaning successive samples are too correlated, reducing efficiency. Figure 14.3 cleary depicts this difference, the blue chain shows a very high level of auto-correlation while the red chain drops very quickly to values around 0. 14.2.3 Geweke diagnostic The geweke diagnostic computes the Geweke-Brooks Z-score (Geweke, J. (1992). “Evaluating the accuracy of sampling-based approaches to the calculation of posterior moments.” In Bayesian Statistics 4, eds. J.M. Bernardo, J.O. Berger, A.P. Dawid, and A.F.M. Smith, pp. 169–193. Oxford University Press), which indicates if the first and following parts of a sample from a Markov chain are drawn from the same distribution as the last part of the chain, usualy the last 50% of the samples. It’s useful to decide if the first few iterations should be discarded and provides information about the stability of the chain. Figure 14.4 shows the geweke plot for the MCMC run without thining and Figure 14.4 when the thining was set at 200 samples. geweke.plot(fitmc01.mc[,1], main=&quot;Uncorrelated chain&quot;) geweke.plot(fitmc02.mc[,1], main=&quot;Correlated chain&quot;) Figure 14.4: Geweke plot of the first parameter in the MCMC chains The panel on the left shows a much more regular chain, where the different blocks of data show similar distributions. The panel on the right clearly shows the z-score statistic out of the confidence intervals until 400 samples are discarded, which points to the need to drop a set of initial samples. The geweke diagnostic is also a good way to look at mixing by comparing the mean and variance of the first part of the chain to the last part. Good mixing will show no significant difference between early and late samples. Poor mixing will show large differences, indicating the chain has not explored the posterior fully. 14.2.4 Cumulative means Inspecting the cumulative mean along the chain is another good way to check for the stability of the chain. When the mixing is good the mean stabilizes quickly, and vice-versa if not. cm01 &lt;- fitmc01.mc[,1] cm01 &lt;- cumsum(cm01) / seq_along(cm01) cm02 &lt;- fitmc02.mc[,1] cm02 &lt;- cumsum(cm02) / seq_along(cm02) plot(cm01, type=&quot;l&quot;, xlab=&quot;samples&quot;, ylab=&quot;mean&quot;, main=&quot;Uncorrelated chain&quot;) plot(cm02, type=&quot;l&quot;, xlab=&quot;samples&quot;, ylab=&quot;mean&quot;, main=&quot;Correlated chain&quot;) Figure 14.5: Cumulative mean plots of the first parameter in the MCMC chains 14.2.5 Distribution density An important element of MCMC is to produce symetric posterior distributions, for one it’s a sign that the chain explored the space of the parameter, for other it makes inference about the parameters a lot more robust. If the distributions are skewed or multimodal, estimating the expected value and variance becomes a lot more complicated. As such having symetric distributions is preferred and should be checked before computing statistics of interest. Figure 14.6 shows the density plots for both runs, where it shows the symetric distribution of the uncorrelated chain (left panel) and the bimodal distribution of the correlated chain. densplot(fitmc01.mc[,1], main=&quot;Uncorrelated chain&quot;) densplot(fitmc02.mc[,1], main=&quot;Correlated chain&quot;) Figure 14.6: Density plots of the first parameter in the MCMC chains "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
