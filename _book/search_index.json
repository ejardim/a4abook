[["index.html", "Fish stock assessment with R The a4a Initiative 1 Before starting 1.1 Installing and loading libraries 1.2 How to read this document 1.3 How to get help 1.4 Notation 1.5 Acknowledgements 1.6 License, documentation and development status", " Fish stock assessment with R The a4a Initiative Ernesto Jardim, Colin Millar, Danai Mantopoulou Palouka and Iago Mosqueira 2025-04-17 1 Before starting 1.1 Installing and loading libraries To run the methods in this book the reader will need to install the FLa4a package (C. Millar and Jardim 2025) and its dependencies. Some datasets are distributed with the package and as such need to be loaded too. # from CRAN install.packages(c(&quot;copula&quot;,&quot;triangle&quot;, &quot;coda&quot;, &quot;grid&quot;, &quot;gridExtra&quot;, &quot;latticeExtra&quot;)) # from FLR install.packages(c(&quot;FLCore&quot;, &quot;FLa4a&quot;), repos=&quot;http://flr-project.org/R&quot;) # libraries library(devtools) library(FLa4a) library(XML) library(reshape2) library(ggplotFL) # datasets data(ple4) data(ple4.indices) data(ple4.index) data(rfLen) packageVersion(&quot;FLCore&quot;) ## [1] &#39;2.6.20.9333&#39; packageVersion(&quot;FLa4a&quot;) ## [1] &#39;1.9.1&#39; 1.2 How to read this document The target audience for this document are readers with some experience in R and some background on stock assessment. The document explains the approach being developed by the Assessment for All Initiative (a4a) for fish stock assessment and scientific advice. It presents a mixture of text and code, where the first explains the concepts behind the methods, while the last shows how these can be run with the software provided. Moreover, having the code allows the reader to copy/paste and replicate the analysis presented here. The sections and subsections are as independent as possible, so they can be used as a reference document for the FLa4a. 1.3 How to get help FLa4a is build using R’s object oriented implementation with S4 classes, and FLCore’s (L. T. Kell et al. 2007) class structures and methods. In order to access S4 methods and classes documentation the user needs to use specific terminology. For example, FLStock is one of our main components in order to run our stock assessment model. We can check the structure of an FLStock object as follows: showClass(&quot;FLStock&quot;) ## Class &quot;FLStock&quot; [package &quot;FLCore&quot;] ## ## Slots: ## ## Name: catch catch.n catch.wt discards discards.n ## Class: FLQuant FLQuant FLQuant FLQuant FLQuant ## ## Name: discards.wt landings landings.n landings.wt stock ## Class: FLQuant FLQuant FLQuant FLQuant FLQuant ## ## Name: stock.n stock.wt m mat harvest ## Class: FLQuant FLQuant FLQuant FLQuant FLQuant ## ## Name: harvest.spwn m.spwn name desc range ## Class: FLQuant FLQuant character character numeric ## ## Extends: ## Class &quot;FLS&quot;, directly ## Class &quot;FLComp&quot;, by class &quot;FLS&quot;, distance 2 ## ## Known Subclasses: ## Class &quot;FLStockR&quot;, directly, with explicit coerce The object oriented structure of FLa4a gives the opportunity to change the behavior of a function according to the object that is applied to. For example we can check the available methods of the function plot showMethods(&quot;plot&quot;) ## Function: plot (package base) ## x=&quot;a4aFit&quot;, y=&quot;FLIndices&quot; ## x=&quot;a4aFit&quot;, y=&quot;FLStock&quot; ## x=&quot;a4aFitCatchDiagn&quot;, y=&quot;missing&quot; ## x=&quot;a4aFitMCMCs&quot;, y=&quot;missing&quot; ## x=&quot;a4aFitResiduals&quot;, y=&quot;missing&quot; ## x=&quot;a4aFits&quot;, y=&quot;missing&quot; ## x=&quot;ANY&quot;, y=&quot;ANY&quot; ## x=&quot;color&quot;, y=&quot;ANY&quot; ## x=&quot;Copula&quot;, y=&quot;ANY&quot; ## x=&quot;FLBiol&quot;, y=&quot;FLFishery&quot; ## x=&quot;FLBiol&quot;, y=&quot;missing&quot; ## x=&quot;FLBiols&quot;, y=&quot;missing&quot; ## x=&quot;FLCatch&quot;, y=&quot;missing&quot; ## x=&quot;FLCohort&quot;, y=&quot;missing&quot; ## x=&quot;FLFisheries&quot;, y=&quot;ANY&quot; ## x=&quot;FLFishery&quot;, y=&quot;ANY&quot; ## x=&quot;FLIndex&quot;, y=&quot;missing&quot; ## x=&quot;FLIndexBiomass&quot;, y=&quot;missing&quot; ## x=&quot;FLIndices&quot;, y=&quot;missing&quot; ## x=&quot;FLPar&quot;, y=&quot;missing&quot; ## x=&quot;FLQuant&quot;, y=&quot;FLQuant&quot; ## x=&quot;FLQuant&quot;, y=&quot;fwdControl&quot; ## x=&quot;FLQuant&quot;, y=&quot;missing&quot; ## x=&quot;FLQuantPoint&quot;, y=&quot;FLQuant&quot; ## x=&quot;FLQuantPoint&quot;, y=&quot;FLQuants&quot; ## x=&quot;FLQuantPoint&quot;, y=&quot;missing&quot; ## x=&quot;FLQuants&quot;, y=&quot;FLPar&quot; ## x=&quot;FLQuants&quot;, y=&quot;FLPars&quot; ## x=&quot;FLQuants&quot;, y=&quot;fwdControl&quot; ## x=&quot;FLQuants&quot;, y=&quot;missing&quot; ## x=&quot;FLSR&quot;, y=&quot;missing&quot; ## x=&quot;FLSRs&quot;, y=&quot;ANY&quot; ## x=&quot;FLStock&quot;, y=&quot;FLPar&quot; ## x=&quot;FLStock&quot;, y=&quot;FLStock&quot; ## x=&quot;FLStock&quot;, y=&quot;FLStocks&quot; ## x=&quot;FLStock&quot;, y=&quot;fwdControl&quot; ## x=&quot;FLStock&quot;, y=&quot;missing&quot; ## x=&quot;FLStocks&quot;, y=&quot;FLPar&quot; ## x=&quot;FLStocks&quot;, y=&quot;fwdControl&quot; ## x=&quot;FLStocks&quot;, y=&quot;missing&quot; ## x=&quot;mvdc&quot;, y=&quot;ANY&quot; ## x=&quot;profile.mle&quot;, y=&quot;missing&quot; by calling showMethods R prints all the possible uses of the plot function. We want to see what it does when it is called on an FLStock object with no other object. We observe that plot takes two arguments, x and y. So, in the signature of the getMethod function we are going to use, we need to define both x and y. getMethod(&#39;plot&#39;, signature = list(&quot;FLStock&quot;,&quot;missing&quot;)) ## Method Definition: ## ## function (x, y, ...) ## { ## .local &lt;- function (x, metrics = list(Rec = rec, SSB = ssb, ## Catch = catch, F = fbar), na.rm = TRUE, ...) ## { ## if (all(is.na(harvest(x))) &amp; missing(metrics)) ## metrics &lt;- list(Catch = catch, Landings = landings, ## Discards = discards) ## metrics &lt;- metrics(x, metrics = metrics) ## if (&quot;F&quot; %in% names(metrics)) ## units(metrics$F) &lt;- paste0(range(x, c(&quot;minfbar&quot;, ## &quot;maxfbar&quot;)), collapse = &quot;-&quot;) ## if (&quot;SSB&quot; %in% names(metrics)) { ## if (all(dimnames(metrics$SSB)$unit %in% c(&quot;F&quot;, &quot;M&quot;))) { ## metrics$SSB &lt;- metrics$SSB[, , &quot;F&quot;] + metrics$SSB[, ## , &quot;M&quot;] ## if (&quot;Rec&quot; %in% names(metrics)) ## metrics$Rec &lt;- unitSums(metrics$Rec) ## } ## } ## p &lt;- plot(metrics, na.rm = na.rm, ...) + ylim(c(0, NA)) ## if (&quot;SSB&quot; %in% names(metrics)) ## if (all(dimnames(metrics$SSB)$unit %in% c(&quot;F&quot;, &quot;M&quot;))) { ## return(p + theme(legend.position = &quot;bottom&quot;, ## legend.key = element_blank()) + labs(color = &quot;Sex&quot;) + ## scale_color_manual(name = &quot;&quot;, labels = c(&quot;Both&quot;, ## &quot;F&quot;, &quot;M&quot;), values = flpalette_colours(3))) ## } ## return(p) ## } ## .local(x, ...) ## } ## &lt;bytecode: 0x5b9c1b38f290&gt; ## &lt;environment: namespace:ggplotFL&gt; ## ## Signatures: ## x y ## target &quot;FLStock&quot; &quot;missing&quot; ## defined &quot;FLStock&quot; &quot;missing&quot; More information can be found in R’s documentation (https://www.r-project.org/). 1.4 Notation Along this chapter the notation presented in Table 1.1 will be used. Mathematical descriptions will be kept as simple as possible for readability. Table 1.1: Mathematical notation Type Symbol Description variables \\(C\\) catches \\(F\\) fishing mortality \\(M\\) natural mortality \\(R\\) recruitment \\(Q\\) vessel or fleet catchability \\(w\\) weights \\(l\\) likelihood \\(I\\) abundance index \\(S\\) spawning stock biomass \\(CV\\) coefficient of variation \\(D\\) residuals or deviances \\(N\\) normal distribution \\(\\beta\\) parameter \\(a\\) stock-recruitment parameter \\(b\\) stock-recruitment parameter \\(\\sigma^2\\) variance of catch \\(\\tau^2\\) variance of index \\(\\phi^2\\) variance of predicted recruitment \\(\\upsilon^2\\) variance of residuals subscripts \\(a\\) age \\(y\\) year \\(C\\) catch \\(I\\) abundance index \\(N\\) normal distribution \\(s\\) survey \\(SR\\) stock recruitment relationship superscripts and accents \\(\\hat{}\\) observation \\(\\tilde{}\\) prediction \\(c\\) catches \\(s\\) abundance index 1.5 Acknowledgements 1.6 License, documentation and development status The software is released under the EUPL 1.1. For more information on the a4a methodologies refer to Jardim et al. (2014), C. P. Millar et al. (2014) and Scott et al. (2016). Documentation can be found at http://flr-project.org/FLa4a. You are welcome to: Submit suggestions and bug-reports at: https://github.com/flr/FLa4a/issues Send a pull request on: https://github.com/flr/FLa4a/ Compose a friendly e-mail to the maintainer, see packageDescription('FLa4a') References "],["introduction.html", "2 Introduction 2.1 The “Assessment for All” Initiative (a4a) 2.2 Multi-stage modelling approach 2.3 Stock Assessment Process 2.4 Stock assessment as a linear model 2.5 Data used in the book", " 2 Introduction 2.1 The “Assessment for All” Initiative (a4a) The European Commission Joint Research Centre’s (JRC) “Assessment for All” Initiative (a4a) was launched to simplify and standardize the complex methodologies often employed in fisheries science, a4a focuses on creating flexible, modular frameworks that can accommodate varying data availability, regional needs, and stakeholder objectives. The JRC started its ‘Assessment for All’ Initiative (a4a), with the aim to develop, test, and distribute methods to assess a large numbers of stocks in an operational time frame, and to build the necessary expertise on stock assessment and advice provision. According to Jardim et al. (2014), the long-term strategy of a4a is to increase the number of stock assessments by reducing the workload required to run each analysis and by bringing more scientists into fisheries management advice. The first is achieved by developing a working framework with the methods required to run all the analyses a stock assessment needs. Such approach should make the model exploration and selection processes easier, as well as decreasing the burden of moving between software platforms. The second can be achieved by making the analysis more intuitive, thereby attracting more experts to join stock assessment teams. One major step to achieve the a4a goals was the development of a stock assessment model that could be applied rapidly to a large number of stocks and for a wide range of applications: traditional stock assessment, conditioning of operating models, forecasting, or informing harvest control rules in MSE algorithms. The modular nature of a4a allows for the integration of data from diverse sources, including biological, environmental, and socioeconomic datasets, ensuring comprehensive assessments. This inclusivity enhances the ability to predict stock dynamics and evaluate the impacts of fishing and environmental changes. While a4a simplifies traditional assessment approaches, it faces challenges such as ensuring the quality and consistency of input data, especially in regions with limited monitoring infrastructure. To address this, the initiative incorporates uncertainty into its models, leveraging MCMC frameworks and other statistical tools to account for variability in data quality and ecosystem processes. The a4a framework has been applied in various European fisheries to improve stock assessment practices, only in the Mediterranean Sea has been used for more than 200 stock assessments, as of 2024 in (GFCM)] and (STECF). Moreover, its use in small pelagic fisheries demonstrated the utility of simple linear models in capturing key population dynamics without the need for data-intensive methods (Jardim et al. 2014). Some of the key elements of stock assessment are the quantity, quality and aggregation level of the data available. As in many other models the data will condition the type of models that can be used. In a4a the minimum set of data, loosely defined as a “moderate data” level, consists of: volume of catches in weight (which should include landings and discards); length structure of the catches (based on selectivity studies or direct observations); natural mortality by length; proportion of mature individuals by length; age-length key or growth model; length-weight relationship; index of abundance and its length structure, or index of biomass (the type of index is left open, it could be from a scientifc survey or a commercial CPUE series); 2.2 Multi-stage modelling approach In ecological and population dynamics modeling, one can choose between integrated models, which estimate correlated parameters together, and two-stage models, which separate estimation into distinct steps. These approaches differ in complexity, data requirements, interpretability, and their ability to address uncertainties. The selection depends largely on the study objectives, available data, and the system’s ecological complexity. Integrated models estimate all parameters within a unified framework, accounting for correlations and interactions between variables such as growth, natural mortality, recruitment, and environmental factors. This approach can provide a realistic depiction of biological systems by preserving dependencies and feedback loops, which are crucial for understanding processes like density dependence or predator-prey interactions (Hamel et al. 2023b). Integrated models are particularly advantageous for ecosystem-based management, where interactions among multiple factors need to be captured. However, the complexity of these models makes them computationally intensive and sensitive to data quality. On the other hand, two-stage models estimate parameters such as growth or natural mortality independently before incorporating them into broader models. This step-wise approach simplifies estimation, reducing computational demands and mitigating issues like parameter confounding. For example, fisheries often use empirical relationships to estimate natural mortality (M) based on growth parameters or life history traits before including M in stock assessment models (Maceina and Sammons 2016). However, this decoupling may overlook dynamic interactions, such as how growth influences mortality, potentially leading to biased or incomplete inferences about ecosystem dynamics (Jacobsen and Essington 2018). Dealing with uncertainty is a critical aspect of both approaches. Integrated models explicitly quantify and propagate uncertainties across correlated parameters. These models incorporate multiple sources of variability, including observation, process, and structural uncertainties, enhancing the robustness of predictions (López Quintero, Contreras-Reyes, and Wiff 2017). Conversely, two-stage models often treat parameter estimates as fixed values, which can underestimate uncertainty propagation in subsequent analyses. However, by treating first-stage estimates as distributions rather than point estimates, two-stage models can partially address this limitation. For fisheries science, the choice between these models often depends on management goals and data availability. Integrated models are better suited for forecasting fish abundance or evaluating complex ecological interactions, such as predator-prey dynamics or responses to environmental variability (Robertson, Regular, and Cadigan 2022). Meanwhile, two-stage models are practical for stock assessments, where simplicity and interpretability are prioritized. Two-stage models are advantageous for practical applications, such as fisheries stock assessments, where simplicity and robustness take precedence over ecological nuance. Empirical estimates of \\(M\\), derived from life-history traits, provide reliable inputs for subsequent models, avoiding the parameter confounding that often occurs in integrated frameworks. Despite the intuitive advantages of integrated models, it is not a panacea for poor quality data or model structure uncertainty in stock assessments. There are several disadvantages, mostly related to model misspecification, the complexity of the resulting models, and the associated, often considerable, computational requirements (e.g. the use of remotely sensed environmental information). Consequently, in some situations, the traditional two-stage approach remains a better approach (Mounder and Punt, 2013). 2.3 Stock Assessment Process The following table breaks down the stock assessment process into three stages: (i) input data preparation, (ii) stock assessment model fitting, and (iii) provision of scientific advice. This breakdown is designed to explain the a4a approach, offering a general framework that outlines the sequence of analyses in the stock assessment process. Each stage includes a plethora of analyses and procedures tailored to the specific stock, considering the available data, time, and resources. Table 2.1: Stock assessment process stages Stage Description Input data preparation Preparation of catch data, potentially separating landings and discards. Preparation of biological data, including maturity, length-weight relationships, natural mortality, and individual growth. Conversion of length data into age data. Stock assessment model fit Fitting the model to data, inspecting diagnostics such as residuals, retrospective analyses, and hindcasts. Fitting the stock-recruitment model to recruitment and spawning stock biomass (SSB) estimates from the stock assessment model fit or within the model itself. Scientific advice provision Estimation of reference points. Assessment of stock status based on reference points and model estimates of SSB and fishing mortality. Running projections under different scenarios. Providing reports with policy outcome evaluations. The main purpose of the above table is to clarify a4a’s multi-stage approach to stock assessment. For instance, parameters like natural mortality and individual growth are estimated outside the stock assessment model fitting process, unlike integrated analyses. This is done for reasons discussed earlier. Nevertheless, the stock-recruitment relationship can be estimated within the model, as is typical in integrated analyses. This mixed approach seeks to exclude highly correlated processes from the model while incorporating those that can enhance the robustness of the stock assessment model fit. When data are limited, the stock assessment model requires more structure, but this constraint reduces the information available to manage parameter correlations. Striking this balance is challenging. The a4a approach incorporates stock-recruitment relationships, as these are believed to exhibit lower correlation with other model parameters compared to natural mortality or individual growth. On the other hand, since natural mortality and individual growth are very important processes acting at a very low level in terms of data processing, there are specific methods to deal with conditioning those processes, in case there’s no data or limited data available, and to provide the opportuinity to propagate their uncertainty into stock assessment. 2.4 Stock assessment as a linear model The submodels formulation uses linear models, which opens the possibility of using the linear modelling tools available in R. For example, mgcv (Wood 2017) gam formulas or factorial design formulas using lm(). The ‘language’ of linear models has been developing within the statistical community for many years, and constitutes an elegant way of defining models without going through the complexity of mathematical representations. This approach makes it also easier to communicate among scientists: (Nelder 1965), notation for randomized block design (Wilkinson and Rogers 1973), symbolic description for factorial designs (Hastie and Tibshirani 1990), introduced notation for smoothers (Chambers and Hastie 1991), further developed for use in S 2.5 Data used in the book 2.5.1 Plaice in area FAO 27, ICES area IV [ToDo] 2.5.2 European hake in FAO 37, GSAs 1,5,6,7 European hake is an important demersal species targeted by Mediterranean fishing fleets in several regions, mainly by bottom trawling, with In GSA 5 (Balearic Islands), bottom trawlers use different fishing tactics depending on the depth, with hake being targeted mainly on the deep shelf and the upper slope. In GSA 6, the fishery is also dominated by trawlers, with a large fleet operating on the shelf and slope and showing relatively stable catches. In GSA 7 (Gulf of Lions), hake is targeted by both French and Spanish vessels using a variety of gear types, including trawlers, gillnets and longlines. 2.5.3 Red mullet in FAO 27, GSA 1 Red mullets is a key commercial species in GSA 1.They can be found in sandy and muddy areas, and most are caught by bottom trawlers. Small scale fisheries also catch some using nets. The amount of discards reported is very low and considered to be negligible. Trawl fisheries developed along the continental shelf and upper slope. Smaller vessels operate almost exclusively on the continental shelf. Red mullet is intensively exploited during its recruitment from September to November 2.5.4 Redfish simulated length data This is a length based dataset simulated with GADGET (REF). References "],["introduction-to-splines.html", "3 Introduction to Splines 3.1 Generalize to polynomials 3.2 Thin plate spline 3.3 The mgcv package inside a4a 3.4 On the number of knots \\(k\\)", " 3 Introduction to Splines Splines are a specific case of smoothers. A smoother is a method or algorithm designed to estimate a smooth function that fits data, capturing underlying trends without overfitting noise. Splines are a powerful tool for modeling complex, non-linear relationships between variables in a flexible and interpretable way. A common way to use splines is to break a function into smooth, continuous polynomial segments, each called a piece or basis function, joined at specific points called knots. This piecewise approach allows us to capture the non-linearity in the data without overfitting. Basis functions consist the main building block of splines.The key concept of “basis functions” is that they transform the input variable (or vector) \\(\\mathbf{x}\\) into a set of new variables, which are then used as inputs in the model. This allows the model to remain linear in these transformed variables, even though it can capture complex, non-linear relationships in the original variable. Splines build their functionality through the core concepts of linear models. Linear models assume that the relationship between data can be described by a straight line, in the case of only one predictor \\(\\mathbf{x}\\). We denote linear model as: \\[\\mathbf{y} = \\beta_0 + \\beta_1\\mathbf{x} + \\mathbf{\\epsilon}\\] Where \\(\\mathbf{y}\\) are the observations, the parameters \\(\\beta_0\\) and \\(\\beta_1\\) uniquely determine a straight line and \\(\\mathbf{\\epsilon}\\) is the observation error. Simple in its construction and representation, linear models can be limited when trying to capture the complexity of a real-world data set. The simplest form of a linear model is the mean or average of a data set: \\[\\mathbf{y} = \\beta_0 + \\epsilon\\] In the following graph of non linear data (\\(\\mathbf{y}\\)), we fit a simple mean value of \\(\\beta_0\\) and a straight line model \\(\\beta_0 + \\beta_1\\mathbf{x}\\): Figure 3.1: Generated data with two linear fits, left a simple mean and right a straight line In this case our parameters are a \\(\\beta_0 =\\) 3.09 for the mean (left) model, while for the straight line model, \\(\\beta_0 =\\) 0.85 and \\(\\beta_1 =\\) 0.46. In Figure 3.1 we see how by adding to the constant model \\(\\beta_0\\) a multiple (\\(\\beta_1\\)) of the variable \\(\\mathbf{x}\\), the model becomes a little more complex but it can now follow the general upward trend compared to the first line, although it fails to follow the peaks and the lows of the data set. One of the mechanisms that splines use is to split the set of values of the predictor, in our case \\(\\mathbf{x}\\), in smaller compartments and fit in those compartments a specified model. The points where the splitting occurs are called knots. In Figure 3.2 we take a stepwise approach following the logic behind the use of piecewise polynomials in smoothing splines. First, we split our range of \\(\\mathbf{x}\\) values in 4 subsets, by defining our knots, then we take the average of \\(\\mathbf{y}\\) data for each of these compartments and in the final step we add a bit of complexity by fitting a straight line model in each of these subsets of our original dataset. Figure X2 demonstrates a naive approach to try and follow better the trends of the data, where in each step we manage to capture a bit more. Figure 3.2: Effect of breaking the data in sections, by knots, and fitting the mean or a linear model to each section. 3.1 Generalize to polynomials We explored the concept of splitting the \\(\\mathbf{x}\\) variable space into compartments and developing a solution by locally fitting linear models that better capture the global trajectory of the data. Smooth functions rely on two fundamental mechanisms. The first, as mentioned earlier, involves how the domain of the function is divided for estimation. The second, which we will focus on here, involves using slightly more complex functions than linear ones, such as polynomials. By combining polynomial pieces and ensuring smoothness at their junctions, i.e. knots, we can create flexible models that adapt to the data. In regression, splines are a powerful tool for fitting complex shapes by introducing non-linear trends while maintaining control over the smoothness of the overall function. This approach allows us to balance flexibility and precision in modeling. The polynomials are build by transforming the predictor variable or the sets of variables into higher order polynomials (usually 2nd or 3rd grade polynomials). These polynomials need to have matching values at the knots. Let \\(S\\) our spline function, that is defined in an interval \\([a,b]\\). We seek to construct \\(S\\) by combining \\(k-1\\) polynomials \\(P\\), where \\(k\\) is the number of knots. Let also \\(t_{i}, i = 0, ..., k-1\\) the positions of the knots in the interval \\([a,b]\\). \\(S\\) is going to be defined as: \\[ S(x) = \\begin{cases} P_1(x) &amp; \\text{if } a \\leq x &lt; t_1, \\\\ P_2(x) &amp; \\text{if } t_1 \\leq x &lt; t_2, \\\\ \\quad\\vdots \\\\ P_{k-1}(x) &amp; \\text{if } t_{k-1} \\leq x \\leq b, \\end{cases} \\] The above definition is a simplified version of how splines work and can help as an intuitive approach. In reality splines need to satisfy some extra conditions like continuity, i.e. \\(P_{i-1}(t_{i}) = P_{i}(t_{i})\\) on the points of junction, and of the first and second derivative. Depending on the basis functions the conditions may differ. In Figure 3.3 we demonstrate the fitting of cubic regression splines. In this case the highest order of the polynomials \\(P_i(x)\\) are 3rd degree polynomials and it comprises 4 cubic segments, i.e basis functions. The spline would be then as follows: \\[\\mathbf{y} = \\beta_0 + \\beta_1P_1(\\mathbf{x}) + \\beta_2P_2(\\mathbf{x}) + \\beta_3P_3(\\mathbf{x}) + \\beta_4P_4(\\mathbf{x})+\\beta_5P_5(\\mathbf{x}) \\] Figure 3.3: A B-spline with 5 knots consists of 5 cubic polynomials The B-spline fit above is constrained at the boundaries, by putting two of the five knots there, resulting in a linear behavior at the ends of the data range. This approach is helpful for data that has an approximately linear trend at the boundaries but exhibits non-linearity in the center. The knots in this regression spline are placed by quantiles through the variable space, so in the case where the data are evenly spread across the variable space the knots will be placed evenly. For the spline fitted above, there are \\(k\\) polynomials plus an intercept (not shown) based on the knots (dashed lines). See Figure 3.4 for a depiction of the basis functions. Figure 3.4: basis functions for a B-spline with 5 knots [A bit confused here … should it be 5 polinomials? are you sure what you’re calling intercet is correct?] The polynomials transform the initial variable \\(\\mathbf{x}\\) and create a model matrix \\(\\mathbf{X}\\) with \\(k\\) columns and \\(n\\) rows, where \\(n\\) is the number of data points. This new transformation is being then used to fit the model and estimate the \\(\\beta_{0}, ... ,\\beta_{k}\\) coefficients, \\(\\beta_0\\) is required for the intercept. The fitted model results from \\(\\mathbf{X} \\mathbf{\\beta}\\) (Figure 3.5). Figure 3.5: basis functions for a B-spline with 5 knots 3.2 Thin plate spline Thin plate splines are particularly useful for smoothing in multiple dimensions. However, they also work well with univariate data, as they offer flexibility and control over smoothness they are the default choice of the mgcv package. Thin plate splines work differently from the splines we have shown so far. They are not composed of a sequence of local polynomials but from basis functions that are smooth across the entire range of the data, and capture increasing amounts of flexibility (Figure 3.6). # Fit a thin plate spline with gam() tps_model &lt;- gam(y ~ s(x, k = 5, bs = &quot;tp&quot;), data = data) # Predict data$tps_fit &lt;- predict(tps_model) Figure 3.6: Thin plate splines fit with k = 5 Thin plate splines are ideal when you need a smooth fit without predefined knots. The notion of knots in thin plate splines does not have the same interpretation as for B-splines and other piecewise functions, and infact it is likely not useful to think of knots when using thin plate spines. Instead it is better to think of the number of basis functions used to represent the smooth term (Figure 3.7). Figure 3.7: basis functions for thin plate splines As in the example before, the fitted model results from \\(X\\beta\\) (Figure 3.8). Figure 3.8: Basis functions for thin plate splines and the fitted model Figure 3.9 shows both models fitted to the dataset, both fits use the same number of knots. [Can we say a bit more here?] Figure 3.9: Thin plate splines and cubic regression splines fit 3.3 The mgcv package inside a4a The mgcv package provides various user input options to define the smoother functions used to construct the submodels. Under the a4a framework, the mgcv package is used to construct the model matrices of the submodels, which are then passed to ADMB where the model fitting takes place. The default option for the basis functions of the splines is bs = tp (thin plate splines) and is considered the optimal option. The user can define other smoothing basis functions using the bs argument. The user can refer to the smooth.terms of the mgcv package for a full description. There are many equivalent basis functions for the splines, and some of them have little or no effect in the context of a4a, since they differ only in the penalty term, which is not used in a4a. Examples for different smoothing terms: fmod00 &lt;- ~s(age)+s(year, bs = &#39;tp&#39;, k = 10) # thin plate splines fmod01 &lt;- ~s(age)+s(year, bs = &#39;cr&#39;, k = 10) # regression cubic splines fmod02 &lt;- ~s(age)+s(year, bs = &#39;bs&#39;, k = 10) # b-splines fmod03 &lt;- ~s(age)+s(year, bs = &#39;ps&#39;, k = 10) # p-splines fmod04 &lt;- ~s(age)+s(year, bs = &#39;ad&#39;, k = 10) # Adaptive smoothers fit00 &lt;- sca(stk, idx, fmodel = fmod00) fit01 &lt;- sca(stk, idx, fmodel = fmod01) fit02 &lt;- sca(stk, idx, fmodel = fmod02) fit03 &lt;- sca(stk, idx, fmodel = fmod03) fit04 &lt;- sca(stk, idx, fmodel = fmod04) In this example we are using the thin plate regression splines, cubic splines, b-splines, p-splines and adaptive smoothers. Figure 3.10 shows the different fits together, where it’s clear the differences are very small. Figure 3.10: Multiple fits of thin plate splines, cubic splines, b-splines, p-splines and adaptive smoothers Functionality of mgcv package provides also the option to work with interactions. Although s(age, year) can be defined, it uses a common bivariate spline for the two variables which are very different in scale. It is preferable if interactions are assumed to use te(age, year). Again is up to the user to define the basis functions for the smoothers. Figure 3.11 shows the differences when using different basis for the tensor product. [The paragraph above needs a bit more explanation about what a tensor is] fmod03 &lt;- ~te(age, year, k = c(3,10)) fmod04 &lt;- ~te(age, year, k = c(3,10), bs = &#39;cr&#39;) fmod05 &lt;- ~te(age, year, bs = &#39;bs&#39;) fit03 &lt;- sca(stk, idx, fmodel = fmod03) fit04 &lt;- sca(stk, idx, fmodel = fmod04) fit05 &lt;- sca(stk, idx, fmodel = fmod05) Figure 3.11: Multiple basis for a tensor and their effects in the fit 3.4 On the number of knots \\(k\\) \\(k\\) is the dimension of the basis used to represent the smooth term \\(s\\). The default depends on the number of variables that the smooth is a function of. In practice k-1 (or k) sets the upper limit on the degrees of freedom associated with an s smooth (1 degree of freedom is usually lost to the intercept of the smooth). For the smooths the upper limit of the degrees of freedom is given by the product of the k values provided for each marginal smooth less one, for the constraint. The choice of the k is not critical and in general it must be large enough to allow to have enough degrees of freedom capturing the underlying process and small enough to prevent overfitting (Figure 3.12). A strong pattern in the residuals can be a sign of low \\(k\\). fmod00 &lt;- ~s(age)+s(year, k = 5) fmod01 &lt;- ~s(age)+s(year, k = 10) # cubic splines [Why is this cubic] fmod02 &lt;- ~s(age)+s(year, k = 20) # b-splines [and this b?] fit00 &lt;- sca(stk, idx, fmodel = fmod00) fit01 &lt;- sca(stk, idx, fmodel = fmod01) fit02 &lt;- sca(stk, idx, fmodel = fmod02) Figure 3.12: Multiple ks and their effects in the fit We can check the result of choosing different \\(k\\) values on BIC (Bayesian Information Criteria) and GCV (Generalized Cross Validation score) by running the stock assessment model with different k and looking at the values of those stastics (Figure 3.13). fmodsk &lt;- list() for(i in 10:20) { fmodsk[[paste0(i)]] &lt;- as.formula(paste0(&quot;~s(age)+s(year, k=&quot;,i,&quot;)&quot;)) } myFits &lt;- scas(FLStocks(stk), list(idx), fmodel = fmodsk) Figure 3.13: BIC (Bayesian Information Criteria) and GCV (Generalized Cross Validation score) profiles based on changing the value of k. "],["growth.html", "4 Modelling Individual Growth and Using Stochastic Slicing to Convert Length-based Data Into Age-based Data 4.1 a4aGr - The growth class 4.2 Adding uncertainty to growth parameters with a multivariate normal distribution 4.3 Adding uncertainty to growth parameters with a multivariate triangle distribution 4.4 Adding uncertainty to growth parameters with statistical copulas 4.5 Converting from length to age based data - the l2a() method", " 4 Modelling Individual Growth and Using Stochastic Slicing to Convert Length-based Data Into Age-based Data The a4a stock assessment framework is based on age dynamics. Therefore, length information must be processed before running the stock assessment model. The methods in this section provide the analyst flexibility to use a large range of information sources, e.g. literature or online databases, to collect information about the species growth model and uncertainty about the model parameters. The framework allows the analyst to parametrize individual growth, set the assumptions about it and condition the stock assessment model on those decisions. It incentivizes the uptake of estimation uncertainty, as well as exploring several parameterizations and/or growth models to deal with structural uncertainty, finally propagating uncertainty into stock assessment. Within the a4a framework this is handled using the a4aGr class and its methods. This class stores information about the growth model and it’s parameters, including parameters’ uncertainty and the distributions governing it. The class’s main method is l2a that converts length to ages based on a length based stock object and using the model defined in the a4aGr instance. 4.1 a4aGr - The growth class The conversion of length data to age is performed through the use of a growth model. The implementation is done through the a4aGr class . showClass(&quot;a4aGr&quot;) ## Class &quot;a4aGr&quot; [package &quot;FLa4a&quot;] ## ## Slots: ## ## Name: grMod grInvMod params vcov distr name desc ## Class: formula formula FLPar array character character character ## ## Name: range ## Class: numeric ## ## Extends: &quot;FLComp&quot; To construct an a4aGr object, the growth model and parameters must be provided. Here we show an example using the von Bertalanffy growth model. To create the a4aGr object it’s necessary to pass the model equation (\\(length \\sim time\\)), the inverse model equation (\\(time \\sim length\\)) and the parameters. Any growth model can be used as long as it’s possible to write the model (and the inverse) as an R formula. vbObj &lt;- a4aGr( grMod=~linf*(1-exp(-k*(t-t0))), grInvMod=~t0-1/k*log(1-len/linf), params=FLPar(linf=58.5, k=0.086, t0=0.001, units=c(&quot;cm&quot;,&quot;year-1&quot;,&quot;year&quot;)) ) Check the model and its inverse: lc=20 predict(vbObj, len=lc) ## iter ## 1 ## 1 4.86575 predict(vbObj, t=predict(vbObj, len=lc)) ## iter ## 1 ## 1 20 The predict method allows the transformation between lengths and ages, and vice-versa, using the growth model. predict(vbObj, len=5:10+0.5) ## iter ## 1 ## 1 1.149080 ## 2 1.370570 ## 3 1.596362 ## 4 1.826625 ## 5 2.061540 ## 6 2.301299 predict(vbObj, t=5:10+0.5) ## iter ## 1 ## 1 22.04376 ## 2 25.04796 ## 3 27.80460 ## 4 30.33408 ## 5 32.65511 ## 6 34.78488 4.2 Adding uncertainty to growth parameters with a multivariate normal distribution Uncertainty in the growth model is introduced through the inclusion of parameter uncertainty. This is done by making use of the parameter variance-covariance matrix (the vcov slot of the a4aGr class) and setting a distribution for the parameters. The variance-covariance matrix could come from the parameter uncertainty from fitting the growth model parameters, or a meta analysis of correlation between parameters. Here we set the variance-covariance matrix by scaling a correlation matrix, using a cv of 0.2. Based on \\[\\rho_{x,y}=\\frac{\\Sigma_{x,y}}{\\sigma_x \\sigma_y}\\] and \\[CV_x=\\frac{\\sigma_x}{\\mu_x}\\] # Make an empty cor matrix cm &lt;- diag(c(1,1,1)) # k and linf are negatively correlated while t0 is independent cm[1,2] &lt;- cm[2,1] &lt;- -0.5 # scale cor to var using CV=0.2 cv &lt;- 0.2 p &lt;- c(linf=60, k=0.09, t0=-0.01) vc &lt;- matrix(1, ncol=3, nrow=3) l &lt;- vc l[1,] &lt;- l[,1] &lt;- p[1]*cv k &lt;- vc k[,2] &lt;- k[2,] &lt;- p[2]*cv t &lt;- vc t[3,] &lt;- t[,3] &lt;- p[3]*cv mm &lt;- t*k*l diag(mm) &lt;- diag(mm)^2 mm &lt;- mm*cm # check that we have the intended correlation all.equal(cm, cov2cor(mm)) ## [1] TRUE Create the a4aGr object as before but now we also include the vcov argument for the variance-covariance matrix. vbObj &lt;- a4aGr( grMod=~linf*(1-exp(-k*(t-t0))), grInvMod=~t0-1/k*log(1-len/linf), params=FLPar( linf=p[&quot;linf&quot;], k=p[&quot;k&quot;], t0=p[&quot;t0&quot;], units=c(&quot;cm&quot;,&quot;year-1&quot;,&quot;year&quot;)), vcov=mm ) First we show a simple example where we assume that the parameters are represented using a multivariate normal distribution. Note that the object we have just created has a single iteration of each parameter. vbObj@params ## An object of class &quot;FLPar&quot; ## params ## linf k t0 ## 60.00 0.09 -0.01 ## units: cm year-1 year dim(vbObj@params) ## [1] 3 1 We simulate 250 iterations from the a4aGr object by calling mvrnorm() using the variance-covariance matrix we created earlier. The object will now have 250 iterations of each parameter, randomly sampled from the multivariate normal distribution. vbNorm &lt;- mvrnorm(250,vbObj) vbNorm@params ## An object of class &quot;FLPar&quot; ## iters: 250 ## ## params ## linf k t0 ## 60.957951(11.45487) 0.088662( 0.01680) -0.010190( 0.00185) ## units: cm year-1 year dim(vbNorm@params) ## [1] 3 250 We can now convert from length to ages data based on the 250 parameter iterations, which will produce 250 sets of age data. For example, the next code will convert a single length vector using each of the 250 parameter iterations. lvec &lt;- 5:10+0.5 ages &lt;- predict(vbNorm, len=lvec) dim(ages) ## [1] 6 250 The marginal distributions of the parameters can be seen in Figure 4.1. Figure 4.1: The marginal distributions of each of the parameters from using a multivariate normal distribution. Pairwise plots show the covariance between each pair of parameters and the shape of their correlation (Figure 4.2). Figure 4.2: Scatter plot of the 10000 samples parameter from the multivariate normal distribution. Using the new generated age vectors one can depict the growth curves for the 250 iterations, which displays individual growth uncertainty (Figure 4.3). Figure 4.3: Growth curves using parameters simulated from a multivariate normal distribution. 4.3 Adding uncertainty to growth parameters with a multivariate triangle distribution One alternative to using a normal distribution is to use a triangle distribution. We use the package triangle (Carnell 2022) where this distribution is parametrized using the minimum, maximum and median values. This can be very attractive if the analyst needs to scrape information from the web or literature, and use a meta-analysis to build the parameters’ distribution. The triangle distribution has the advantage of setting hard tail limits, avoiding to generate extreme values. Here we show an example of setting a triangle distribution with values taken from Fishbase (Froese and Pauly 2000). The following shows a method to extract data from fishbase. However, due to potential changes in the way one gets data from fishbase from whithin R, we’ve downloaded the data beforehand and load it for this example. # The web address for the growth parameters for redfish (Sebastes norvegicus) addr &lt;- &quot;https://fishbase.se/PopDyn/PopGrowthList.php?ID=501&quot; # Scrape the data tab &lt;- try(readHTMLTable(addr)) # Interrogate the data table and get vectors of the values linf &lt;- as.numeric(as.character(tab$dataTable[,2])) k &lt;- as.numeric(as.character(tab$dataTable[,4])) t0 &lt;- as.numeric(as.character(tab$dataTable[,5])) # Set the min (a), max (b) and median (c) values for the parameter as a list of lists # Note that t0 has no &#39;c&#39; (median) value. This makes the distribution symmetrical triPars &lt;- list( linf=list(a=min(linf), b=max(linf), c=median(linf)), k=list(a=min(k), b=max(k), c=median(k)), t0=list(a=median(t0, na.rm=T)-IQR(t0, na.rm=T)/2, b=median(t0, na.rm=T)+IQR(t0, na.rm=T)/2)) # Draw 250 samples using mvrtriangle vbTri &lt;- mvrtriangle(250, vbObj, paramMargins=triPars) Note that in this case we’re not building a new object with all the parameters’ information. We’re using the argument paramMargins to pass the parameters’ information to the method. The marginals will reflect the uncertainty on the parameter values that were scraped from Froese and Pauly (2000) but, as we don’t really believe the parameters are multivariate normal, here we adopted a distribution based on a t copula with triangle marginals. The marginal distributions can be seen in Figure 4.4 and the shape of the correlation can be seen in Figure 4.5. Figure 4.4: The marginal distributions of each of the parameters from using a multivariate triangle distribution. Figure 4.5: Scatter plot of the 10000 samples parameter from the multivariate triangle distribution. We can still use predict() to see the growth model uncertainty (Figure 4.6). Comparing with Figure 4.3 one can see that using triangle distribution generates a lot less outliers, or values outside the central range of the growth curve. Figure 4.6: Growth curves using parameters simulated from a multivariate triangle distribution. Remember that the above examples use a variance-covariance matrix that we essentially made up. An alternative would be to scrape the entire growth parameters dataset from Fishbase and compute the shape of the variance-covariance matrix yourself. 4.4 Adding uncertainty to growth parameters with statistical copulas A more general approach to adding parameter uncertainty is to make use of statistical copulas (Sklar 1959). Genest, Okhrin, and Bodnar (2024) describes statistical “copula” as a multivariate cumulative distribution function with uniform margins on the unit interval. Sklar (1959) highlighted the fact that any multivariate distribution can be expressed as a function of its margins and a copula. The idea is very actractive, one can simulate any multivariate distribution by setting a multivariate function in the unit interval which describes how the margins relate to each other, and scale up the univariate uniform margin with any continuos univariate distribution. In our case this is possible with the mvrcop() function, borrowed from the package copula (R-copula?). The example below keeps the same parameters and changes only the copula type and family but a lot more can be done. Check the package copula for more information. vbCop &lt;- mvrcop(250, vbObj, copula=&quot;archmCopula&quot;, family=&quot;clayton&quot;, param=2, margins=&quot;triangle&quot;, paramMargins=triPars) The shape of the correlation as well as the resulting growth curves are shown in Figures 4.7 and 4.8. Figure 4.7: Scatter plot of the 250 samples parameter from the using an archmCopula copula with triangle margins. Figure 4.8: Growth curves using parameters simulated from an archmCopula copula with triangle margins. 4.5 Converting from length to age based data - the l2a() method After introducing uncertainty in the growth model through the parameters it’s time to transform the length-based dataset into an age-based dataset. The method that deals with this process is l2a(). The implementation of this method for the FLQuant class is the main workhorse. There are two other implementations, for the FLStock and FLIndex classes, which are mainly wrappers that call the FLQuant method several times. When converting from length-based data to age-based data you need to be aware of how the aggregation of length classes is performed. For example, individuals in length classes 1-2, 2-3, and 3-4 cm may all be considered as being of age 1 (obviously depending on the growth model). How should the values in those length classes be combined? If the values are abundances then the values should be summed. Summing other types of values, such as mean weight, does not make sense. Instead these values are averaged over the length classes (possibly weighted by the abundance). This is controlled using the stat argument which can be either mean or sum (the default). Fishing mortality is not computed to avoid making wrong assumptions about the meaning of F at length. We demonstrate the method by converting a catch-at-length FLQuant to a catch-at-age FLQuant. First we make an a4aGr object with a multivariate triangle distribution using parameters extracted from an AI agent. We use 10 iterations as an example, and call l2a() by passing in the length-based FLQuant and the a4aGr object. triPars &lt;- list( linf=list(a=55, b=60), k=list(a=0.05, b=0.06), t0=list(a=-3, b=-2)) # Draw 10 samples using mvrtriangle vbTriSmall &lt;- mvrtriangle(10, vbObj, paramMargins=triPars) # slice catch numbers at lengths to ages by summing catches cth.n &lt;- l2a(catch.n(rfLen.stk), vbTriSmall) # note there&#39;s a lot of 0 catches so we&#39;ll set the plus group at 21 cth.n &lt;- setPlusGroup(cth.n, 21) # there&#39;s also negative ages. The simulated data included individuals in lengths that won&#39;t show in the catches, like 1 cm. We&#39;ll trim those ages cth.n &lt;- cth.n[ac(0:21)] # slice catch weights at lengths to ages by averaging catches cth.wt &lt;- l2a(catch.wt(rfLen.stk), vbTriSmall, stat=&quot;mean&quot;) # same process to deal with negative ages cth.wt &lt;- cth.wt[ac(0:21)] In the previous example, the FLQuant object that was sliced (catch.n(rfLen.stk)) had only one iteration. This iteration was sliced by each of the iterations in the growth model. It is possible for the FLQuant object to have the same number of iterations as the growth model, in which case each iteration of the FLQuant and the growth model are used together. It is also possible for the growth model to have only one iteration while the FLQuant object has many iterations. The same growth model is then used for each of the FLQuant iterations. As with all FLR objects, the general rule is one or n iterations. As well as converting one FLQuant at a time, we can convert entire FLStock and FLIndex objects. In these cases the individual FLQuant slots of those classes are converted from length-based to age-based. As mentioned above, the aggregation method depends on the type of values the slots contain. The abundance slots (*.n, such as stock.n) are summed. The *.wt, m, mat, harvest.spwn and m.spwn slots of an FLStock object are averaged. The catch.wt and sel.pattern slots of an FLIndex object are averaged, while the index, index.var and catch.n slots are summed. The method for FLStock classes takes an additional argument for the plusgroup. aStk &lt;- l2a(rfLen.stk, vbTriSmall, plusgroup=21) ## Warning in .local(object, model, ...): Individual weights, M and maturity will be (weighted) averaged accross lengths, ## harvest is not computed and everything else will be summed. ## If this is not what you want, you&#39;ll have to deal with these slots by hand. ## Warning in .local(object, model, ...): Some ages are less than 0, indicating a mismatch between input data lengths ## and growth parameters (possibly t0) ## Warning in .local(object, model, ...): Trimming age range to a minimum of 0 ## [1] &quot;maxfbar has been changed to accomodate new plusgroup&quot; aIdx &lt;- l2a(rfTrawl.idx, vbTriSmall) ## Warning in l2a(rfTrawl.idx, vbTriSmall): Some ages are less than 0, indicating a mismatch between input data lengths ## and growth parameters (possibly t0) ## Warning in l2a(rfTrawl.idx, vbTriSmall): Trimming age range to a minimum of 0 When converting with l2a() all lengths above Linf are converted to the maximum age, as there is no information in the growth model about how to deal with individuals larger than Linf. References "],["modelling-natural-mortality.html", "5 Modelling Natural Mortality 5.1 a4aM - The M class 5.2 Adding uncertainty to natural mortality parameters with a multivariate normal distribution 5.3 Adding uncertainty to natural mortality parameters with statistical copulas 5.4 Computing natural mortality time series - the “m” method", " 5 Modelling Natural Mortality Natural mortality (\\(M\\)) is a critical parameter in stock assessment models, representing all sources of mortality not related to fishing or harvest (\\(F\\)). Combined, these two sources constitute the total mortality (\\(Z\\)) that individuals experience, with \\(Z = F + M\\). However, natural mortality is notoriously difficult to observe and estimate. Only a few methods, such as mark-recapture studies, provide direct estimates of \\(M\\). However, these methods are not applicable to all species and are often costly. As an alternative, life-history theory is commonly used to derive values for \\(M\\) that are consistent with individual growth and reproduction. There is an extensive body of literature on natural mortality. Works by Pauly (1980), Gislason et al. (2010), Charnov (1993), Maunder et al. (2023) and Quinn and Deriso (1999), as well as a dedicated special issue in Fisheries Research (Hamel et al. 2023a), offer valuable insights and serve as excellent starting points. Given the parameter’s importance and the difficulty of obtaining direct observations, natural mortality is widely regarded as one of the most significant sources of uncertainty in stock assessments. Within the a4a framework, natural mortality is treated as an externally fixed parameter in the stock assessment model. Our aim is to develop a system that enables analysts to explore alternative models for \\(M\\) and compare the resulting assessment outcomes. This approach provides a more comprehensive information base to support informed decision-making throughout the stock assessment process. Within the a4a framework, the general method for adding natural mortality in the stock assessment model is to: Create an object of class a4aM which holds the natural mortality model and parameters. Add uncertainty to the parameters in the a4aM object. Apply the m() method to the a4aM object to create an age or length based FLQuant object of the required dimensions. The resulting FLQuant object can then be directly inserted into an FLStock object to be used for the assessment. In this section we go through each of the steps in detail using a variety of different models. 5.1 a4aM - The M class Natural mortality is implemented in a class named a4aM. This class is made up of three objects of the class FLModelSim. Each object is a model that represents one effect: an age or length effect, a scaling (level) effect and a time trend, named shape, level and trend, respectively. The impact of the models is multiplicative, i.e. the overal natural mortality is given by shape x level x trend. showClass(&quot;a4aM&quot;) ## Class &quot;a4aM&quot; [package &quot;FLa4a&quot;] ## ## Slots: ## ## Name: shape level trend name desc range ## Class: FLModelSim FLModelSim FLModelSim character character numeric ## ## Extends: &quot;FLComp&quot; The a4aM constructor requires that the models and parameters are provided. The default method will build each of these models as a constant value of 1. As a simple example, the usual “0.2” guessestimate could be set up by setting the level model to have a single parameter with a fixed value, while the other two models, shape and trend, have a default value of 1 (meaning that they have no effect). mod02 &lt;- FLModelSim(model=~a, params=FLPar(a=0.2)) m1 &lt;- a4aM(level=mod02) m1 ## a4aM object: ## shape: ~1 ## level: ~a ## trend: ~1 More interesting natural mortality shapes can be set up using biological knowledge. The following example uses an exponential decay over ages (implying that the resulting FLQuant} generated by the m() method will be age based). We also use Jensen’s second estimator (Kenchington 2014) as a scaling level model, which is based on the von Bertalanffy \\(K\\) parameter, \\(M=1.5K\\). shape2 &lt;- FLModelSim(model=~exp(-age-0.5)) level2 &lt;- FLModelSim(model=~1.5*k, params=FLPar(k=0.4)) m2 &lt;- a4aM(shape=shape2, level=level2) m2 ## a4aM object: ## shape: ~exp(-age - 0.5) ## level: ~1.5 * k ## trend: ~1 Note that the shape model has age as a parameter of the model but is not set using the params argument. The shape model does not have to be age-based. For example, here we set up a shape model using Gislason’s second estimator (Kenchington 2014): \\(M_l=K(\\frac{L_{\\inf}}{l})^{1.5}\\). We use the default level and trend models. shape_len &lt;- FLModelSim(model=~K*(linf/len)^1.5, params=FLPar(linf=60, K=0.4)) m_len &lt;- a4aM(shape=shape_len) Another option is to model how an external factor may impact natural mortality. This can be added through the trend model. Suppose natural mortality can be modelled with a dependency on the NAO index, due to some mechanism that results in having lower mortality when NAO is negative and higher when it’s positive. In this example, the impact is represented by the NAO value on the quarter before spawning, which occurs in the second quarter. We use this to make a complex natural mortality model with an age based shape model, a level model based on \\(K\\) and a trend model driven by NAO, where mortality increases by 50% if NAO is positive on the first quarter. # Get NAO nao &lt;- read.table(&quot;https://www.cpc.ncep.noaa.gov/products/precip/CWlink/pna/norm.nao.monthly.b5001.current.ascii.table&quot;, skip=1, fill=TRUE, na.strings=&quot;-99.90&quot;) dnms &lt;- list(quant=&quot;nao&quot;, year=1950:2024, unit=&quot;unique&quot;, season=1:12, area=&quot;unique&quot;) # Build an FLQuant from the NAO data nao.flq &lt;- FLQuant(unlist(nao[,-1]), dimnames=dnms, units=&quot;nao&quot;) # Build covar by calculating mean over the first 3 months nao &lt;- seasonMeans(trim(nao.flq, year=dimnames(stock.n(ple4))$year)) # Turn into Boolean nao &lt;- (nao&gt;0) # Constructor trend3 &lt;- FLModelSim(model=~1+b*nao, params=FLPar(b=0.5)) shape3 &lt;- FLModelSim(model=~exp(-age-0.5)) level3 &lt;- FLModelSim(model=~1.5*k, params=FLPar(k=0.4)) m3 &lt;- a4aM(shape=shape3, level=level3, trend=trend3) m3 ## a4aM object: ## shape: ~exp(-age - 0.5) ## level: ~1.5 * k ## trend: ~1 + b * nao 5.2 Adding uncertainty to natural mortality parameters with a multivariate normal distribution Uncertainty on natural mortality is added through uncertainty on the parameters. In this section we’ll’ show how to add multivariate normal uncertainty. We make use of the class FLModelSim method mvrnorm(), which is a wrapper for the method mvrnorm() distributed by the package MASS (mass?). We’ll create an a4aM object with an exponential shape, a level model based on \\(k\\) and temperature, Jensen’s third estimator (Kenchington 2014), and a trend model driven by the NAO (as above). Afterwards a variance-covariance matrix for the level and trend models will be included. Finally, create an object with 100 iterations using the mvrnorm() method. Create the object: shape4 &lt;- FLModelSim(model=~exp(-age-0.5)) level4 &lt;- FLModelSim(model=~k^0.66*t^0.57, params=FLPar(k=0.4, t=10), vcov=array(c(0.002, 0.01,0.01, 1), dim=c(2,2))) trend4 &lt;- FLModelSim(model=~1+b*nao, params=FLPar(b=0.5), vcov=matrix(0.02)) m4 &lt;- a4aM(shape=shape4, level=level4, trend=trend4) # Call mvrnorm() m4 &lt;- mvrnorm(100, m4) m4 ## a4aM object: ## shape: ~exp(-age - 0.5) ## level: ~k^0.66 * t^0.57 ## trend: ~1 + b * nao Inspect the level model (for example): level(m4) ## An object of class &quot;FLModelSim&quot; ## Slot &quot;model&quot;: ## ~k^0.66 * t^0.57 ## ## Slot &quot;params&quot;: ## An object of class &quot;FLPar&quot; ## iters: 100 ## ## params ## k t ## 0.40072(0.0427) 9.97178(0.8938) ## units: NA ## ## Slot &quot;vcov&quot;: ## [,1] [,2] ## [1,] 0.002 0.01 ## [2,] 0.010 1.00 ## ## Slot &quot;distr&quot;: ## [1] &quot;norm&quot; Note the variance in the parameters: params(trend(m4)) ## An object of class &quot;FLPar&quot; ## iters: 100 ## ## params ## b ## 0.50803(0.159) ## units: NA Note the shape model has no parameters and no uncertainty: params(shape(m4)) ## An object of class &quot;FLPar&quot; ## params ## ## NA ## units: NA In this particular case, the shape model will not be randomized because it doesn’t have a variance-covariance matrix. Also note that because there is only one parameter in the trend model, the randomization will use a univariate normal distribution. The same model could be achieved by using mnrnorm() on each model component: m4 &lt;- a4aM(shape=shape4, level=mvrnorm(100, level4), trend=mvrnorm(100, trend4)) an exact match would require to control the random seed so that the draws would be exactly the same. 5.3 Adding uncertainty to natural mortality parameters with statistical copulas We can also use copulas to add parameter uncertainty to the natural mortality model, similar to the way we use them for the growth model in Section 4. As stated above these processes make use of the methods implemented for the FLModelSim class. In the following example we’ll use again Gislason’s second estimator, \\(M_l=K(\\frac{L_{\\inf}}{l})^{1.5}\\) and a triangle copula to model parameter uncertainty. The method mvrtriangle() is used to create 1000 iterations. linf &lt;- 60 k &lt;- 0.4 # vcov matrix (make up some values) mm &lt;- matrix(NA, ncol=2, nrow=2) # 10% cv diag(mm) &lt;- c((linf*0.1)^2, (k*0.1)^2) # 0.2 correlation mm[upper.tri(mm)] &lt;- mm[lower.tri(mm)] &lt;- c(0.05) # a good way to check is using cov2cor cov2cor(mm) ## [,1] [,2] ## [1,] 1.0000000 0.2083333 ## [2,] 0.2083333 1.0000000 # create object mgis2 &lt;- FLModelSim(model=~k*(linf/len)^1.5, params=FLPar(linf=linf, k=k), vcov=mm) # set the lower, upper and centre of the parameters pars &lt;- list(list(a=55,b=65), list(a=0.3, b=0.6, c=0.35)) mgis2 &lt;- mvrtriangle(1000, mgis2, paramMargins=pars) mgis2 ## An object of class &quot;FLModelSim&quot; ## Slot &quot;model&quot;: ## ~k * (linf/len)^1.5 ## ## Slot &quot;params&quot;: ## An object of class &quot;FLPar&quot; ## iters: 1000 ## ## params ## linf k ## 60.04353(2.0645) 0.41196(0.0723) ## units: NA ## ## Slot &quot;vcov&quot;: ## [,1] [,2] ## [1,] 36.00 0.0500 ## [2,] 0.05 0.0016 ## ## Slot &quot;distr&quot;: ## [1] &quot;un &lt;deprecated slot&gt; triangle&quot; The resulting parameter estimates and marginal distributions can be seen in Figures 5.1 and 5.2. By default the method uses an elliptical copula of t family (see ?ellipCopula for more information). Figure 5.1: Pairwise depiction of Gislason’s second natural mortality model estimates using a ‘t’ family elliptic copula and triangle distribution margins. Figure 5.2: Marginal distributions of the parameters for Gislason’s second natural mortality model using triangle distributions. We now have a new model that can be used for the shape model. You can use the constructor or the set method to add the new model. Note that we have a quite complex method now for M. A length based shape model from Gislason’s work, Jensen’s third model based on temperature level and a time trend depending on NAO. All of the component models have uncertainty in their parameters. m5 &lt;- a4aM(shape=mgis2, level=level4, trend=trend4) # or m5 &lt;- m4 shape(m5) &lt;- mgis2 5.4 Computing natural mortality time series - the “m” method Now that we have set up the natural mortality a4aM model and added parameter uncertainty to each component, we are ready to generate the FLQuant of natural mortality with m(). The m() method is the workhorse method for computing natural mortality. The method returns a FLQuant that can be inserted in an FLStock to be used in the assessment method. The size of the FLQuant object is determined by the min, max, minyear and maxyear elements of the range slot of the a4aM object. By default the values of these elements are set to 0, which generates a FLQuant with length 1 in the quant and year dimension. The range slot can be set by hand, or by using the rngquant() and rngyear() methods. The name of the first dimension of the output FLQuant (e.g. ‘age’ or ‘len’) is determined by the parameters of the shape model. If it is not clear what the name should be then the name is set to ‘quant’. Here we demonstrate m() using the simple a4aM object we created above that has constant natural mortality. Start with the simplest model: m1 ## a4aM object: ## shape: ~1 ## level: ~a ## trend: ~1 Check the range: range(m1) ## min max plusgroup minyear maxyear minmbar maxmbar ## 0 0 0 0 0 0 0 The \\(M\\) FLQuant won’t have ages or years: m(m1) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## quant 0 ## 0 0.2 ## ## units: NA To have a more useful matrix of values that cover the ages and years in the FLStock object, the analyst needs to set the quant and year ranges. # set the quant range range(m1, c(&quot;min&quot;,&quot;max&quot;)) &lt;- c(0,7) # set the year range range(m1, c(&quot;minyear&quot;,&quot;maxyear&quot;)) &lt;- c(2000, 2010) range(m1) ## min max plusgroup minyear maxyear minmbar maxmbar ## 0 7 0 2000 2010 0 0 Create the object with the M estimates by age and year, note the name of the first dimension is ‘quant’. m(m1) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## quant 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 ## 0 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## 1 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## 2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## 3 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## 4 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## 5 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## 6 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## 7 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## ## units: NA The next example has an age-based shape (model “m2” from above). As the shape model has ‘age’ as a variable which is not included in the FLPar slot it is used as the name of the first dimension of the resulting FLQuant. An important feature of the m() method is the use of the level model. The outcome of the level model will be applied to a range of ages or lengths, set by the mbar information in the range slot. In this example the level model is \\(1.5*K\\) and since \\(K=0.4\\), the level predicted by the model will be \\(0.6\\). The m() model will use the information in the range, minmbar and maxmbar to compute the mean level. This mean level will match the value given by the level model. The mbar range can be changed with the rngmbar() method. We illustrate this by making an FLQuant with age varying natural mortality. Check the model and set the ranges: m2 ## a4aM object: ## shape: ~exp(-age - 0.5) ## level: ~1.5 * k ## trend: ~1 # set the quant range range(m2, c(&quot;min&quot;,&quot;max&quot;)) &lt;- c(0,7) # set the year range range(m2, c(&quot;minyear&quot;,&quot;maxyear&quot;)) &lt;- c(2000, 2003) range(m2) ## min max plusgroup minyear maxyear minmbar maxmbar ## 0 7 0 2000 2003 0 0 m(m2) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 2000 2001 2002 2003 ## 0 0.600000 0.600000 0.600000 0.600000 ## 1 0.220728 0.220728 0.220728 0.220728 ## 2 0.081201 0.081201 0.081201 0.081201 ## 3 0.029872 0.029872 0.029872 0.029872 ## 4 0.010989 0.010989 0.010989 0.010989 ## 5 0.004043 0.004043 0.004043 0.004043 ## 6 0.001487 0.001487 0.001487 0.001487 ## 7 0.000547 0.000547 0.000547 0.000547 ## ## units: NA Note that the level value is: predict(level(m2)) ## iter ## 1 ## 1 0.6 Which is the same as: m(m2)[&quot;0&quot;] ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 2000 2001 2002 2003 ## 0 0.6 0.6 0.6 0.6 ## ## units: NA This is because the mbar range is currently set to “0” and “0” (see above) and the mean natural mortality value over this range is given by the level model. We can change the mbar range: range(m2, c(&quot;minmbar&quot;,&quot;maxmbar&quot;)) &lt;- c(0,5) range(m2) ## min max plusgroup minyear maxyear minmbar maxmbar ## 0 7 0 2000 2003 0 5 Which rescales the the natural mortality at age: m(m2) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 2000 2001 2002 2003 ## 0 2.28129 2.28129 2.28129 2.28129 ## 1 0.83924 0.83924 0.83924 0.83924 ## 2 0.30874 0.30874 0.30874 0.30874 ## 3 0.11358 0.11358 0.11358 0.11358 ## 4 0.04178 0.04178 0.04178 0.04178 ## 5 0.01537 0.01537 0.01537 0.01537 ## 6 0.00565 0.00565 0.00565 0.00565 ## 7 0.00208 0.00208 0.00208 0.00208 ## ## units: NA Check that the mortality over the mean range is the same as the level model: quantMeans(m(m2)[ac(0:5)]) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 2000 2001 2002 2003 ## all 0.6 0.6 0.6 0.6 ## ## units: NA The next example uses a time trend for the trend model. We use the m3 model we made earlier. The trend model for this model has a covariate, ‘nao’. This needs to be passed to the m() method. The year range of the ‘nao’ covariate should match that of the range slot. Simple, pass in a single nao value (only one year): m(m3, nao=1) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 0 ## 0 0.9 ## ## units: NA Set ages: range(m3, c(&quot;min&quot;,&quot;max&quot;)) &lt;- c(0,7) m(m3, nao=0) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 0 ## 0 0.600000 ## 1 0.220728 ## 2 0.081201 ## 3 0.029872 ## 4 0.010989 ## 5 0.004043 ## 6 0.001487 ## 7 0.000547 ## ## units: NA With ages and years - passing in the NAO data as numeric (1,0,1,0) range(m3, c(&quot;minyear&quot;,&quot;maxyear&quot;)) &lt;- c(2000, 2003) m(m3, nao=as.numeric(nao[,as.character(2000:2003)])) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 2000 2001 2002 2003 ## 0 0.600000 0.900000 0.600000 0.900000 ## 1 0.220728 0.331091 0.220728 0.331091 ## 2 0.081201 0.121802 0.081201 0.121802 ## 3 0.029872 0.044808 0.029872 0.044808 ## 4 0.010989 0.016484 0.010989 0.016484 ## 5 0.004043 0.006064 0.004043 0.006064 ## 6 0.001487 0.002231 0.001487 0.002231 ## 7 0.000547 0.000821 0.000547 0.000821 ## ## units: NA The final example show how m() can be used to make an FLQuant with uncertainty (see Figure 5.3). We use the m4 object from earlier with uncertainty on the level and trend parameters. range(m4, c(&quot;min&quot;,&quot;max&quot;)) &lt;- c(0,7) range(m4, c(&quot;minyear&quot;,&quot;maxyear&quot;)) &lt;- c(2000, 2003) flq &lt;- m(m4, nao=c(nao[,ac(2000:2003)])) flq ## An object of class &quot;FLQuant&quot; ## iters: 100 ## ## , , unit = unique, season = all, area = unique ## ## year ## age 2000 2001 2002 2003 ## 0 2.01905(0.204543) 3.06740(0.433415) 2.01905(0.204543) 3.06740(0.433415) ## 1 0.74277(0.075247) 1.12843(0.159445) 0.74277(0.075247) 1.12843(0.159445) ## 2 0.27325(0.027682) 0.41513(0.058656) 0.27325(0.027682) 0.41513(0.058656) ## 3 0.10052(0.010184) 0.15272(0.021578) 0.10052(0.010184) 0.15272(0.021578) ## 4 0.03698(0.003746) 0.05618(0.007938) 0.03698(0.003746) 0.05618(0.007938) ## 5 0.01360(0.001378) 0.02067(0.002920) 0.01360(0.001378) 0.02067(0.002920) ## 6 0.00500(0.000507) 0.00760(0.001074) 0.00500(0.000507) 0.00760(0.001074) ## 7 0.00184(0.000187) 0.00280(0.000395) 0.00184(0.000187) 0.00280(0.000395) ## ## units: NA dim(flq) ## [1] 8 4 1 1 1 100 Figure 5.3: Natural mortality with age and year trend. Notably, the last example created a M model that varies with time, based on an environmental variable, and adds estimation uncertainty, showing the huge flexibility this method can deal with. References "],["stock-assessment-framework.html", "6 Stock assessment framework 6.1 Maths description 6.2 Classes Description ", " 6 Stock assessment framework 6.1 Maths description 6.1.1 Observations The stock assessment model is based on two types of observations: catches, \\(\\hat{C}\\), and abundance indices, \\(\\hat{I}\\). The model predicts catches at age \\(C_{ay}\\) and indices of abundance \\(I_{ays}\\) for each age \\(a\\), year \\(y\\) and survey \\(s\\) in the input dataset. It is assumed that the observed catches are normally distributed about the model predictions on the log scale with observation variance \\(\\sigma^2_{ay}\\). that is: \\[\\begin{equation} \\log \\hat{C}_{ay} \\sim \\text{Normal} \\Big( \\log C_{ay}, \\sigma^2_{ay}\\Big) \\end{equation}\\] likewise, the observed survey indices are assumed to be normally distributed about the model predictions on the log scale with observation variance \\(\\tau^2_{ays}\\): \\[\\begin{equation} \\log \\hat{I}_{ays} \\sim \\text{Normal} \\Big( \\log I_{ays}, \\tau^2_{ays} \\Big) \\end{equation}\\] This leads to the definition of the log-likelihood of the observed catches \\[\\begin{equation} \\ell_C = \\sum_{ay} w^{(c)}_{ay}\\ \\ell_N \\Big( \\log C_{ay}, \\sigma^2_{ay} ;\\ \\log \\hat{C}_{ay} \\Big) \\end{equation}\\] and the log-likelihood of the observed survey indices \\[\\begin{equation} \\ell_I = \\sum_s \\sum_{ay} w^{(s)}_{ays}\\ \\ell_N \\Big( \\log I_{ays}, \\tau_{ays}^2 ;\\ \\log \\hat{I}_{ays} \\Big) \\end{equation}\\] where \\(\\ell_N\\) is the normal log-likelihood function and \\(w^{(c)}_{ay}\\) and \\(w^{(s)}_{ays}\\) are optional weights for the catch and index observations, respectively. The total log-likelihood is then \\[\\begin{equation} \\ell = \\ell_C + \\ell_I \\end{equation}\\] 6.1.2 The population dynamics model To predict catches and survey indices, the model uses the standard population dynamics model \\[\\begin{equation} N_{a+1,y+1} = N_{ay} \\exp \\left( - F_{ay} - M_{ay} \\right) \\end{equation}\\] where \\(N_{ay}\\) is the number of individuals at age \\(a\\) in year \\(y\\), \\(F_{ay}\\) is the fishing mortality at age \\(a\\) in year \\(y\\), and \\(M_{ay}\\) is the natural mortality at age \\(a\\) in year \\(y\\). Any fish that survived beyond the oldest age \\(A\\) in the model are accumulated in the oldest age group and are assumed to be fished at a common rate \\(F_{A,y}\\). \\[\\begin{align} N_{A,y+1} = &amp;N_{A-1,y} \\exp \\left( - F_{A-1,y} - M_{A-1,y} \\right) \\\\ &amp;+ N_{A,y} \\exp \\left( - F_{A,y} - M_{A,y} \\right) \\end{align}\\] The numbers \\(N_{a,y}\\) are initiated in the first year, \\(y=1\\) and at the youngest age, \\(a=1\\), and the matrix of numbers at age are filled in according to the population dynamics model stated above (see Figure 6.1). Figure 6.1: Statistical catch at age population dynamics model Defining \\(R_y = N_{1,y}\\), the numbers at age can be written (ignoring the plus group) as: \\[\\begin{equation} N_{a,y} = \\left\\{ \\begin{matrix} R_{y-a+1} \\exp \\left( - \\sum^a_{i=1} F_{a-i,y-i} + M_{a-i,y-i} \\right) &amp; y \\geq a \\\\ N_{a-y+1,1} \\exp \\left( - \\sum^{a-y}_{i=1} F_{a-i,y-i} + M_{a-i,y-i} \\right) &amp; y \\lt a \\end{matrix} \\right. \\end{equation}\\] Catches in numbers by age and year are defined in terms of the three quantities: natural mortality, fishing mortality and recruitment; using a modified form of the well known Baranov catch equation: \\[\\begin{equation} C_{ay} = \\frac{F_{ay}}{F_{ay}+M_{ay}}\\left(1 - e^{-(F_{ay}+M_{ay})}\\right) R_{y}e^{-\\sum (F_{ay} + M_{ay})} \\end{equation}\\] Survey indices by age and year are defined in terms of the same three quantities with the addition of survey catchability: \\[\\begin{equation} I_{ays} = Q_{ays} R_{y}e^{-\\sum (F_{ay} + M_{ay})} \\end{equation}\\] Observed catches and observed survey indices are assumed to be log-normally distributed, or equivalently, normally distributed on the log-scale, with specific observation variance: \\[\\begin{equation} \\log \\hat{C}_{ay} \\sim \\text{Normal} \\Big( \\log C_{ay}, \\sigma^2_{ay}\\Big) \\end{equation}\\] \\[\\begin{equation} \\log \\hat{I}_{ays} \\sim \\text{Normal} \\Big( \\log I_{ays}, \\tau^2_{ays} \\Big) \\end{equation}\\] The log-likelihood can now be defined as the sum of the log-likelihood of the observed catches: \\[\\begin{equation} \\ell_C = \\sum_{ay} w^{(c)}_{ay}\\ \\ell_N \\Big( \\log C_{ay}, \\sigma^2_{ay} ;\\ \\log \\hat{C}_{ay} \\Big) \\end{equation}\\] and the log-likelihood of the observed survey indices as: \\[\\begin{equation} \\ell_I = \\sum_s \\sum_{ay} w^{(s)}_{ays}\\ \\ell_N \\Big( \\log I_{ays}, \\tau_{ays}^2 ;\\ \\log \\hat{I}_{ays} \\Big) \\end{equation}\\] giving the total log-likelihood \\[\\begin{equation} \\ell = \\ell_C + \\ell_I \\end{equation}\\] which is defined in terms of the strictly positive quantites, \\(M_{ay}\\), \\(F_{ay}\\), \\(Q_{ays}\\) and \\(R_{y}\\), and the observation variances \\(\\sigma_{ay}\\) and \\(\\tau_{ays}\\). As such, the log-likelihood is over-parameterised as there are many more parameters than observations. In order to reduce the number of parameters, \\(M_{ay}\\) is assumed known (as is common). The remaining parameters are written in terms of a linear combination of covariates \\(x_{ayk}\\), e.g. \\[\\begin{equation} \\log F_{ay} = \\sum_k \\beta_k x_{ayk} \\end{equation}\\] where \\(k\\) is the number of parameters to be estimated and is sufficiently small. Using this tecnique the quantities \\(\\log F\\), \\(\\log Q\\), \\(\\log \\sigma\\) and \\(\\log \\tau\\) %\\(\\log \\text{initial\\,age\\,structure}\\) % this is not present in the above (in bold in the equations above) can be described by a reduced number of parameters. The following section has more discussion on the use of linear models in a4a. The a4a statistical catch-at-age model can addionally allow for a functional relationship that links predicted recruitment \\(\\tilde{R}\\) based on spawning stock biomass and modelled recruitment \\(R\\), to be imposed as a fixed variance random effect. [NEEDS REVISION, sentence not clear] Options for the relationship are the hard coded models Ricker, Beverton Holt, smooth hockeystick or geometric mean. This is implemented by including a third component in the log-likelihood: \\[\\begin{equation} \\ell_{SR} = \\sum_y \\ell_N \\Big( \\log \\tilde{R}_y(a, b), \\phi_y^2 ;\\ \\log R_y \\Big) \\end{equation}\\] giving the total log-likelihood \\[\\begin{equation} \\ell = \\ell_C + \\ell_I + \\ell_{SR} \\end{equation}\\] Using the (time varying) Ricker model as an example, predicted recruitment is \\[\\begin{equation} \\tilde{R}_y(a_y,b_y) = a_y S_{y-1} e^{-b_y S_{y-1}} \\end{equation}\\] where \\(S\\) is spawning stock biomass derived from the model parameters \\(F\\) and \\(R\\), and the fixed quantites \\(M\\) and mean weights by year and age. It is assumed that \\(R\\) is log-normally distributed, or equivalently, normally distributed on the log-scale about the (log) recruitment predicted by the SR model \\(\\tilde{R}\\), with known variance \\(\\phi^2\\), i.e. \\[\\begin{equation} \\log R_y \\sim \\text{Normal} \\Big( \\log \\tilde{R}_y, \\phi_y^2 \\Big) \\end{equation}\\] which leads to the definition of \\(\\ell_{SR}\\) given above. In all cases \\(a\\) and \\(b\\) are strictly positive, and with the quantities \\(F\\), \\(R\\), etc. linear models are used to parameterise \\(\\log a\\) and/or \\(\\log b\\), where relevant. By default, recruitment \\(R\\) as apposed to the reruitment predicted from a stock recruitment model \\(\\tilde{R}\\), is specified as a linear model with a parameter for each year, i.e. \\[\\begin{equation} \\log R_y = \\gamma_y \\end{equation}\\] This is to allow modelled recruitment \\(R_y\\) to be shrunk towards the stock recruitment model. However, if it is considered appropriate that recruitment can be determined exactly by a relationship with covariates, it is possible, to instead define \\(\\log R\\) in terms of a linear model in the same way as \\(\\log F\\), \\(\\log Q\\), \\(\\log \\sigma\\) and \\(\\log \\tau\\). %But this is pretty much the same as taking a geometric mean, with a model on log a, and making the variance very small. 6.2 Classes Description The data structure used to store and report the fitting process follows an object-oriented paradigm (e.g., the S4 system in R) and is hierarchically organized. The type argument in the sca method defines the fitting method—either maximum likelihood or MCMC — and specifies whether the variance-covariance matrix of the parameters is calculated and returned in the case of maximum likelihood. The resulting object belongs to a specific class, depending on the selected option. Figure 6.2 illustrates the input/output model of the statistical stock assessment method based on catch-at-age data. Figure 6.2: The fit process input/output model Table 6.1 provides details about the type of fit approach and computation of variance covariance information. Table 6.1: Fit Types and Associated Classes Type of Fit Fit Method Variance-Covariance Matrix Output Object Class MP Maximum Likelihood No a4aFit Assessment Maximum Likelihood Yes a4aFitSA MCMC MCMC Yes(check) a4aFitMCMC Type MP, for “Management Procedure,” returns an a4aFit class object designed for use in MSEs (Management Strategy Evaluations) with full feedback models (REF PUNT and KELL). Inverting the Jacobian to compute the variance-covariance matrix is computationally intensive in maximum likelihood models, and as MSE analyses often involve thousands of iterations, using type=\"MP\" significantly speeds up the process. This option is advantageous for scenarios requiring multiple model fits. However, the lack of immediate feedback on model convergence is a drawback, as convergence is assessed by inverting the Jacobian. A failed inversion indicates non-convergence. Table 6.2 describes the composition of the a4aFit class. Table 6.2: a4aFit Class Description Class Slot Slot’s Class Description a4aFit call call Code used to run the analysis catch.n FLQuant Catch numbers at age and year clock numeric Time to run the analysis desc character Description of the stock and/or analysis fitSumm array Summary statistics of the fit (e.g., number of data points) harvest FLQuant Fishing mortality at age and year index FLQuants Indices of abundance (age/biomass, by year) name character Stock name range numeric Age and year range of the data stock.n FLQuant Population in numbers (age and year) The a4aFitSA and a4aFitMCMC classes extend a4aFit, retaining all its slots while adding a pars slot of class SCAPars. Table 6.3 outlines these classes. Table 6.3: a4aFitSA and a4aFitMCMC Class Description Class Slot Slot’s Class Description a4aFitSA All a4aFit Inherited from a4aFit pars SCAPars Parameter information a4aFitMCMC All a4aFit Inherited from a4aFit pars SCAPars Parameter information The SCAPars class stores details about submodel parameters, such as formulas and distributions, and includes three slots: stkmodel for stock model parameters, qmodel for catchability parameters, and vmodel for variance parameters. Table 6.4 describes the SCAPars class. Table 6.4: SCAPars Class Description Class Slot Slot’s Class Description SCAPars stkmodel a4aStkParams Details of fishing mortality, stock recruitment, and initial stock numbers qmodel submodel Details of catchability parameters vmodel submodel Details of variance parameters The stkmodel slot encompasses parameters for fishing mortality, stock recruitment, and initial stock numbers. Due to potential correlations among these parameters, their variance-covariance matrix is reported collectively. Table 6.5 describes the slots of the a4aStkParams class. Table 6.5: a4aStkParams Class Description Class Slot Slot’s Class Description a4aStkParams centering FLPar Centering parameters coefficients FLPar Model coefficients desc character Description distr character Distributions fMod formula Fishing mortality model link function Link function linkinv function Inverse link function m FLQuant Mortality parameters mat FLQuant Maturity parameters n1Mod formula Initial stock numbers model name character Stock name range numeric Age and year range srMod formula Stock-recruitment model units character Units of measurement vcov array Variance-covariance matrix wt FLQuant Weights The qmodel and vmodel slots share the submodel class, which describes single submodels. Table 6.6 provides details. Table 6.6: submodel Class Description Class Slot Slot’s Class Description submodel centering FLPar Centering parameters coefficients FLPar Model coefficients desc character Description distr character Distributions formula formula Submodel formula link function Link function linkinv function Inverse link function name character Stock name range numeric Age and year range vcov array Variance-covariance matrix "],["submodel-structure.html", "7 Submodel structure 7.1 Submodel building blocks and fundamental R formulas 7.2 The major effects available for modelling 7.3 The submodel class and methods", " 7 Submodel structure The a4a stock assessment framework allows the user to set up a large number of different models. The mechanics which provide this flexibility are designed around the concept of submodels. Each unknown variable that must be estimated is treated as a linear model, for which the user has to define the model structure using R formulas, including mgcv gam formulas. All submodels use the same specification process, the R formula interface, wich gives lot’s of flexibility to explore models and combination of submodels. There are 5 submodels in operation: a model for F-at-age (\\(F_{ay}\\)) a (list) of model(s) for abundance indices catchability-at-age (\\(Q_{ays}\\)), a model for recruitment (\\(R_y\\)) a list of models for the observation variance of catch-at-age and abundance indices (\\(\\{\\sigma^2_{ay}, \\tau^2_{ays}\\}\\)) a model for the initial age structure \\(N_{a,y=1}\\), When setting the structure of each submodel the user is in fact building the predictive model and its parameters. The optimization process, done through ADMB, estimates the parameters and their variance-covariance matrix, allowing further analysis to be carried out, like simulation, prediction, diagnostics, etc. All the statistical machinery will be at the user’s reach. 7.1 Submodel building blocks and fundamental R formulas The elements available to build submodels formulas are ‘age’ and ‘year’, which can be used to build models with different structures. In R’s linear modelling language, a constant model is coded as $ 1$, while a slope over time would simply be \\(\\sim year\\), a smoother over time \\(\\sim s(year, k=10)\\), a model with a coefficient for each year would be \\(\\sim factor(year)\\). Transformations of the variables are as usual, e.g. \\(\\sim sqrt(year)\\), etc; while combinations of all the above can be done although non-convergence will limit the possibilities. Using the \\(F\\) submodel as example the following specifies the models described in the previous paragraph: # models m1 &lt;- ~1 m2 &lt;- ~ year m3 &lt;- ~ s(year, k=10) m4 &lt;- ~ factor(year) m5 &lt;- ~ sqrt(year) # fits fit1 &lt;- sca(ple4, ple4.indices, fmodel=m1, fit=&quot;MP&quot;) fit2 &lt;- sca(ple4, ple4.indices, fmodel=m2, fit=&quot;MP&quot;) fit3 &lt;- sca(ple4, ple4.indices, fmodel=m3, fit=&quot;MP&quot;) fit4 &lt;- sca(ple4, ple4.indices, fmodel=m4, fit=&quot;MP&quot;) fit5 &lt;- sca(ple4, ple4.indices, fmodel=m5, fit=&quot;MP&quot;) # plot lst &lt;- FLStocks(constant=ple4+fit1, linear=ple4+fit2, smooth=ple4+fit3, factor=ple4+fit4, sqrt=ple4+fit5) lst &lt;- lapply(lst, fbar) lgnd &lt;- list(points=FALSE, lines=TRUE, space=&#39;right&#39;) xyplot(data~year, groups=qname, lst, auto.key=lgnd, type=&#39;l&#39;, ylab=&#39;fishing mortality&#39;) (#fig:fund_forms)Example of fundamental R formulas The models above and their combinations can be used to model both ‘age’ and ‘year’. The corresponding fits for age are: # models m1 &lt;- ~1 m2 &lt;- ~ age m3 &lt;- ~ s(age, k=3) m4 &lt;- ~ factor(age) m5 &lt;- ~ sqrt(age) # fits fit1 &lt;- sca(ple4, ple4.indices, fmodel=m1, fit=&quot;MP&quot;) fit2 &lt;- sca(ple4, ple4.indices, fmodel=m2, fit=&quot;MP&quot;) fit3 &lt;- sca(ple4, ple4.indices, fmodel=m3, fit=&quot;MP&quot;) fit4 &lt;- sca(ple4, ple4.indices, fmodel=m4, fit=&quot;MP&quot;) fit5 &lt;- sca(ple4, ple4.indices, fmodel=m5, fit=&quot;MP&quot;) # plot lst &lt;- FLStocks(constant=ple4+fit1, linear=ple4+fit2, smooth=ple4+fit3, factor=ple4+fit4, sqrt=ple4+fit5) lst &lt;- lapply(lst, function(x) harvest(x)[,&#39;2000&#39;]) xyplot(data~age, groups=qname, lst, auto.key=lgnd, type=&#39;l&#39;, ylab=&#39;fishing mortality in 2000&#39;) (#fig:fund_forms_age)Example of fundamental R formulas 7.2 The major effects available for modelling Although the building blocks for formulas are ‘age’ and ‘year’, in fact there are three effects that can be modelled for each submodel: ‘age’, ‘year’ and ‘cohort’. As examples note the following models for fishing mortality. # the age effect ageeffect &lt;- ~ factor(age) # the year effect yeareffect &lt;- ~ factor(year) # the cohort cohorteffect &lt;- ~ factor(year-age) # the fits fit1 &lt;- sca(ple4, ple4.indices, fmodel=yeareffect) fit2 &lt;- sca(ple4, ple4.indices, fmodel=ageeffect) fit3 &lt;- sca(ple4, ple4.indices, fmodel=cohorteffect) and the graphical representation of the three models in Figures 7.1 to 7.3. wireframe(harvest(fit1), main=&#39;year effect&#39;) Figure 7.1: Major effects: the year effect (~ factor(year)) wireframe(harvest(fit2), main=&#39;age effect&#39;) Figure 7.2: Major effects: the age effect (~ factor(age)) wireframe(harvest(fit3), main=&#39;cohort effect&#39;) Figure 7.3: Major effects: the cohort effect (~ factor(year-age)) 7.3 The submodel class and methods Although the specification of each submodel is done through a R formula, internally the a4a sca fit creates an object (of class ‘submodel’) which stores more information and allows a wide number of methods to be applied. The most important cases are prediction and simulation methods. The submodel class is a S4 class with the following slots: showClass(&quot;submodel&quot;) ## Class &quot;submodel&quot; [package &quot;FLa4a&quot;] ## ## Slots: ## ## Name: formula coefficients vcov centering distr ## Class: formula FLPar array FLPar character ## ## Name: link linkinv name desc range ## Class: function function character character numeric ## ## Extends: &quot;FLComp&quot; Objects of class ‘submodel’ are created in the fitting process using the formulas set by the user (or defaults if missing) and information from the fit. For example begining with a simple a4a fit: # fit a model with indices &quot;IBTS_Q1&quot; and &quot;IBTS_Q3&quot; fit0 &lt;- sca(ple4, ple4.indices[c(&quot;IBTS_Q1&quot;, &quot;IBTS_Q3&quot;)], fmodel = ~ s(age, k = 5), qmodel = list( ~ s(age, k = 4), ~ s(age, k = 4)), srmodel = ~ 1, n1model = ~ s(age, k = 5), vmodel = list( ~ 1, ~ 1, ~ 1), verbose = FALSE) ## a4a model fit for: PLE ## ## Call: ## .local(stock = stock, indices = indices, fmodel = ..1, qmodel = ..2, ## srmodel = ..3, n1model = ..4, vmodel = ..5, verbose = FALSE) ## ## Time used: ## Pre-processing Running a4a Post-processing Total ## 0.25930214 0.62122297 0.04072976 0.92125487 ## ## Submodels: ## fmodel: ~s(age, k = 5) ## srmodel: ~1 ## n1model: ~s(age, k = 5) ## qmodel: ## IBTS_Q1: ~s(age, k = 4) ## IBTS_Q3: ~s(age, k = 4) ## vmodel: ## catch: ~1 ## IBTS_Q1: ~1 ## IBTS_Q3: ~1 "],["fitting.html", "8 Fitting 8.1 Fishing mortality submodel (\\(F_{ay}\\)) 8.2 Abundance indices catchability submodel (\\(Q_{ays}\\)) 8.3 Stock-recruitment submodel (\\(R_y\\)) 8.4 Observation variance submodel (\\(\\{\\sigma^2_{ay}, \\tau^2_{ays}\\}\\)) 8.5 Initial year abundance submodel (\\(N_{a,y=1}\\)) 8.6 More models 8.7 Working with covariates 8.8 Assessing files 8.9 Missing observations in the catch matrix or index", " 8 Fitting The a4a stock assessment framework is implemented in R through the method sca(). The method call requires as a minimum a FLStock object and a FLIndices object, in which case the default submodels will be set by the method. Having described building blocks, basic formulations and effects available to build a submodel’s model, it’s important to look into specific formulations and relate them to commonly known representations. Note that although a large number of formulations are available for each submodel, the user must carefuly decide on the full stock assessment model being build and avoid over-paramerizing. Over-parametrization may lead to non-convergence, but may also end up not being very useful for prediction/forecasting, which is one of the main objectives of stock assessment. data(ple4) data(ple4.indices) fit &lt;- sca(ple4, ple4.indices) stk &lt;- ple4 + fit Figure 8.1: Stock summary By calling the fitted object the default submodel formulas are printed in the console: ## a4a model fit for: PLE ## ## Call: ## .local(stock = stock, indices = indices) ## ## Time used: ## Pre-processing Running a4a Post-processing Total ## 1.2767117 18.5619087 0.1805279 20.0191483 ## ## Submodels: ## fmodel: ~te(age, year, k = c(6, 30), bs = &quot;tp&quot;) + s(age, k = 6) ## srmodel: ~factor(year) ## n1model: ~s(age, k = 3) ## qmodel: ## BTS-Isis-early: ~s(age, k = 6) ## BTS-Combined (ISIS and TRIDENS): ~s(age, k = 6) ## SNS: ~s(age, k = 5) ## BTS-Combined (all): ~s(age, k = 6) ## IBTS_Q3: ~s(age, k = 6) ## IBTS_Q1: ~s(age, k = 5) ## vmodel: ## catch: ~s(age, k = 3) ## BTS-Isis-early: ~1 ## BTS-Combined (ISIS and TRIDENS): ~1 ## SNS: ~1 ## BTS-Combined (all): ~1 ## IBTS_Q3: ~1 ## IBTS_Q1: ~1 To set specific submodels the user has to write the relevant R formula and include it in the call. The arguments for each submodel are self-explanatory: fishing mortality is ‘fmodel’, indices’ catchability is ‘qmodel’, stock-recruitment is ‘srmodel’, observation variance is ‘vmodel’ and for initial year’s abundance is ‘n1model’. The following model comes closer to the official stock assessment of North Sea plaice, as such we’ll name it \\(0\\) and keep it for future comparisons. fmod0 &lt;- ~s(age, k=6)+s(year, k=10)+te(age, year, k=c(3,8)) qmod0 &lt;- list(~s(age, k = 4), ~s(age, k = 3), ~s(age, k = 3)+year, ~s(age, k = 3), ~s(age, k = 4), ~s(age, k = 6)) srmod0 &lt;- ~ s(year, k=20) vmod0 &lt;- list(~s(age, k=4), ~1, ~1, ~1, ~1, ~1, ~1, ~1) n1mod0 &lt;- ~ s(age, k=3) fit0 &lt;- sca(ple4, ple4.indices, fmodel=fmod0, qmodel=qmod0, srmodel=srmod0, n1model=n1mod0, vmodel=vmod0) stk0 &lt;- ple4 + fit0 Figure 8.2: Stock summary - close to official assessment As before by calling the fitted object submodels’ formulas are printed in the console: ## a4a model fit for: PLE ## ## Call: ## .local(stock = stock, indices = indices) ## ## Time used: ## Pre-processing Running a4a Post-processing Total ## 1.2767117 18.5619087 0.1805279 20.0191483 ## ## Submodels: ## fmodel: ~te(age, year, k = c(6, 30), bs = &quot;tp&quot;) + s(age, k = 6) ## srmodel: ~factor(year) ## n1model: ~s(age, k = 3) ## qmodel: ## BTS-Isis-early: ~s(age, k = 6) ## BTS-Combined (ISIS and TRIDENS): ~s(age, k = 6) ## SNS: ~s(age, k = 5) ## BTS-Combined (all): ~s(age, k = 6) ## IBTS_Q3: ~s(age, k = 6) ## IBTS_Q1: ~s(age, k = 5) ## vmodel: ## catch: ~s(age, k = 3) ## BTS-Isis-early: ~1 ## BTS-Combined (ISIS and TRIDENS): ~1 ## SNS: ~1 ## BTS-Combined (all): ~1 ## IBTS_Q3: ~1 ## IBTS_Q1: ~1 The method sca has other arguments which may be set by the user: Argument Description covar a FLQuant with covariates wkdir a folder (character) where the files will be saved for posterior inspection by the user verbose be more verbose (logical) fit type of fit (character): ‘MP’ runs the minimizer without trying to invert the hessian and as such doesn’t return the covariance matrix of the parameters, normally used inside MSE loops where parameter variance may not be relevant; ‘assessment’ runs minimizer and inverts hessian, returns the covariance matrix of the estimated parameters and the convergence criteria set in ADMB; ‘MCMC’ runs ADMB’s MCMC fit center shall observations be centered before fitting (logical) mcmc ADMB’s MCMC arguments (character vector), must be paired with `fit=“MCMC” There are a set of methods for a4a fit objects which help manipulating sca() results, namely: [+:] update the stock object with the fitted fishing mortalities, population abundance and catch in numbers at age; can be applied to FLStocks and a4aFits objects as well. 8.1 Fishing mortality submodel (\\(F_{ay}\\)) We will now take a look at some examples for \\(F\\) models and the forms that we can get. A non-separable model, where we consider age and year to interact can be modeled using a smooth interaction term in the F model using a tensor product of cubic splines with the te method (Figure 8.3), again borrowed from mgcv. fmod &lt;- ~ te(age, year, k = c(4,20)) fit &lt;- sca(ple4, ple4.indices[1], fmod) Figure 8.3: Fishing mortality smoothed non-separable model In the last examples the fishing mortalities (Fs’) are linked across age and time. What if we want to free up a specific age class because in the residuals we see a consistent pattern. This can happen, for example, if the spatial distribution of juveniles is disconnected to the distribution of adults. The fishery focuses on the adult fish, and therefore the the F on young fish is a function of the distribution of the juveniles and could deserve a specific model. This can be achieved by adding a component for the year effect on age 1 (Figure 8.4). fmod &lt;- ~ te(age, year, k = c(4,20)) + s(year, k = 5, by = as.numeric(age==1)) fit &lt;- sca(ple4, ple4.indices[1], fmod) Figure 8.4: Fishing mortality age-year interaction model with extra age 1 smoother. 8.1.1 Separable model One of the most useful models for fishing mortality is one in which ‘age’ and ‘year’ effects are independent, that is, where the shape of the selection pattern does not change over time, but the overall level of fishing mortality do. Commonly called a ‘separable model’. A full separable model in a4a is written using the factor function which converts age and year effects into categorical values, forcing a different coefficient to be estimated for each level of both effects. This model has age x year number of parameters. fmod1 &lt;- ~ factor(age) + factor(year) fit1 &lt;- sca(ple4, ple4.indices, fmodel=fmod1, fit=&quot;MP&quot;) One can reduce the number of parameters and add dependency along both effects, although still keeping independence of each other, by using smoothers rather than factor. We’ll use a (unpenalised) thin plate spline provided by package mgcv method s(). We’re using the North Sea Plaice data, and since it has 10 ages we will use a simple rule of thumb that the spline should have fewer than \\(\\frac{10}{2} = 5\\) degrees of freedom, and so we opt for 4 degrees of freedom. We will also do the same for year and model the change in \\(F\\) through time as a smoother with 20 degrees of freedom. fmod2 &lt;- ~ s(age, k=4) + s(year, k=20) fit2 &lt;- sca(ple4, ple4.indices, fmodel=fmod2, fit=&quot;MP&quot;) An interesting extension of the separable model is the ‘double separable’ where a third factor or smoother is added for the cohort effect. fmod3 &lt;- ~ s(age, k=4) + s(year, k=20) + s(as.numeric(year-age), k=10) fit3 &lt;- sca(ple4, ple4.indices, fmodel=fmod3, fit=&quot;MP&quot;) Figures 8.5 and 8.6 depicts the three models selectivities for each year. Each separable model has a single selectivity that changes it’s overall scale in each year, while the double separable introduces some variability over time by modeling the cohort factor. Figure 8.5: Selection pattern of separable models. Each line represents the selection pattern in a specific year. Independent age and year effects (factor), internally dependent age and year (smooth), double separable (double). Figure 8.6: Fishing mortality of separable models. Independent age and year effects (factor), internally dependent age and year (smooth), double separable (double). 8.1.2 Constant selectivity for contiguous ages or years To set these models we’ll use the method replace() to define which ages or years will be modelled together with a single coefficient. The following example shows replace() in operation. The dependent variables used in the model will be changed and attributed the same age or year, as such during the fit observations of those age or year with will be seen as replicates. One can think of it as sharing the same mean value, which will be estimated by the model. age &lt;- 1:10 # last age same as previous replace(age, age&gt;9, 9) ## [1] 1 2 3 4 5 6 7 8 9 9 # all ages after age 6 replace(age, age&gt;6, 6) ## [1] 1 2 3 4 5 6 6 6 6 6 year &lt;- 1950:2010 replace(year, year&gt;2005, 2005) ## [1] 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 ## [16] 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 ## [31] 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 ## [46] 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2005 2005 2005 2005 ## [61] 2005 In the \\(F\\) submodel one can use this method to fix the estimation of \\(F\\) in the plus group to be the same as in the last non-aggregated age. fmod &lt;- ~ s(replace(age, age&gt;9, 9), k=4) + s(year, k=20) fit &lt;- sca(ple4, ple4.indices, fmod) Figure 8.7: F-at-age fixed above age 9 Or estimate the average \\(F\\) in the most recent years, instead of averaging after the assessment to compute the statu quo selection pattern. fmod &lt;- ~ s(age, k=4) + s(replace(year, year&gt;2013, 2013), k=20) fit &lt;- sca(ple4, ple4.indices, fmod) Figure 8.8: F-at-age fixed for the most recent 5 years 8.1.3 Time blocks selectivity To define blocks of data sca() uses the method breakpts(), which creates a factor from a vector with levels defined by the second argument. year &lt;- 1950:2010 # two levels separated in 2000 breakpts(year, 2000) ## [1] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] ## [7] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] ## [13] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] ## [19] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] ## [25] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] ## [31] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] ## [37] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] ## [43] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] ## [49] (1949,2000] (1949,2000] (1949,2000] (2000,2010] (2000,2010] (2000,2010] ## [55] (2000,2010] (2000,2010] (2000,2010] (2000,2010] (2000,2010] (2000,2010] ## [61] (2000,2010] ## Levels: (1949,2000] (2000,2010] # five periods with equal interval breakpts(year, seq(1949, 2010, length=6)) ## [1] (1949,1961.2] (1949,1961.2] (1949,1961.2] (1949,1961.2] ## [5] (1949,1961.2] (1949,1961.2] (1949,1961.2] (1949,1961.2] ## [9] (1949,1961.2] (1949,1961.2] (1949,1961.2] (1949,1961.2] ## [13] (1961.2,1973.4] (1961.2,1973.4] (1961.2,1973.4] (1961.2,1973.4] ## [17] (1961.2,1973.4] (1961.2,1973.4] (1961.2,1973.4] (1961.2,1973.4] ## [21] (1961.2,1973.4] (1961.2,1973.4] (1961.2,1973.4] (1961.2,1973.4] ## [25] (1973.4,1985.6] (1973.4,1985.6] (1973.4,1985.6] (1973.4,1985.6] ## [29] (1973.4,1985.6] (1973.4,1985.6] (1973.4,1985.6] (1973.4,1985.6] ## [33] (1973.4,1985.6] (1973.4,1985.6] (1973.4,1985.6] (1973.4,1985.6] ## [37] (1985.6,1997.8] (1985.6,1997.8] (1985.6,1997.8] (1985.6,1997.8] ## [41] (1985.6,1997.8] (1985.6,1997.8] (1985.6,1997.8] (1985.6,1997.8] ## [45] (1985.6,1997.8] (1985.6,1997.8] (1985.6,1997.8] (1985.6,1997.8] ## [49] (1997.8,2010] (1997.8,2010] (1997.8,2010] (1997.8,2010] ## [53] (1997.8,2010] (1997.8,2010] (1997.8,2010] (1997.8,2010] ## [57] (1997.8,2010] (1997.8,2010] (1997.8,2010] (1997.8,2010] ## [61] (1997.8,2010] ## 5 Levels: (1949,1961.2] (1961.2,1973.4] (1973.4,1985.6] ... (1997.8,2010] Note seq() computes ‘left-open’ intervals, which means that to include 1950 the sequence must start one year earlier. These methods can be used to create discrete time series, for which a different selection pattern is allowed in each block. This is called an interaction in statistical modelling parlance, and typically a * denotes an interaction term; for smoothers an interaction is achieved using the by argument. When this argument is a factor a replicate of the smooth is produced for each factor level. In the next case we’ll use the breakpts() to split the time series at 1990, although keeping the same shape in both periods, a thin plate spline with 3 knots (Figure 8.9). fmod &lt;- ~s(age, k = 3, by = breakpts(year, 1990)) fit &lt;- sca(ple4, ple4.indices, fmod) Figure 8.9: F-at-age in two periods using in both cases a thin plate spline with 3 knots 8.1.4 Time changing selectivity In many cases, it may be desirable to allow the selection pattern to evolve over time, from year to year. Again there are several ways to do this, one way is to estimate a mean selection pattern, while also allowing F to vary over time for each age. This is like a seperate smoother over year, with ‘age blocks’ so, looking back at previous examples, we have: fmodel &lt;- ~ s(year, k = 15, by = factor(age)) + s(age, k = 4) This is a type of interaction between age and year, but the only connection (or correlation) across ages is via the smoother on age, however there are still 15 degrees of freedom for each age, so the model 5 x 15 + 4 = 69 degrees of freedom. To include correlation across ages and years together then the tensor product (te() function) is used, this has the effect of restricting the flexibility of the model for F. In the following, there is a smoother in 2 dimensions (age and year) where there is 5 degrees of freedom in the age direction, and 15 in the year dimension, resulting in a total of 5 x 15 = 65 degrees of freedom fmodel &lt;- ~ te(age, year, k = c(5, 15)) Often the above formulations provide too much flexibility, and a more complicated, but simpler model is preferable: fmodel &lt;- ~ s(age, k = 4) + s(year, k = 15) + te(age, year, k = c(3, 5)) in the above model, the main effects for age and year still have similar flexibility to the full tensor model, however, the interaction (or the change in F at age over time) has been restricted, so that the full model now has 4 + 15 + 3 x 5 = 34 degrees of freedom. 8.1.5 Closed form selection pattern One can use a closed form for the selection pattern. The only requirement is to be able to write it as a formula, the example below uses a logistic form. fmod &lt;- ~ I(1/(1+exp(-age))) fit &lt;- sca(ple4, ple4.indices, fmod) Figure 8.10: F-at-age logistic 8.2 Abundance indices catchability submodel (\\(Q_{ays}\\)) The catchability submodel is set up the same way as the \\(F\\) submodel and the tools available are the same. The only difference is that the submodel is set up as a list of formulas, where each formula relates with one abundance index. There’s no limitation in the number of indices or type that can be used for a fit. It’s the analyst that has to decide based on her/his expertise and knowledge of the stock and fleet dynamics. 8.2.1 Catchability submodel for age based indices A first model is simply a dummy effect on age, which means that a coefficient will be estimated for each age. Note that this kind of model considers that levels of the factor are independent (Figure 8.11). qmod &lt;- list(~factor(age)) fit &lt;- sca(ple4, ple4.indices[1], qmodel=qmod) qhat &lt;- predict(fit)$qmodel[[1]] Figure 8.11: Catchability age independent model If one considers catchability at a specific age to be dependent on catchability on the other ages, similar to a selectivity modelling approach, one option is to use a smoother at age, and let the data ‘speak’ regarding the shape (Figure 8.12). qmod &lt;- list(~ s(age, k=4)) fit &lt;- sca(ple4, ple4.indices[1], qmodel=qmod) qhat &lt;- predict(fit)$qmodel[[1]] wireframe(qhat, zlab=&quot;q&quot;) Figure 8.12: Catchability smoother age model Finally, one may want to investigate a trend in catchability with time, very common in indices built from CPUE data. In the example given here we’ll use a linear trend in time, set up by a simple linear model (Figure 8.13). qmod &lt;- list( ~ s(age, k=4) + year) fit &lt;- sca(ple4, ple4.indices[1], qmodel=qmod) qhat &lt;- predict(fit)$qmodel[[1]] Figure 8.13: Catchability with a linear trend in year 8.2.2 Catchability submodel for age aggregated biomass indices The previous section was focused on age disaggregated indices, but age aggregated indices (CPUE, biomass, DEPM, etc) may also be used to tune the total biomass of the population. In these cases a different class for the index must be used, the FLIndexBiomass, which uses a vector index with the age dimension called ‘all’. Note that in this case the qmodel should be set without age factors, although it can have a ‘year’ component and covariates if needed. An interesting feature with biomass indices is the age range they refer to can be specified. # simulating a biomass index (note the name of the first dimension element) using # the ple4 biomass and an arbritary catchability of 0.001 plus a lognormal error. dnms &lt;- list(age=&quot;all&quot;, year=range(ple4)[&quot;minyear&quot;]:range(ple4)[&quot;maxyear&quot;]) bioidx &lt;- FLIndexBiomass(FLQuant(NA, dimnames=dnms)) index(bioidx) &lt;- stock(ple4)*0.001 index(bioidx) &lt;- index(bioidx)*exp(rnorm(index(bioidx), sd=0.1)) range(bioidx)[c(&quot;startf&quot;,&quot;endf&quot;)] &lt;- c(0,0) # note the name of the first dimension element index(bioidx) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 ## all 404 381 401 418 463 827 517 645 444 625 512 474 446 442 ## year ## age 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 ## all 408 490 427 508 480 487 493 471 508 492 474 542 552 811 ## year ## age 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 ## all 621 671 711 629 578 592 563 366 377 309 307 289 392 402 ## year ## age 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 ## all 366 360 366 386 330 461 431 418 479 510 718 622 674 714 ## year ## age 2013 2014 2015 2016 2017 ## all 940 915 935 1111 981 ## ## units: t # fitting the model fit &lt;- sca(ple4, FLIndices(bioidx), qmodel=list(~1)) To estimate a constant selectivity over time one used the model \\(\\sim 1\\). As a matter of fact the estimate value, 0.001, is not very far from the simulated one, 0.001. An example where the biomass index refers only to age 2 to 4 (for example a CPUE that targets these particular ages). # creating the index dnms &lt;- list(age=&quot;all&quot;, year=range(ple4)[&quot;minyear&quot;]:range(ple4)[&quot;maxyear&quot;]) bioidx &lt;- FLIndexBiomass(FLQuant(NA, dimnames=dnms)) # but now use only ages 2:4 index(bioidx) &lt;- tsb(ple4[ac(2:4)])*0.001 index(bioidx) &lt;- index(bioidx)*exp(rnorm(index(bioidx), sd=0.1)) range(bioidx)[c(&quot;startf&quot;,&quot;endf&quot;)] &lt;- c(0,0) # to pass this information to the model one needs to specify an age range range(bioidx)[c(&quot;min&quot;,&quot;max&quot;)] &lt;- c(2,4) # fitting the model fit &lt;- sca(ple4, FLIndices(bioidx), qmodel=list(~1)) Once more the estimate value, 0.001, is not very far from the simulated one, 0.001. 8.2.3 Catchability submodel for single age indices Similar to age aggregated indices one may have an index that relates only to one age, like a recruitment index. In this case the `FLIndex} object must have in the first dimension the age it referes to. The fit is then done relating the index with the proper age in numbers. Note that in this case the qmodel should be set without age factors, although it can have a ‘year’ component and covariates if needed. idx &lt;- ple4.indices[[1]][1] fit &lt;- sca(ple4, FLIndices(recidx=idx), qmodel=list(~1)) # the estimated catchability is predict(fit)$qmodel[[1]] ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 1985 1986 1987 1988 1989 1990 1991 1992 ## 1 0.000302 0.000302 0.000302 0.000302 0.000302 0.000302 0.000302 0.000302 ## year ## age 1993 1994 1995 ## 1 0.000302 0.000302 0.000302 ## ## units: 8.3 Stock-recruitment submodel (\\(R_y\\)) The S/R submodel is a special case, in the sense that it can be set up with the same linear tools as the \\(F\\) and \\(Q\\) models, but it can also use some hard coded models. The example shows how to set up a simple dummy model with factor(), a smooth model with s(), a Ricker model (ricker()), a Beverton and Holt model (bevholt()), a hockey stick model (hockey()), and a geometric mean model (geomean()). See Figure 8.14 for results. As mentioned before, the ‘structural’ models have a fixed variance, which must be set by defining the coefficient of variation. srmod &lt;- ~ factor(year) fit &lt;- sca(ple4, ple4.indices, srmodel=srmod) srmod &lt;- ~ s(year, k=10) fit1 &lt;- sca(ple4, ple4.indices, srmodel=srmod) srmod &lt;- ~ ricker(CV=0.3) fit2 &lt;- sca(ple4, ple4.indices, srmodel=srmod) srmod &lt;- ~ bevholt(CV=0.3) fit3 &lt;- sca(ple4, ple4.indices, srmodel=srmod) srmod &lt;- ~ hockey(CV=0.3) fit4 &lt;- sca(ple4, ple4.indices, srmodel=srmod) srmod &lt;- ~ geomean(CV=0.3) fit5 &lt;- sca(ple4, ple4.indices, srmodel=srmod) Figure 8.14: Stock-recruitment models fits 8.4 Observation variance submodel (\\(\\{\\sigma^2_{ay}, \\tau^2_{ays}\\}\\)) The variance model allows the user to set up the shape of the observation variances \\(\\sigma^2_{ay}\\) and \\(\\tau^2_{ays}\\). This is an important subject related with fisheries data used for input to stock assessment models. The defaults assume a U-shape model for catch-at-age and constant variance for abundance indices. The first relies on the fact that it’s common to have more precision on the most represented ages and less precision on the less frequent ages which tend to be the younger and older individuals. These sizes are less caught by the fleets and as such do not appear as often at the auction markets samples. With regards to the abundance indices, one assumes a scientific survey to have a well designed sampling scheme and protocols which keep observation error at similar levels across ages. vmod &lt;- list(~s(age, k=3), ~1) fit1 &lt;- sca(ple4, ple4.indices[1], vmodel=vmod) vmod &lt;- list(~s(age, k=3), ~s(age, k=3)) fit2 &lt;- sca(ple4, ple4.indices[1], vmodel=vmod) Variance estimated for the constant model is 0.476 while for the U-shape model, fitted with a smoother, changes with ages (Figure 8.15). Figure 8.15: Abundance index observation variance estimate Observation variance options have an impact in the final estimates of population abundance, which can be seen in Figure 8.16. Figure 8.16: Population estimates using two different variance models 8.5 Initial year abundance submodel (\\(N_{a,y=1}\\)) The submodel for the stock number at age in the first year of the time series is set up with the usual modelling tools (Figure 8.17). Beare in mind that the year effect does not make sense here since it refers to a single year, the first in the time series of data available. This model has its influence limited to the initial lower triangle of the population matrix, which in assessments with long time series doesn’t make much difference. Nevertheless, when modelling stocks with short time series in relation to the number of ages present, it becomes more important and should be given proper attention. n1mod &lt;- ~s(age, k=3) fit1 &lt;- sca(ple4, ple4.indices, fmodel=fmod0, qmodel=qmod0, srmodel=srmod0, vmodel=vmod0, n1model=n1mod) n1mod &lt;- ~factor(age) fit2 &lt;- sca(ple4, ple4.indices, fmodel=fmod0, qmodel=qmod0, srmodel=srmod0, vmodel=vmod0, n1model=n1mod) flqs &lt;- FLQuants(smother=stock.n(fit1)[,1], factor=stock.n(fit2)[,1]) Figure 8.17: Nay=1 models The impact in the overall perspective of the stock status is depicted in Figure 8.18. As time goes by the effect of this model vanishes and the fits become similar. Figure 8.18: Population estimates using two different variance models 8.6 More models More complicated models can be built with these tools. The models need to be appropriate for the fmodel dimension, e.g. the N1 model doesn’t have the year dimension so setting a model with an year effect will fail, apart from that every submodel can use any model the analyst is interested in. The limitation is going to be the potential overparametrization of the model and the failure to fit if the data isn’t informative enough. For example, Figure 8.19 shows a model where the age effect is modelled as a smoother (the same thin plate spline) throughout years but independent from each other. fmod &lt;- ~ factor(age) + s(year, k=10, by = breakpts(age, c(2:8))) fit &lt;- sca(ple4, ple4.indices, fmod) Figure 8.19: F-at-age as thin plate spline with 3 knots for each age A quite complex model that implements a cohort effect can be set through the following formula. Figure 8.20 shows the resulting fishing mortality. Note that in this case we end up with a variable F pattern over time, but rather than using 4 * 10 = 40 parameters, it uses, 4 + 10 + 10 = 24. fmodel &lt;- ~ s(age, k = 4) + s(pmax(year - age, 1957), k = 10) + s(year, k = 10) fit &lt;- sca(ple4, ple4.indices, fmodel=fmodel) Figure 8.20: F-at-age with a cohort effect. The following model is applied to the vmodel and it introduces an time trend to reflect the increase in precision in more recent years with improvements in sampling design and increase in sampling effort. vmod &lt;- list( ~ s(age, k = 3) + year, ~1, ~1, ~1, ~1, ~1, ~1 ) fit &lt;- sca(ple4, ple4.indices, vmodel=vmod) Figure 8.21: Catch at age variance model with a year effect. This model fits smoothers to different sets of ages. fmod &lt;- ~s(age, k = 3, by = breakpts(age, 5)) + s(year, k = 10) fit &lt;- sca(ple4, ple4.indices, fmodel = fmod) Figure 8.22: Catch at age variance model with a year effect. flsts &lt;- FLStocks(nowgt=ple4+fit0, wgt=ple4 + fit1) plot(flsts) 8.7 Working with covariates In linear model one can use covariates to explain part of the variance observed on the data that the ‘core’ model does not explain. The same can be done in the a4a framework. The example below uses the North Atlantic Oscillation (NAO) index to model recruitment. nao &lt;- read.table(&quot;https://www.cpc.ncep.noaa.gov/products/precip/CWlink/pna/norm.nao.monthly.b5001.current.ascii.table&quot;, skip=1, fill=TRUE, na.strings=&quot;-99.90&quot;) dnms &lt;- list(quant=&quot;nao&quot;, year=1950:2024, unit=&quot;unique&quot;, season=1:12, area=&quot;unique&quot;) nao &lt;- FLQuant(unlist(nao[,-1]), dimnames=dnms, units=&quot;nao&quot;) nao &lt;- seasonMeans(trim(nao, year=dimnames(stock.n(ple4))$year)) First by simply assuming that the NAO index drives recruitment (Figure 8.23). srmod &lt;- ~ s(nao, k=10) fit2 &lt;- sca(ple4, ple4.indices[1], qmodel=list(~s(age, k=4)), srmodel=srmod, covar=FLQuants(nao=nao)) flqs &lt;- FLQuants(simple=stock.n(fit)[1], covar=stock.n(fit2)[1]) Figure 8.23: Recruitment model with covariates. Using the NAO index as a recruitment index. In a second model we’re using the NAO index not to model recruitment directly but to model one of the parameters of the S/R function (Figure 8.24). srmod &lt;- ~ ricker(a=~nao, CV=0.25) fit3 &lt;- sca(ple4, ple4.indices[1], qmodel=list(~s(age, k=4)), srmodel=srmod, covar=FLQuants(nao=nao)) flqs &lt;- FLQuants(simple=stock.n(fit)[1], covar=stock.n(fit3)[1]) Figure 8.24: Recruitment model with covariates. Using the NAO index as a covariate for the stock-recruitment model parameters. Note that covariates can be added to any submodel using the linear model capabilities of R. 8.8 Assessing files The framework gives access to the files produced to run the ADMB fitting routine through the argument wkdir. When set up all the ADMB files will be left in the directory. Note that the ADMB tpl file is distributed with the FLa4a. One can get it from your R library, under the folder myRlib/FLa4a/admb/. fit1 &lt;- sca(ple4, ple4.indices, wkdir=&quot;fit1run&quot;) 8.9 Missing observations in the catch matrix or index Missing observations are encoded as NA, and usually occur if there was no sampling for a year, or, since we model observations on the log scale, if the observation was zero. The a4a framework can deal with missing observations in the catches and indices. The example below shows how to set up a model with missing observations in the catch matrix, to demonstrate the effect of missing observations, using the default model settings. fit &lt;- sca(ple4, ple4.indices) ple4_missing &lt;- ple4 catch.n(ple4_missing)[ac(1:2), &quot;2013&quot;] &lt;- NA fit_missing &lt;- sca(ple4_missing, ple4.indices) In effect the information on F and Q for the missing observations is taken from the structural assumptions in the model. If a seperable F model is used, the F at age comes from the relationship between F at the other ages in that year, and the level of F from the relationship across years for the ages which have data. The same is true for the Q model. The effect of missing observations can be seen in the figures below, where box plots of the predicted catch at age with estimation error is shown. Figure 8.25: Stock estimates with missing observations. This is a simple example, but the same principle applies to more complex models. However, if there are many missing observations, the model cannot be too flexible, otherwise the model will not be able to estimate the missing observations. Another point to note, is that if observations are systematically missing, for example due to the actual observation being below a detection limit, or zero, then the model may overestimate the true catch at age. This is a common problem in stock assessment models, and is not unique to the a4a framework. Proposed solutions to this issue are to replace zeros with a small number, or half of the smallest observed value. "],["diagnostics.html", "9 Diagnostics 9.1 Residuals 9.2 Predictive skill 9.3 Fit summary, information and cross-validation metrics 9.4 The package a4adiags 9.5 Retrospective analysis 9.6 Hindcast", " 9 Diagnostics There’s a large number of diagnostics that can be computed for a stock assessment model, the a4a framework implements several analysis of residuals, visualizations and statistics that can be used to evaluate the fit quality and chose across multiple fits. 9.1 Residuals Residuals are a ubiquos metrics to check quality of a fit. For sca() fits there are out-of-the-box methods to compute in the log scale, raw residuals (aka deviances), standardized residuals and pearson residuals. A set of plots to inspect residuals and evaluate fit quality and assumptions are implemented. Consider \\(x_{ay}\\) to be either a catch-at-age matrix (\\(C_{ay}\\)) or one abundance index (\\(I_{ay}\\)) and \\(d\\) to represent residuals. Raw residuals are compute by \\(d_{ay} = \\log{x_{ay}} - \\log{\\tilde{x}_{ay}}\\) and have distribution \\(N(0,\\upsilon^2_{a})\\). Standardized residuals will be compute with \\(d^s_{ay} = \\frac{d_{ay}}{\\hat{\\upsilon}^2_{a}}\\) where \\(\\hat{\\upsilon}^2_{a} = (n-1)^{-1} \\sum_y(d_{ay})^2\\). Pearson residuals scale raw residuals by the estimates of \\(\\sigma^2\\) or \\(\\tau^2\\), as such \\(d^p_{ay} = \\frac{d_{ay}}{\\tilde{\\upsilon}^2_{a}}\\) where \\(\\tilde{\\upsilon}^2_{a} = \\tilde{\\sigma}^2_{a}\\) for catches, or \\(\\tilde{\\upsilon}^2_{a} = \\tilde{\\tau}^2_{a}\\) for each index of abundance. The residuals() method will compute these residuals and generate a object which can be plotted using a set of packed methods. The argument type will allow the user to chose which residuals will be computed. By default the method computes standardized residuals. fit &lt;- sca(ple4, ple4.indices) d_s &lt;- residuals(fit, ple4, ple4.indices) Figure 9.1 shows a scatterplot of standardized residuals with a smoother to guide (or mis-guide …) your visual analysis. Note that the standardization should produce residuals with variance=1, which means that most residual values should be between \\(\\sim -2\\) and \\(\\sim 2\\). plot(d_s) Figure 9.1: Standardized residuals for abundance indices and catch numbers (catch.n). Each panel is coded by age class, dots represent standardized residuals and lines a simple smoother. When plotting residuals by default the auxiliar line is a smoother. However it’s possible to use other type of lines by setting the argument “auxline” in plot. The argument can take the values used by xyplot, which are (from panel.xyplot help page) one or more of the following: “p”, “l”, “h”, “b”, “o”, “s”, “S”, “g”, “r”, “a”, “smooth”, and “spline”. If type has more than one element, an attempt is made to combine the effect of each of the components. The behaviour if any of the first five are included is similar to the effect of the corresponding type in plot: “p” and “l” stand for points and lines respectively; “b” and “o” (for ‘overlay’) plot both; “h” draws vertical (or horizontal if horizontal = TRUE) line segments from the points to the origin. Types “s” and “S” are like “l” in the sense that they join consecutive points, but instead of being joined by a straight line, points are connected by a vertical and a horizontal segment forming a ‘step’, with the vertical segment coming first for “s”, and the horizontal segment coming first for “S”. “g” adds a reference grid. Type “r” adds a linear regression line, “smooth” adds a loess fit, “spline” adds a cubic smoothing spline fit, and “a” draws line segments joining the average y value for each distinct x value. Figure 9.2 shows a regression line over the residuals instead of the loess smooother. plot(d_s, auxline=&quot;r&quot;) Figure 9.2: Standardized residuals for abundance indices and catch numbers (catch.n). Each panel is coded by age class, dots represent standardized residuals and lines a simple smoother. The common bubble plot (bubble()) are shown in Figure 9.3. It shows the same information as Figure 9.1 but in a multivariate perspective. bubbles(d_s) Figure 9.3: Bubbles plot of standardized residuals for abundance indices and for catch numbers (catch.n). Figure 9.4 shows a quantile-quantile plot to assess how well standardized residuals match a normal distribution. qqmath(d_s) Figure 9.4: Quantile-quantile plot of standardized residuals for abundance indices and catch numbers (catch.n). Each panel is coded by age class, dots represent standardized residuals and lines the normal distribution quantiles. Pearson residuals can be computed and plotted the same way as standardized residuals by setting fit='pearson' (Figure 9.5). d_p &lt;- residuals(fit, ple4, ple4.indices, type=&#39;pearson&#39;) plot(d_p) Figure 9.5: Pearson residuals for abundance indices and catch numbers (catch.n). Each panel is coded by age class, dots represent standardized residuals and lines a simple smoother. Finally, the raw residuals are computed by setting fit='deviances' and plotted the same way as before (Figure 9.6). These residuals are usefull to identify which data points are not well modelled, showing a large dispersion of the residuals and requiring more attention from the analyst. d_r &lt;- residuals(fit, ple4, ple4.indices, type=&#39;deviances&#39;) plot(d_r) Figure 9.6: Raw residuals for abundance indices and catch numbers (catch.n). Each panel is coded by age class, dots represent standardized residuals and lines a simple smoother. 9.2 Predictive skill An important feature of stock assessment model fits is the capacity to predict, since one of the most important analysis done with these fits is forecasting future fishing opportunities under pre-defined conditions. The a4a framework implements a visualization of the fit’s predictive skill for both catch-at-age and abundance indices. These are generated by the method plot() with the fit object and a FLStock (Figure 9.7) or FLIndices (Figure ??) object as arguments. plot(fit, ple4) Figure 9.7: Predict and observed catch-at-age plot(fit, ple4.indices) Figure 9.8: Predict and observed abundance-at-age Figure 9.9: Predict and observed abundance-at-age Figure 9.10: Predict and observed abundance-at-age Figure 9.11: Predict and observed abundance-at-age Figure 9.12: Predict and observed abundance-at-age Figure 9.13: Predict and observed abundance-at-age 9.3 Fit summary, information and cross-validation metrics To get information about the likelihood fit the method fitSumm() can be used to report number of parameters (npar), negative log-likelkihood (nlogl), ADMB maximum gradient par (maxgrad), number of observations (nobs), generalized cross validation score (gcv), convergence flag (convergence) and acceptance rate (accrate) relevant for MCMC fits only. The second part refers to the likelihood value for each component. fitSumm(fit) ## iters ## 1 ## nopar 2.870000e+02 ## nlogl -3.823602e+02 ## maxgrad 2.060663e-04 ## nobs 1.728000e+03 ## gcv 1.185680e-01 ## convergence 0.000000e+00 ## accrate NA ## nlogl_comp1 -1.058450e+03 ## nlogl_comp2 6.695020e+01 ## nlogl_comp3 5.643150e+01 ## nlogl_comp4 4.025550e+02 ## nlogl_comp5 5.836470e+01 ## nlogl_comp6 6.057810e+01 ## nlogl_comp7 3.121530e+01 Information criteria based metrics are reported with the methods: AIC(fit) ## [1] -190.7205 BIC(fit) ## [1] 1374.784 9.4 The package a4adiags The package a4adiags contains some additional diagnostics based on the reference. Runs test checks weather the residuals are randomly distributed. A “run” is a sequence of the same sign residuals. Few runs indicate a trend or a correlation in the residuals while too many runs may suggest overfitting. The primary output of a runstest is a p-value where: a high p value \\((p\\leq 0.05)\\) suggests that the residuals are randomly distributed, a low p value indicates a non-random pattern in the residuals. library(a4adiags) ## ## Attaching package: &#39;a4adiags&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## fit theme_set(theme_bw()) fit &lt;- sca(mut09, mut09.idx, fmod = ~factor(age) + s(year, k = 8)) res &lt;- residuals(fit, mut09, mut09.idx) plotRunstest(fit, mut09.idx, combine = F) + theme_bw() + facet_wrap(~age) Figure 9.14: Runstest for the abundance index plotRunstest(catch.n(mut09), catch.n(mut09 + fit), combine = F) + theme_bw() + facet_wrap(~age) Figure 9.15: Runstest for the catch by age Green shading indicates no evidence \\((p &lt; 0.05)\\) and red shading evidence \\((p &gt;0.05)\\) to reject the hypothesis of a randomly distributed time-series of residuals, respectively. The shaded (green/red) area spans three residual standard deviations to either side from zero, and the red points outside of the shading violate the ‘\\(3\\sigma\\) limit’ for that series. 9.5 Retrospective analysis A legacy analysis from back when stock assessments were done with VPAs (REF Sheppeard, …), not so relevant with statistical catch at age models, which do not backwarsd fit fishing mortality and abundance as those models did. Cadrin (2025) notes the circularity of arguments when using retrospective analysis to make decisions about the stock assessment fit. Nevertheless, most experts still relly on this analysis to make decisions about the fit. The most common statistic used to summarize the retrospective analysis is Mohn’s rho (REF). Retrospective analysis consists on removing the most recent year of data, refit the model and compare estimates of metrics, e.g. the estimate of fishing mortality in year y-1 by each fit. The rationale is that a stable well fitted model would have similar estimates in both fits. In the example below the fishing mortality model is kept for all fits, without updating the smoothness factor. fit0 &lt;- sca(ple4, ple4.indices) n &lt;- 5 nret &lt;- as.list(1:n) stks &lt;- FLStocks(lapply(nret, function(x){window(ple4, end=(range(ple4)[&quot;maxyear&quot;]-x))})) idxs &lt;- lapply(nret, function(x){window(ple4.indices, end=(range(ple4)[&quot;maxyear&quot;]-x))}) fits &lt;- scas(stks, idxs, fmodel=list(fmodel(fit0))) stks &lt;- stks + fits stks[[6]] &lt;- ple4 + simulate(fit0, 250) Note fmodel doesn’t change: ## $fit1 ## ~te(age, year, k = c(6, 30), bs = &quot;tp&quot;) + s(age, k = 6) ## &lt;environment: 0x5b9c18000420&gt; ## ## $fit2 ## ~te(age, year, k = c(6, 30), bs = &quot;tp&quot;) + s(age, k = 6) ## &lt;environment: 0x5b9c18000420&gt; ## ## $fit3 ## ~te(age, year, k = c(6, 30), bs = &quot;tp&quot;) + s(age, k = 6) ## &lt;environment: 0x5b9c18000420&gt; ## ## $fit4 ## ~te(age, year, k = c(6, 30), bs = &quot;tp&quot;) + s(age, k = 6) ## &lt;environment: 0x5b9c18000420&gt; ## ## $fit5 ## ~te(age, year, k = c(6, 30), bs = &quot;tp&quot;) + s(age, k = 6) ## &lt;environment: 0x5b9c18000420&gt; The retrospective plot shown below presents the current fit with uncertainty and each retrospective fit on top. If the retrospective fit is not whithin the confidence interval of the current fit the analyst can argue that the estiate is different and as such reflecting a “poor” fit. plot(window(stks, start=2005)) Figure 9.16: Retrospective analysis of the plaice in ICES area IV stock One could use specific submodels and pass them to the fitting function scas, including with some adjustments to take into account the data reduction. In the next example the fishing mortality model is set reducing the smoothness taking into account the length of the dataset. Not considering the adjustment of the model to the new dataset may result in comparisons across models which are very different due to the relationship between information contained in the data and the number of parameters in the model. This issue is more relevant for stocks with shorter time series. n &lt;- 5 nret &lt;- as.list(1:n) stks &lt;- FLStocks(lapply(nret, function(x){window(ple4, end=(range(ple4)[&quot;maxyear&quot;]-x))})) idxs &lt;- lapply(nret, function(x){window(ple4.indices, end=(range(ple4)[&quot;maxyear&quot;]-x))}) # each model will have smootheness scaled to length of time series fmod &lt;- lapply(stks, defaultFmod) fits &lt;- scas(stks, idxs, fmodel=fmod) stks &lt;- stks + fits stks[[6]] &lt;- ple4 + simulate(fit0, 250) Note fmodel changes: ## $fit1 ## ~te(age, year, k = c(6, 30), bs = &quot;tp&quot;) + s(age, k = 6) ## &lt;environment: 0x5b9c1b07a0e8&gt; ## ## $fit2 ## ~te(age, year, k = c(6, 29), bs = &quot;tp&quot;) + s(age, k = 6) ## &lt;environment: 0x5b9c1b1b3f40&gt; ## ## $fit3 ## ~te(age, year, k = c(6, 29), bs = &quot;tp&quot;) + s(age, k = 6) ## &lt;environment: 0x5b9c25cbc260&gt; ## ## $fit4 ## ~te(age, year, k = c(6, 28), bs = &quot;tp&quot;) + s(age, k = 6) ## &lt;environment: 0x5b9c2664da80&gt; ## ## $fit5 ## ~te(age, year, k = c(6, 28), bs = &quot;tp&quot;) + s(age, k = 6) ## &lt;environment: 0x5b9c28ee4478&gt; And the retrospective plot plot(window(stks, start=2005)) Figure 9.17: Retrospective analysis of the plaice in ICES area IV stock 9.6 Hindcast A hindcast is a method used in modeling and simulation where historical data is used to test and validate predictive models. In a hindcast, known outcomes from the past are compared with the model’s predictions to assess the model’s accuracy and performance. The primary goal of hindcasting is to improve the reliability and accuracy of predictive models by identifying discrepancies between predicted and actual outcomes and adjusting model parameters accordingly (Mason and Mimmack 2002). The term retroactive forecasting is used by Mason and Mimmack (2002) to denote the form of hindcasting in which forecasts are made for past years (e.g. 2006–2010) using data prior to those years (perhaps 1970–2005). The terminology ex post is used in business forecasting, referring to predictions for historical periods for which verification data are already available at the time of forecast. For this exercise we’ll use the package a4adiags hindcast method, which follows the suggestions by Carvalho et al. (2021) and Laurence T. Kell, Kimoto, and Kitakado (2016). The hindacast is carried out by sequentially removing the most recent year in the data, similar to a retrospective analysis, refit the stock assessment model and project one year ahead. The Mean Absolute Scale Error (MASE) (Carvalho et al. 2021) is used to assess the predictive skill, a score higher than 1 indicates that the model forecasts have less skill than a random walk. nyears &lt;- 5 # set number of year for average biology and selectivity nsq &lt;- 3 hc &lt;- a4ahcxval(ple4, ple4.indices, nyears = nyears, nsq = nsq) Figure 9.18 depicts the hincast results for the abudance indices used in the assessment. The MASE value is included in the strip above the plot. In this case one can see that 3 out of the 5 surveys are not better predictors than a random walk. Figure 9.18: Survey predictions of year ahead indices in hindcast process. The MASE is presented in the strip about the index and is related to the predictive skill of the index. References "],["predict-and-simulate.html", "10 Predict and simulate 10.1 predict 10.2 simulate 10.3 simulate and predict 10.4 Details", " 10 Predict and simulate To predict and simulate R uses the methods predict() and simulate(), which are implemented in FLa4a, adapted to work with several of the model classes. The simulate and predict methods only work if the mode object has be run with the option fit = \"assessment\", and not fit = \"MP\", the later is used when space and time are at a premium, for example in a Management Strategy Evaluation. In this chapter we will only cover prediction and simulation of full stock objects. This chapter is based on the following model: fit0 &lt;- sca(ple4, ple4.index) submodels(fit0) ## fmodel: ~te(age, year, k = c(6, 30), bs = &quot;tp&quot;) + s(age, k = 6) ## srmodel: ~factor(year) ## n1model: ~s(age, k = 3) ## qmodel: ## IND: ~s(age, k = 6) ## vmodel: ## catch: ~s(age, k = 3) ## IND: ~1 Simulation and prediction in FLa4a is based on two functions: simulate and predict. 10.1 predict Predict simply computes the quantities of interest using the estimated coefficients and the design matrix of the model, defined via the formulas in the submodels. In the case of predicting from a fitted model, a list is returned, with one element for each submodel, and each element of the list is a FLQuants object with the following FLQuants fit.pred &lt;- predict(fit0) lapply(fit.pred, names) ## $stkmodel ## [1] &quot;harvest&quot; &quot;rec&quot; &quot;ny1&quot; ## ## $qmodel ## [1] &quot;IND&quot; ## ## $vmodel ## [1] &quot;catch&quot; &quot;IND&quot; This allows easy access to the parameterised parts of the model, for example the initial population structure, ny1, can be accessed via fit.pred$stkmodel$ny1. If the whole stock is of interest, for example, to inspect model predictions of SSB, then the following code can be used stk.pred &lt;- ple4 + fit0 The + operator is used to add the predictions from the model to the initial stock object. Then quantities can be extracted as normal, ssb(stk.pred). 10.2 simulate Simulations are always done by generating random draws from a multivariate normal distribution with mean given by the coeffients of the model, and variance matrix given by the estimated covariance matrix of the coefficients (in practice this is a submatrix of the inverse of the hessian matrix, see details below). simulate performed on a fitted model returns a fitted model with the same structure as the original model, but with the model parameters replaced by nsim simulated parameters. The following code simulates the model 200 times, and a histogram of the result show in Figure @ref(fig:predsim_hist). sim.fit &lt;- simulate(fit0, nsim = 200) hist( exp(coef(sim.fit)$vmodel$IND), main = &quot;200 draws of a model parameter&quot;, nclass = 20, xlab = &quot;observation std error for survey index&quot; ) (#fig:predsim_hist)Histogram of 200 draws from the approximate distribution of the estimate of survey observation error. The fitted model can also be used to generate some predictions, such as harvest, stock.n or catch.n. Figure 10.1 shows a comparison between the estimated values and the medians of the simulation, while Figure 10.2 presents the stock summary of the simulated and fitted data. fits &lt;- simulate(fit0, 100) flqs &lt;- FLQuants(sim=iterMedians(stock.n(fits)), det=stock.n(fit0)) xyplot(data~year|age, groups=qname, data=flqs, type=&quot;l&quot;, scales=list(y=list(relation=&quot;free&quot;, draw=FALSE)), auto.key=list(points=FALSE, lines=TRUE, columns=2), par.settings=list(superpose.line=list(col=c(&quot;gray35&quot;, &quot;black&quot;)), strip.background=list(col=&quot;gray90&quot;)), ylab=&quot;&quot;) Figure 10.1: Median simulations VS fit stks &lt;- ple4 + fits plot(stks) Figure 10.2: Stock summary of the simulated and fitted data 10.3 simulate and predict The simulate() and predict() functions can be combined to simulate and then predict from each of the simulations. The following code simulates the model 100 times and then predicts the fitted quantitites from the model, resulting in simulations from the core model elements: initial age structure, recruitment, fishing mortality, survey catchability and observation variance. A plot of the initial age structure is shown as an example in Figure @ref(fig:sim_ny1). sim.pred &lt;- predict(sim.fit) (#fig:sim_ny1)Simulations from the model prediction of initial age structure If interest is in the complete stock, then the + operator can be used to add the simulated quantities to the initial stock object, as shown below. The stock information can be extracted, such as SSB, a plot of which is shown in Figure @ref(fig:sim_ssb). stk.sim &lt;- ple4 + sim.fit (#fig:sim_ssb)Simulations from the model prediction of initial age structure 10.4 Details Simulate uses the variance-covariance matrix computed from the Hessian returned by ADMB and the fitted parameters, to parametrize a multivariate normal distribution. The simulations are carried out using the method mvrnorm() provided by the R package MASS. Such an approach is often called a parametric bootstrap, and is a common method for generating uncertainty in the parameters of a model. It is imporatant to note that the method approximates the joint distribution of the model parameters as a multivariate normal, which is true if the model is linear and well specified. Since stock assessment models tend to be non-linear, a more robust approach is to use MCMC methods instread. Another point to note, is that in order to save space, only the subset of the covariance matrix is stored for each submodel. This in effect sets any correlations between submodel parameters to zero in simulations. "],["the-statistical-catch-at-age-stock-assessment-framework-with-markov-chain-monte-carlo-mcmc.html", "11 The statistical catch-at-age stock assessment framework with Markov Chain Monte Carlo (MCMC) 11.1 The MCMC method for sca 11.2 Diagnostics with CODA 11.3 ADMB’s arguments to tune the MCMC algorithm", " 11 The statistical catch-at-age stock assessment framework with Markov Chain Monte Carlo (MCMC) The previous methods were demonstrated using maximum likelihood estimation (MLE). However, ADMB also supports Markov Chain Monte Carlo (MCMC) methods, which provide significant advantages, particularly when working with complex models that involve many parameters. The key difference is that while MLE finds a single best estimate of parameters by maximizing the likelihood function, MCMC offers a broader perspective by generating an entire distribution of possible values. This approach is more informative because it does not just give the most likely estimate but also helps us understand the uncertainty surrounding it. With MCMC, researchers can incorporate prior knowledge and obtain results that are often more realistic and reliable (A. Gelman et al. 2013). This is especially useful when dealing with complicated models where traditional likelihood-based methods struggle, as MCMC allows for efficient exploration of possible solutions without requiring an exact mathematical formulation (Gilks, Richardson, and Spiegelhalter 1995; Robert and Casella 2005). One of the biggest advantages of MCMC is its flexibility when working with models that have irregular behavior, such as those with multiple peaks or abrupt changes in likelihood. Standard MLE methods assume that the likelihood function behaves smoothly, like a well-shaped bowl, but this is rarely true in real-world applications. In fisheries, ecology, and other applied sciences, models often have parameters that interact in complex ways, creating likelihood surfaces with ridges and multiple solutions. In these cases, MLE can easily get stuck in a local peak, failing to find the best possible estimate or underestimating the real uncertainty in the system (Neal 1993). Since MCMC uses a probabilistic sampling approach instead of strict optimization, it moves freely across the entire space of possible values, making it more robust and adaptable to challenging problems (Robert and Casella 2005). Traditional MLE-based uncertainty estimation relies on the Hessian matrix, which essentially measures how quickly the likelihood function changes as parameters vary. This method assumes that the shape of the likelihood function is roughly the same everywhere—meaning that a quadratic (bowl-like) approximation is valid (Pawitan 2001). However, this assumption is often unrealistic, especially in models with many parameters or correlations between them, as is common in fisheries stock assessment models. Furthermore, MLE uncertainty estimates require a large sample size for them to be accurate, which is not always available in real-world applications (Wasserman 2004; Vaart 1998). If these assumptions do not hold, MLE can give misleading confidence intervals, making decision-making riskier. Additionally, MLE assumes that the model is correctly specified—meaning that it accurately represents the real system being studied. If the model is misspecified or overly simplified, the Hessian-based uncertainty estimates may be highly unreliable, requiring alternative approaches like robust standard errors or resampling methods (White 1982). In fields like fisheries science, where models often involve multiple correlated parameters, MCMC provides a much more flexible and realistic way to estimate uncertainty. Unlike MLE, which assumes uncertainty follows a simple symmetrical pattern, MCMC can handle more complex distributions, giving a better representation of real-world variability. This is especially important when estimating key fisheries management indicators, such as spawning stock biomass (\\(SSB\\)) or fishing mortality (\\(F\\)), which influence critical policy decisions. Because MCMC does not impose strict mathematical assumptions about the shape of uncertainty, it produces estimates that are more reflective of real-world conditions, ultimately leading to more informed and reliable management strategies. ADMB’s approach to Markov Chain Monte Carlo (MCMC) enhances Bayesian analysis by efficiently exploring parameter uncertainty in complex models. Unlike standard MCMC tools, ADMB leverages automatic differentiation to improve sampling efficiency and speed (Fournier et al. 2012). It supports various sampling algorithms, including Metropolis-Hastings and Hamiltonian Monte Carlo, which help navigate high-dimensional parameter spaces and complex likelihood structures more effectively. This makes ADMB particularly useful in applied sciences like fisheries and ecology, where uncertainty estimation is crucial for decision-making. Additionally, ADMB provides built-in diagnostics to assess MCMC convergence and reliability, ensuring that posterior distributions are well-explored and results are robust (A. Gelman et al. 2013). To evaluate the quality of MCMC sampling, ADMB offers several key diagnostics. Autocorrelation analysis detects dependencies between successive samples, while the effective sample size (ESS) measures the number of independent samples in the chain. The Gelman-Rubin diagnostic (\\(\\hat{R}\\)) helps assess whether multiple chains have converged to the same distribution, with values close to 1 indicating good convergence. Trace plots visually inspect parameter behavior over iterations, revealing trends or poor mixing. Additionally, ADMB monitors the acceptance rate to ensure efficient sampling and provides posterior density estimates to check if the distribution has been properly explored. These tools help users refine their MCMC runs, adjusting sampling length or proposal distributions to improve performance and ensure reliable uncertainty estimates. The manual “A Guide for Bayesian Analysis in AD Model Builder” by Cole C. Monnahan, Melissa L. Muradian and Peter T. Kuriyam describe and explain a larger group of arguments that can be set when running MCMC with ADMB, which the a4a uses. 11.1 The MCMC method for sca This section shows how the sca methods interface with ADMB to use the MCMC fits. For this section we’ll use the hake assessment in mediterranean areas (gsa) 1, 5, 6 and 7. We’ll start buy fitting the MLE model and afterwards call the MCMC methods. The outcomes of the MCMC fit need to be inspected to make sure the chain converged and the results are robust. A set of diagnostics are available to do this work. [TO CHECK] For many Bayesian software platforms, the MCMC algorithms are started at user-specified or arbitrary places. ADMB has the advantage that it can robustly estimate the posterior mode and the covariance at that point. This information is very valuable in initializing the MCMC chain. Specifically, an MCMC chain starts from the posterior mode and uses the estimated covariance matrix in its proposed jumps (see the algorithm sections below). As such, ADMB chains typically do not need a long period to reach areas of high density. However, we caution the user to always check the MCMC output as other issues may lead to a chain that needs a longer burn-in. (Monnahan et al. 2019) # load libraries and data library(FLa4a) library(ggplotFL) data(hke1567) data(hke1567.idx) nsim &lt;- 250 # MLE estimate fmod &lt;- ~s(age, k = 4) + s(year, k = 8) + s(year, k = 8, by = as.numeric(age == 0)) + s(year, k = 8, by = as.numeric(age == 4)) qmod &lt;- list(~I(1/(1 + exp(-age)))) fit &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod) fit &lt;- simulate(fit, nsim) To run the MCMC method, one needs to configure a set of arguments, which is done by creating a SCAMCMC object. Table 11.1 describes the arguments available to run the MCMC method, extracted from Monnahan (Monnahan et al. 2019). For more details on the MCMC configuration in ADMB visit the ADMB website. Table 11.1: ADMB MCMC arguments Argument Description mcmc N Run N MCMC iterations mcsave N Save every N th MCMC iteration mcscale N Rescale step size for first N iterations mcmult N Rescale the covariance matrix mcrb N Reduce high parameter correlations mcprobe X Use a fat-tailed proposal distribution mcdiag Use a diagonal covariance matrix mcnoscale Do not scale the algorithm during mcu Use a uniform distribution as proposal distribution hybrid Use the hybrid method hynstep N Mean number of steps for the leapfrog method hyeps X The stepsize for the leapfrog method [X numeric and &gt; 0] # mcmc mc &lt;- SCAMCMC() # check the default pars mc ## An object of class &quot;SCAMCMC&quot; ## Slot &quot;mcmc&quot;: ## [1] 10000 ## ## Slot &quot;mcsave&quot;: ## [1] 100 ## ## Slot &quot;mcscale&quot;: ## [1] NaN ## ## Slot &quot;mcmult&quot;: ## [1] NaN ## ## Slot &quot;mcrb&quot;: ## [1] NaN ## ## Slot &quot;mcprobe&quot;: ## [1] NaN ## ## Slot &quot;mcseed&quot;: ## [1] NaN ## ## Slot &quot;mcdiag&quot;: ## [1] FALSE ## ## Slot &quot;mcnoscale&quot;: ## [1] FALSE ## ## Slot &quot;mcu&quot;: ## [1] FALSE ## ## Slot &quot;hybrid&quot;: ## [1] FALSE ## ## Slot &quot;hynstep&quot;: ## [1] NaN ## ## Slot &quot;hyeps&quot;: ## [1] NaN Defaults for now are ok, so lets fit the model. Note that the argument fit must be set to MCMC and the argument mcmc takes the SCAMCMC object. # fit the model fitmc00 &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) # check acceptance rate fitSumm(fitmc00) ## iters ## 1 ## nopar 52.0000 ## nlogl NA ## maxgrad NA ## nobs 176.0000 ## gcv NA ## convergence NA ## accrate 0.3271 As usual fitSumm store relevant information about the model fit. In the case of MCMC fit the information stored is the number of model paramters (nopar), the number of observations (nobs) and the acceptance rate (accrate). plot(FLStocks(mle=hke1567 + fit, mc=hke1567 + fitmc00)) Figure 11.1: Stock assessment summaries of maximum likelihood (mle) and monte carlo (mc) fits. 11.2 Diagnostics with CODA In essence, the diagnostics are used to give the analyst confidence that the posterior distribution of the parameters is unbiased, as much as possible with symetric non correlated distributions of each parameter, over which one can make inference. There’s a large body of literature about MCMC convergence. In this section we’ll focus on the out-of-the-box methods for metropolis hastings algorithm available to the stock assessment scientist: trace plots, autocorrelation and cross correlation analysis, geweke diagnostic, Gelman and Rubin’s convergence diagnostic, acceptance rate, cumulative means, distribution density and acceptance rate. These tools should be used together to evaluate proper mixing and convergence. ADMB has an hybrid algorithm based on Hamiltonian dynamic which will not be addressed here. The reader is invited to consult Monnahan et al. (2019) for more information. We use the package CODA to run the diagnostics on MCMC fits. One needs to convert the sca output into a mcmc CODA object over which several diagnostics can be ran. The mcmc object is a matrix with the parameters (row = iters, cols= pars). library(coda) ## ## Attaching package: &#39;coda&#39; ## The following object is masked from &#39;package:FLa4a&#39;: ## ## as.mcmc For demonstration purposes we’ll create a chain with 1000 samples (mcmc=10000) and save every 10 iters (mcsave=10), which will create a highly correlated and unstable chain, and update the initial MCMC fit to also have 1000 samples (mcmc=100000, mcsave=100). The latter will have lower correlation due to the higher thinning. # update initial fit, control random seed mc &lt;- SCAMCMC(mcmc=100000, mcsave=100, mcseed=10) fitmc01 &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) fitmc01.mc &lt;- FLa4a::as.mcmc(fitmc01) # highly correlated fit, control random seed mc &lt;- SCAMCMC(mcmc=10000, mcsave=10, mcseed=10) fitmc02 &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) fitmc02.mc &lt;- FLa4a::as.mcmc(fitmc02) 11.2.1 Traceplots Trace plots show the sampled values of a parameter over iterations. A plot that looks like a random, stable “cloud” of points with no trends or drifts, with rapid fluctuactions, is a signal of convergence, meaning the chain mixes well and is stationary. If the trace plot shows a strong trend or periodicity, drifts, or long autocorrelated stretches, it means the chain hasn’t converged. Figure 11.2 cleary depicts this difference between the two runs. traceplot(mcmc.list(mc01=fitmc01.mc[,1], mc02=fitmc02.mc[,2]), lwd=2, col=c(2,4), lty=1) Figure 11.2: MCMC chain’s trace for the first parameter. High correlation chain in blue, low correlation chain in red. Ploting the chains for the parameter clearly shows autocorrelation for the first parameter in the blue chain. It also shows an initial phase where the chain seems to be stuck in a single value. This initial phase, when the parameter seems to be stuck in a fixed position, is called the “burn-in” phase. These iterations can be dropped with the burnin method (Figure 11.3), although it doesn’t sort out the autocorrelation or the parameter density. traceplot(FLa4a::as.mcmc(burnin(fitmc02, 250))[,1], lwd=2, col=4, lty=1) Figure 11.3: MCMC chain with high autocorrelation after removing the initial 250 samples (burnin period). 11.2.2 Autocorrelation and crosscorrelation analysis Autocorrelation analysis is useful to assess stationarity, a stationary chain should have low autocorrelation, meaning that each sample is approximately independent. On the opposite, high autocorrelation indicates slow mixing and possible non-stationarity. furthermore, in a good mixed chain autocorrelation drops quickly to near zero, while a poor mixing will display high autocorrelation, meaning successive samples are too correlated, reducing efficiency. The autocorrelation plot produced by the acf function, will show correlation along the chain for each parameter at different lags. Figure 11.4 shows there’s a strong autocorrelation for the first parameter in the blue chain (right panel), which we’d like to avoid. acfplot(fitmc01.mc[,1], lwd=3, main=&quot;Low correlation chain&quot;) acfplot(fitmc02.mc[,1], lwd=3, main=&quot;High correlation chain&quot;) Figure 11.4: Autocorrelation plot of the first parameter in the MCMC chain Crosscorrelation inspects the pairwise correlation of all parameters, which is a useful tool to assess the efficiency of the sampling process and the independence of the generated samples. If cross-correlations are high, it often means that transitions between states are sluggish, leading to an increased autocorrelation time and requiring a larger number of samples to achieve effective independent samples. Conversely, low cross-correlation implies that parameters are explored more independently, leading to faster convergence and better mixing. crosscorr.plot(fitmc01.mc, main=&quot;Low correlation chain&quot;) crosscorr.plot(fitmc02.mc, main=&quot;High correlation chain&quot;) Figure 11.5: Crosscorrelation plots 11.2.3 Geweke diagnostic The geweke diagnostic computes the Geweke-Brooks Z-score (Geweke 1992), which indicates if the first and following parts of a sample from a Markov chain are drawn from the same distribution as the last part of the chain, usualy the last 50% of the samples. It’s useful to decide if the first few iterations should be discarded and provides information about the stability of the chain. Figure 11.6 shows the geweke plot for the MCMC run without thining and Figure 11.6 when the thining was set at 200 samples. geweke.plot(fitmc01.mc[,1], main=&quot;Low correlation chain&quot;) geweke.plot(fitmc02.mc[,1], main=&quot;High correlation chain&quot;) Figure 11.6: Geweke plot of the first parameter in the MCMC chains The panel on the left shows a much more regular chain, where the different blocks of data show similar distributions. The panel on the right clearly shows the z-score statistic out of the confidence intervals until 400 samples are discarded, which points to the need to drop a set of initial samples. The geweke diagnostic is also a good way to look at mixing by comparing the mean and variance of the first part of the chain to the last part. Good mixing will show no significant difference between early and late samples. Poor mixing will show large differences, indicating the chain has not explored the posterior fully. 11.2.4 Cumulative means Inspecting the cumulative mean along the chain is another good way to check for the stability of the chain. When the mixing is good the mean stabilizes quickly, and vice-versa if not. cm01 &lt;- fitmc01.mc[,1] cm01 &lt;- cumsum(cm01) / seq_along(cm01) cm02 &lt;- fitmc02.mc[,1] cm02 &lt;- cumsum(cm02) / seq_along(cm02) plot(cm01, type=&quot;l&quot;, xlab=&quot;samples&quot;, ylab=&quot;mean&quot;, main=&quot;Low correlation chain&quot;) plot(cm02, type=&quot;l&quot;, xlab=&quot;samples&quot;, ylab=&quot;mean&quot;, main=&quot;High correlation chain&quot;) Figure 11.7: Cumulative mean plots of the first parameter in the MCMC chains 11.2.5 Distribution density An important element of MCMC is to produce symetric posterior distributions, for one it’s a sign that the chain explored the space of the parameter, for other it makes inference about the parameters a lot more robust. If the distributions are skewed or multimodal, estimating the expected value and variance becomes a lot more complicated. As such having symetric distributions is preferred and should be checked before computing statistics of interest. Figure 11.8 shows the density plots for both runs, where it shows the symetric distribution of the uncorrelated chain (left panel) and the bimodal distribution of the correlated chain. densplot(fitmc01.mc[,1], main=&quot;Low correlation chain&quot;) densplot(fitmc02.mc[,1], main=&quot;High correlation chain&quot;) Figure 11.8: Density plots of the first parameter in the MCMC chains 11.2.6 Gelman-Rubin statistic The Gelman-Rubin statistic (\\(\\hat{R}\\)) (Andrew Gelman and Rubin 1992) can be used to check if multiple chains have reached a stable state and are properly exploring the target distribution. It compares how much variation exists within each chain to the variation between different chains. If all chains are sampling from the same distribution, these variations should be similar, and \\(\\hat{R}\\) will be close to 1, otherwise, if it’s greater than 1.1 it suggests that the chains have not yet converged. To compute \\(\\hat{R}\\), multiple chains are run with different starting points. The algorithm measures how spread out the samples are within each chain and compares it to how much the chains differ from each other. If the chains have not mixed well, they will appear too different from each other, and \\(\\hat{R}\\) will be large. If the chains have mixed properly, they will have a similar spread, and the statistic will be close to 1. To run another chain one makes use of the mcseed argument to make sure the 2 chains start from different places. The Gelman-Rubin statistics is computed by the gelman.diag method and depicted by the gelman.plot function. Ist’s easy to see the difference between the two fits. While the low corrrelation fit shows values close to 1 for most parameters, the high correlation fit shows a number of large values. # low correlation mc &lt;- SCAMCMC(mcmc=100000, mcsave=100, mcseed=30) fitmc01b &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) fitmc01b.mc &lt;- FLa4a::as.mcmc(fitmc01b) # highly correlated fit mc &lt;- SCAMCMC(mcmc=10000, mcsave=10, mcseed=30) fitmc02b &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) fitmc02b.mc &lt;- FLa4a::as.mcmc(fitmc02b) # create lists for comparison mclst01 &lt;- mcmc.list(a=fitmc01.mc, b=fitmc01b.mc) mclst02 &lt;- mcmc.list(a=fitmc02.mc, b=fitmc02b.mc) gelman.diag(mclst01) ## Potential scale reduction factors: ## ## Point est. Upper C.I. ## fMod:(Intercept) 1.001 1.001 ## fMod:s(age).1 1.024 1.112 ## fMod:s(age).2 1.002 1.005 ## fMod:s(age).3 1.021 1.099 ## fMod:s(year).1 1.001 1.011 ## fMod:s(year).2 1.004 1.020 ## fMod:s(year).3 1.011 1.051 ## fMod:s(year).4 1.008 1.040 ## fMod:s(year).5 1.002 1.013 ## fMod:s(year).6 1.004 1.024 ## fMod:s(year).7 1.001 1.007 ## fMod:s(year):by1..1 0.999 1.000 ## fMod:s(year):by1..2 1.003 1.011 ## fMod:s(year):by1..3 1.001 1.005 ## fMod:s(year):by1..4 1.003 1.013 ## fMod:s(year):by1..5 1.005 1.006 ## fMod:s(year):by1..6 1.018 1.088 ## fMod:s(year):by1..7 0.999 0.999 ## fMod:s(year):by1..8 0.999 1.000 ## fMod:s(year):by2..1 1.008 1.039 ## fMod:s(year):by2..2 0.999 1.001 ## fMod:s(year):by2..3 1.008 1.040 ## fMod:s(year):by2..4 1.000 1.004 ## fMod:s(year):by2..5 1.003 1.018 ## fMod:s(year):by2..6 1.003 1.003 ## fMod:s(year):by2..7 1.007 1.037 ## fMod:s(year):by2..8 1.000 1.001 ## n1Mod:(Intercept) 1.002 1.013 ## n1Mod:s(age).1 1.000 1.003 ## n1Mod:s(age).2 1.001 1.006 ## rMod:(Intercept) 1.000 1.000 ## rMod:factor(year)1 0.999 0.999 ## rMod:factor(year)2 1.000 1.001 ## rMod:factor(year)3 1.002 1.002 ## rMod:factor(year)4 1.002 1.005 ## rMod:factor(year)5 1.000 1.000 ## rMod:factor(year)6 1.005 1.008 ## rMod:factor(year)7 0.999 0.999 ## rMod:factor(year)8 1.010 1.045 ## rMod:factor(year)9 1.001 1.003 ## rMod:factor(year)10 1.000 1.004 ## rMod:factor(year)11 1.001 1.008 ## rMod:factor(year)12 1.011 1.053 ## rMod:factor(year)13 1.003 1.003 ## rMod:factor(year)14 1.004 1.019 ## rMod:factor(year)15 1.008 1.034 ## qMod:IND:(Intercept) 1.003 1.017 ## qMod:IND:I(1/(1 + exp(-age))) 1.004 1.017 ## vMod:catch:(Intercept) 0.999 0.999 ## vMod:catch:s(age).1 1.002 1.013 ## vMod:catch:s(age).2 1.001 1.010 ## vMod:IND:(Intercept) 1.000 1.000 ## ## Multivariate psrf ## ## 1.11 gelman.diag(mclst02) ## Potential scale reduction factors: ## ## Point est. Upper C.I. ## fMod:(Intercept) 1.067 1.25 ## fMod:s(age).1 1.014 1.07 ## fMod:s(age).2 1.047 1.15 ## fMod:s(age).3 1.009 1.05 ## fMod:s(year).1 1.041 1.06 ## fMod:s(year).2 1.118 1.43 ## fMod:s(year).3 1.001 1.00 ## fMod:s(year).4 1.111 1.40 ## fMod:s(year).5 1.012 1.03 ## fMod:s(year).6 1.121 1.44 ## fMod:s(year).7 1.067 1.23 ## fMod:s(year):by1..1 1.022 1.02 ## fMod:s(year):by1..2 1.070 1.25 ## fMod:s(year):by1..3 1.016 1.04 ## fMod:s(year):by1..4 1.055 1.23 ## fMod:s(year):by1..5 1.080 1.21 ## fMod:s(year):by1..6 1.038 1.17 ## fMod:s(year):by1..7 1.048 1.10 ## fMod:s(year):by1..8 1.076 1.30 ## fMod:s(year):by2..1 1.122 1.44 ## fMod:s(year):by2..2 1.044 1.06 ## fMod:s(year):by2..3 1.077 1.26 ## fMod:s(year):by2..4 1.027 1.07 ## fMod:s(year):by2..5 1.093 1.35 ## fMod:s(year):by2..6 1.034 1.13 ## fMod:s(year):by2..7 1.105 1.38 ## fMod:s(year):by2..8 1.044 1.07 ## n1Mod:(Intercept) 1.007 1.01 ## n1Mod:s(age).1 1.007 1.01 ## n1Mod:s(age).2 1.005 1.03 ## rMod:(Intercept) 1.010 1.03 ## rMod:factor(year)1 1.019 1.03 ## rMod:factor(year)2 1.027 1.12 ## rMod:factor(year)3 1.025 1.10 ## rMod:factor(year)4 1.047 1.18 ## rMod:factor(year)5 1.086 1.27 ## rMod:factor(year)6 1.000 1.00 ## rMod:factor(year)7 1.027 1.07 ## rMod:factor(year)8 1.049 1.20 ## rMod:factor(year)9 1.088 1.30 ## rMod:factor(year)10 1.042 1.18 ## rMod:factor(year)11 1.011 1.04 ## rMod:factor(year)12 1.002 1.01 ## rMod:factor(year)13 1.004 1.02 ## rMod:factor(year)14 1.002 1.01 ## rMod:factor(year)15 1.002 1.00 ## qMod:IND:(Intercept) 1.004 1.02 ## qMod:IND:I(1/(1 + exp(-age))) 0.999 1.00 ## vMod:catch:(Intercept) 0.999 1.00 ## vMod:catch:s(age).1 1.007 1.01 ## vMod:catch:s(age).2 1.056 1.18 ## vMod:IND:(Intercept) 1.002 1.01 ## ## Multivariate psrf ## ## 1.98 mclst01 &lt;- mcmc.list(a=fitmc01.mc[,1], b=fitmc01b.mc[,1]) mclst02 &lt;- mcmc.list(a=fitmc02.mc[,1], b=fitmc02b.mc[,1]) gelman.plot(mclst01, main=&quot;Low correlation chain&quot;) gelman.plot(mclst02, main=&quot;High correlation chain&quot;) Figure 11.9: Gelman-Rubin’s diagnostic plots for the first parameter. 11.2.7 Acceptance rate The acceptance rate in Markov Chain Monte Carlo (MCMC) methods plays a crucial role in balancing exploration and efficiency when sampling from a posterior distribution. It represents the proportion of proposed states that are accepted in the Markov chain and directly influences mixing, convergence, and the quality of inference. A low acceptance rate (e.g., &lt;20%) means that most proposed moves are rejected, leading to slow exploration of the posterior distribution. This can result in poor mixing and high autocorrelation between samples (A. Gelman et al. 2013). A high acceptance rate (e.g., &gt;80%) suggests that the proposals are too conservative, leading to small moves and highly correlated samples. Monnahan et al. (2019) suggests that the optimal acceptance rate varies by model size, among other things, but is roughly 40%, although models with more parameters should have a lower optimal acceptance rate. Roberts, Gelman, &amp; Gilks, 1997 complementary suggest that for Random Walk Metropolis-Hastings (RWMH) in high-dimensional spaces an optimal acceptance rate is about 23%. The acceptance rate is reported out of the MCMC fit and can be accessed with the method fitSumm. Inspecting the acceptance rate for the models we’re using shows a higher acceptance rate for the high correlation model, although both are above the recommended optimal for high dimentional models, like the models used in stock assessment. cbind(fitSumm(fitmc01), fitSumm(fitmc02)) ## 1 1 ## nopar 52.00000 52.0000 ## nlogl NA NA ## maxgrad NA NA ## nobs 176.00000 176.0000 ## gcv NA NA ## convergence NA NA ## accrate 0.31914 0.3254 11.3 ADMB’s arguments to tune the MCMC algorithm This section is based on Monnahan et al. (2019) and describes a set of arguments and methods which the stock assessment analyst can use to tune the MCMC algorythm and be more confident on its convergence and follow up inference. 11.3.1 Thinning rate For the Metropolis-Hastings algorithm, the most important tuning option available to the user is the saving rate (the inverse of the thinning rate). This is the rate at which parameters are saved, such that thinning is effectively discarding draws. This tuning option is critical since this algorithm generates autocorrelated parameters by design. The user controls the thinning rate by the argument mcsave. If N = 1 every single draw is saved (none are thinned out), which generates high autocorrelation, suggesting the need to thin more (save fewer). This is the case of the fitmc02 fit. In fitmc01 mcsave was increased to 100, by increasing the total samples by 100 and saving every 100th. This helps reduce the autocorrelation and produces independent draws from the posterior of interest. 11.3.2 mcscale and mcnoscale ADMB accomplishes this by “scaling” the covariance matrix up or down, depending on the current acceptance rate, during the first part of the chain. Scaling the covariance matrix down produces proposed sets closer to the current set, and vice versa for scaling up. By default, it scales during the first 500 iterations before thining, but the user can specify this with mcscale or turn off scaling with mcnoscale. ADMB rescales the covariance matrix every 200 iterations until the acceptance rate is between 0.15 and 0.4, or the scaling period is exceeded. Draws from this tuning phase should be discarded as part of the burn-in. The code below illustrates the effect in the acceptance rate of not scaling the hessian, the acceptance rate drops significantly, which means poor mixing and higher autocorrelation. # no scale mc &lt;- SCAMCMC(mcmc=100000, mcsave=100, mcseed=10, mcnoscale=TRUE) fitmc01ns &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) fitmc01ns.mc &lt;- FLa4a::as.mcmc(fitmc01ns) data.frame(hessian_scale=c(fitSumm(fitmc01)), hessian_noscale=c(fitSumm(fitmc01ns))) ## hessian_scale hessian_noscale ## 1 52.00000 52.00000 ## 2 NA NA ## 3 NA NA ## 4 176.00000 176.00000 ## 5 NA NA ## 6 NA NA ## 7 0.31914 0.08582 In the next analysis we show the aiutocorrelation statistics for lags of 0, 1, 5, 10 and 50. When the hessian is not scaled the autocorreltion of lag 1 increased from 0.33 to 0.43. data.frame(hessian_scale=c(autocorr.diag(fitmc01.mc[,1])), hessian_noscale=c(autocorr.diag(fitmc01ns.mc[,1]))) ## hessian_scale hessian_noscale ## 1 1.000000000 1.0000000000 ## 2 0.331021210 0.4283436263 ## 3 0.020797197 0.0006163336 ## 4 0.038728809 0.0067920000 ## 5 0.005681063 -0.0193736621 11.3.3 mcprobe For some models, there may be concern of being “stuck” in a local minimum and simply never proposing a value far enough away to escape it and find other regions of high density. Obviously this problem would present issues for maximum likelihood inference as well. ADMB has a built-in algorithm which modifies the default proposal distribution so it occasionally proposes very distant parameters (i.e. “probes”)5. The mcprobe X argument initiates this option. The modified proposal distribution is a mixture distribution of normal and Cauchy distributions. The argument X controls how the two distributions are mixed, with larger values being more Cauchy (fatter tails, larger jumps). The range of valid inputs is 0.00001 to 0.499, and if no value is supplied a default of 0.05 is used6 For some models, there may be concern of being “stuck” in a local minimum and simply never proposing a value far enough away to escape it and find other regions of high density. Obviously this problem would present issues for maximum likelihood inference as well. ADMB has a built-in algorithm which modifies the default proposal distribution so it occasionally proposes very distant parameters (i.e. “probes”). The mcprobe argument initiates this option. The modified proposal distribution is a mixture distribution of normal and Cauchy distributions. The argument X controls how the two distributions are mixed, with larger values being more Cauchy (fatter tails, larger jumps). The range of valid inputs is 0.00001 to 0.499, and if no value is supplied a default of 0.05 is used. # more Cauchy mc &lt;- SCAMCMC(mcmc=1000, mcsave=1, mcseed=10, mcprobe=0.45) fitmc02p &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) fitmc02p.mc &lt;- FLa4a::as.mcmc(fitmc02p) data.frame(probe_0.05=c(fitSumm(fitmc02)), probe_0.45=c(fitSumm(fitmc02p))) ## probe_0.05 probe_0.45 ## 1 52.0000 52.000 ## 2 NA NA ## 3 NA NA ## 4 176.0000 176.000 ## 5 NA NA ## 6 NA NA ## 7 0.3254 0.343 In the next analysis we show the aiutocorrelation statistics for lags of 0, 1, 5, 10 and 50. When the hessian is not scaled the autocorreltion of lag 1 increased from 0.33 to 0.37. data.frame(probe_0.05=c(autocorr.diag(fitmc02.mc[,1])), probe_0.45=c(autocorr.diag(fitmc02p.mc[,1]))) ## probe_0.05 probe_0.45 ## 1 1.00000000 1.0000000 ## 2 0.87352614 0.9771081 ## 3 0.54400301 0.8964793 ## 4 0.33676814 0.7867470 ## 5 0.05451792 0.3936894 11.3.4 mcrb The -mcrb N option (which stands for “rescaled bounded”) alters the covariance matrix used to propose new parameter sets in the Metropolis-Hastings algorithm. Its intended use is to create a more efficient MCMC sampler so the analyses run faster. This option reduces the estimated correlation between parameters. The value of N must be integer and between 1 and 9, inclusive, with lower values leading to a bigger reduction in correlation. The option will be most effective under circumstances where the correlation between parameters at the posterior mode is higher than other regions of the parameter space. In this case, the algorithm may make efficient proposals near the posterior mode, but inefficient proposals in other parts of the parameter space. By reducing the correlation using mcrb the proposal function may be more efficient on average across the entire parameter space and require less thinning (and hence run faster) If poor performance is suspected to be caused by correlations that are too high, the mcrb option provides a quick, convenient to try a reduced correlation matrix in the algorithm # reduce correlation mc &lt;- SCAMCMC(mcmc=10000, mcsave=10, mcseed=10, mcrb=1) fitmc02r &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) fitmc02r.mc &lt;- FLa4a::as.mcmc(fitmc02r) data.frame(rslbound_no=c(fitSumm(fitmc02)), rslbound_high=c(fitSumm(fitmc02r))) ## rslbound_no rslbound_high ## 1 52.0000 52.0000 ## 2 NA NA ## 3 NA NA ## 4 176.0000 176.0000 ## 5 NA NA ## 6 NA NA ## 7 0.3254 0.2641 In the next analysis we show the autocorrelation statistics for lags of 0, 1, 5, 10 and 50. When the hessian is not scaled the autocorreltion of lag 1 increased from 0.33 to 0.37. data.frame(rslbound_no=c(autocorr.diag(fitmc02.mc[,1])), rslbound_high=c(autocorr.diag(fitmc02r.mc[,1]))) ## rslbound_no rslbound_high ## 1 1.00000000 1.0000000 ## 2 0.87352614 0.9907956 ## 3 0.54400301 0.9586125 ## 4 0.33676814 0.9169621 ## 5 0.05451792 0.6132896 References "],["projections-and-harvest-control-rules.html", "12 Projections and harvest control rules 12.1 Simple workflow 12.2 Initial condition assumptions 12.3 Scenarios 12.4 Relative scenarios 12.5 Limits 12.6 Harvest Control Rule", " 12 Projections and harvest control rules Massad et al. (2018) provide a useful distinction between forecasting and projection in scientific prediction, stating that prediction in general science can be categorized into two main components: forecasting and projection. According to their definition, “a forecast is an attempt to predict what will happen, whereas a projection describes what would happen, given certain hypotheses” (Massad et al., 2018). This distinction is also reflected in the Organisation for Economic Co-operation and Development (OECD 2009), which notes that “forecasting” and “prediction” are often used interchangeably in assessing the magnitude that a given variable will assume in the future. In the context of economic and environmental modeling, the OECD distinguishes between medium-term projections, which typically extend five to six years into the future, and other predictive exercises that rely on different methodologies. The term “projection” is generally used in two interrelated senses: (1) as a future value of a time series computed based on specific assumptions about environmental changes, and (2) in probability theory, as the conditional expectation of a variable. Since regression models provide conditional expectations of dependent variables based on predictor variables, this probabilistic use of projection aligns with forecasting and predictive modeling (OECD, 2009). Applying this conceptual framework to fisheries science, the process of advising on future fishing opportunities can be viewed as a combination of forecasting and projection. When using a harvest control rule (HCR) approved by decision-makers to predict catch levels that can be extracted from a stock, the analysis falls into the realm of forecasting, as it predicts what will happen under known system conditions. Conversely, when assessing potential future outcomes based on predefined scenarios—such as climate change effects or policy interventions—the approach aligns more closely with projections. In fisheries science, the distinction between forecasting and prediction is not always explicitly made, with the discipline generally emphasizing broader predictive approaches. It is common practice to run scenarios when advising fisheries managers, testing different assumptions about future ecological and management conditions. Strictly speaking, projections are not part of the stock assessment process itself. Stock assessment concludes when analysts compare estimates of biomass and fishing mortality with reference points, allowing for determinations of whether a stock is overfished or subject to overfishing (Hilborn and Walters 2013). Projections typically follow stock assessments, incorporating estimates or assumptions about population dynamics—growth, reproduction, and natural mortality—to predict future catches, biomass, and abundance under specific conditions and with quantified uncertainty. By integrating both forecasting and projection methodologies, fisheries science can offer robust, scenario-based decision-making tools that aid sustainable resource management (Punt, Smith, and Cui 2001). For this section we’ll be using the package FLasher [] from the FLR family of packages. library(FLa4a) library(FLasher) library(FLBRP) library(ggplotFL) data(ple4) data(ple4.indices) 12.1 Simple workflow The basic workflow to project with FLasher is to extend the FLStock object to store the predictions using the method fwdWindow, set the targets for the projection with method fwdControl and project the fishery with the method fwd with a FLStock and a FLSR. We’ll start by fitting a model including, a stock recruitment model which will be used to forecast recruitment. We’ll also set the number of iterations we’ll be working with and the time period we want to project. # fit model fit00 &lt;- sca(ple4, ple4.indices, srmodel=~geomean()) stk00 &lt;- ple4 + fit00 # create stock recruitment model object sr00 &lt;- as(fit00, &quot;FLSR&quot;) # set projection # number of iterations nsim &lt;- 250 # most recent year in the data maxy &lt;- range(ple4)[&quot;maxyear&quot;] # number of years to project projy &lt;- 5 # last year for projections endpy &lt;- maxy + projy # initial year for projections inipy &lt;- maxy + 1 # extend stock object to store projection&#39;s results stk00 &lt;- fwdWindow(stk00, end = endpy) # set the control for the projections, in this case a fixed f of 0.3 trg00 &lt;- fwdControl(year = inipy:endpy, quant = &quot;f&quot;, value = 0.3) # project stk01 &lt;- fwd(stk00, control=trg00, sr=sr00) Figure 12.1: Projection of stock for 5 years with fixed fishing mortality and recruitment A natural addition to this forecast is to add uncertainty. We’ll do that by generating uncertainty in population numbers, catch numbers and fishing mortality, using simulate, and add stock recruitment uncertainty using the residuals of the fit. stk00 &lt;- ple4 + simulate(fit00, nsim) stk00 &lt;- fwdWindow(stk00, end = endpy) res00 &lt;- residuals(sr00) rec00 &lt;- window(rec(stk00), 2018, 2022) rec00 &lt;- rlnorm(rec00, mean(res00), sqrt(var(res00))) stk02 &lt;- fwd(stk00, control=trg00, sr=sr00, residuals=rec00) Figure 12.2: Stochastic projection of stock for 5 years with fixed fishing mortality and recruitment An alternative to the above workflow is to fit the stock recruitment model after the stock assessment model, using the output of the assessment as input to the stock recruitment fit. In which case stock recruitment estimation uncertainty can be added by fitting the stock recruitment model over stock assessment uncertainty, so that there will be stock-recruitment fit to each iteration generated from the stock assessment model. fit00 &lt;- sca(ple4, ple4.indices) stk00 &lt;- ple4 + simulate(fit00, nsim) sr00 &lt;- as.FLSR(stk00, model=&quot;geomean&quot;) sr00 &lt;- fmle(sr00, control = list(trace = 0)) stk00 &lt;- fwdWindow(stk00, end = endpy) res00 &lt;- residuals(sr00) rec00 &lt;- window(rec(stk00), inipy, endpy) rec00 &lt;- rlnorm(rec00, c(yearMeans(res00)), sqrt(c(yearVars(res00)))) stk03 &lt;- fwd(stk00, control=trg00, sr=sr00, residuals=rec00) Figure 12.3: Stochastic projection of stock for 5 years with fixed fishing mortality and recruitment These two methods don’t give very different result when the stock recruitment model is not having a large impact in the other parameters. However the second method is much slower due to all the fits needed to have the empirical distribution of the stock recruitment model parameters. Figure 12.4: Stochastic projection of stock for 5 years with fixed fishing mortality and recruitment. 01: projection without uncertainty, stock recruitment model fit within the stock assessment model; 02: projection with uncertainty, stock recruitment model fit within the stock assessment model; 03: projection with uncertainty, stock recruitment model fit after the stock assessment model 12.2 Initial condition assumptions When projecting the stock forward one needs to make a number of assumptions about initial conditions, the starting point from where projections will be made. The method fwdWindow has a set of options that allows the analyst to decide about those assumptions: Argument Default value Description wt 3 Number of years to average over to get the future mean weights at age mat 3 Number of years to average over to get the future proportion mature at age m 3 Number of years to average over to get the future natural mortality at age spwn 3 Number of years to average over to get the future fraction of mortality before spawning discards.ratio 3 Number of years to average over to get the future mean proportion of discards at age catch.sel 3 Number of years to average over to get the future selection patern (fishing mortality at age which will be scaled based on canges in \\(\\bar{F}\\)) One can also define if those assumptions will be based on the mean value over the time period set, or randomly sampled from historical values, through setting the argument fun to mean or sample, respectively. For the next examples we’ll use the approach of fitting the stock recruitment within the assessment together with other parameters. We’ll set to 20 the number of years to compute mean weights at age, to 10 the number of years to average across and estimate the selection pattern in terms of fishing mortality at age. Finally, we’ll use a 10 year period to compute the average discard ratio. #fit00 &lt;- sca(ple4, ple4.indices, srmodel=~geomean()) #sr00 &lt;- as(fit00, &quot;FLSR&quot;) #stk00 &lt;- ple4 + fit00 #stk00 &lt;- fwdWindow(stk00, end = endpy, years = list(wt = 20, catch.sel = 10, discards.ratio = 10), fun = list(wt = &quot;sample&quot;)) #trg00 &lt;- fwdControl(year = inipy:endpy, quant = &quot;f&quot;, value = 0.3) #stk04 &lt;- fwd(stk00, control=trg00, sr=sr00) 12.3 Scenarios There’s a wide range of scenarios that can be of interest to project in order to give advice to policy makers, or to better understand the fitted stock assessment model. For example, projecting the stock in the absence of fishing for a few generations, gives good insights about the dynamics of the population being modelled. fit00 &lt;- sca(ple4, ple4.indices, srmodel=~geomean()) sr00 &lt;- as(fit00, &quot;FLSR&quot;) stk00 &lt;- ple4 + simulate(fit00, nsim) # set projection projy &lt;- 25 endpy &lt;- maxy + projy inipy &lt;- maxy + 1 stk00 &lt;- fwdWindow(stk00, end = endpy) trg00 &lt;- fwdControl(year = inipy:endpy, quant = &quot;f&quot;, value = 0) # recruitment uncertainty res00 &lt;- residuals(sr00) rec00 &lt;- window(rec(stk00), inipy, endpy) rec00 &lt;- rlnorm(rec00, mean(res00), sqrt(var(res00))) # project stk05 &lt;- fwd(stk00, control=trg00, sr=sr00, residuals=rec00) Figure 12.5: Stochastic projection of stock for 25 years in the absence of fishing These scenarios are defined by the target quantities one’s trying to achieve. In FLasher there are the following target quantities: srp ssb_end biomass_end ssb_spawn biomass_spawn ssb_flash biomass_flash inmb_end indb catch landings discards f fbar revenue effort When projecting the stock under the conditions defined by the scenario one can mix several quantities. For example it may be interesting to project an initial situation of growing the stock followed by a higher exploitation to evaluate how catches would behave. trg00 &lt;- fwdControl(year = inipy:endpy, quant = c(rep(&quot;ssb_end&quot;, 15), rep(&quot;f&quot;, 10)), value = c(rep(2000000, 15), rep(0.3, 10))) stk06 &lt;- fwd(stk00, control=trg00, sr=sr00, residuals=rec00) Figure 12.6: Stochastic projection of stock for 25 years with fixed SSB for 15 years followed by fixed fishing mortality for 10 years and constant recruitment 12.4 Relative scenarios Another scenario that is very useful when advising decision makers is to have objectives which are relative to previous preformances. For example one could increase spwanwing stock biomass by 10% each year. This is done buy using the argument relYear and setting value in relative terms, \\(1.1\\). fit00 &lt;- sca(ple4, ple4.indices, srmodel=~geomean()) sr00 &lt;- as(fit00, &quot;FLSR&quot;) stk00 &lt;- ple4 + simulate(fit00, nsim) # set projection projy &lt;- 5 endpy &lt;- maxy + projy inipy &lt;- maxy + 1 stk00 &lt;- fwdWindow(stk00, end = endpy) trg00 &lt;- fwdControl(year = inipy:endpy, quant = &quot;ssb_end&quot;, value = 1.1, relYear = inipy:endpy-1) # recruitment uncertainty res00 &lt;- residuals(sr00) rec00 &lt;- window(rec(stk00), inipy, endpy) rec00 &lt;- rlnorm(rec00, mean(res00), sqrt(var(res00))) # project stk07 &lt;- fwd(stk00, control=trg00, sr=sr00, residuals=rec00) Similar scenarios can be set for all quantities and any years to use as reference. The next example sets a scenario where \\(SSB\\) levels are set in relation to the most recent estimate out of the assessmeent. trg00 &lt;- fwdControl(year = inipy:endpy, quant = &quot;ssb_end&quot;, value = 1.1, relYear = maxy) stk08 &lt;- fwd(stk00, control=trg00, sr=sr00, residuals=rec00) Figure 12.7: Stochastic projection of stock for 25 years with fixed SSB for 15 years followed by fixed fishing mortality for 10 years and constant recruitment 12.5 Limits An important element when projecting the stock forward is to keep the performance of the fishery within some boundaries. A common one requested by the industry is to keep catches within some stability. fwd can include those constraints using the min and max arguments. The next example sets the minimum future catches to half of mean historical catches. minc &lt;- 0.2*mean(catch(stk00), na.rm=TRUE) trg00 &lt;- fwdControl(year = inipy:endpy, quant = rep(c(&quot;ssb_end&quot;, &quot;catch&quot;), projy), value = rep(c(1500000, NA), projy), min=rep(c(NA, minc), projy)) stk09 &lt;- fwd(stk00, control=trg00, sr=sr00, residuals=rec00) Figure 12.8: Stochastic projection of stock for 25 years with SSB target of 1500000t and catch limit of 50% historical catches 12.6 Harvest Control Rule Harvest Control Rules (HCR) can be complex and of many shapes (REF). We’ll keep our examples simple to demonstrate the mechanism of coding HCR. HCR are decision algorythms that can be used to codify the decision making process, allowing for longer term stability of management decisions in fisheries. The HCR we’re going to explore is based on a target and a limit. The target is applied to the management objective and represents the intent of management. The limit is applied to the process we want to use as trigger for protective actions. For example, the objective of the management system is to extract the highest catches possible for a very long time (aka equilibrium). However, due to natural variability and scientific uncertainty, it can happen that the stock’s biomass decreases below what’s expected, in which case decision makers want to make sure the stock stays healthy and productive. This situation can be translated into a target of fishing mortality at the level that extracts the Maximum Sustainable Yield, and a biomass limit of e.g. half the biomass that would produce the referred catches, everything being in a stable equilibrium. If SSB falls below the limit then fishing mortality is set at 80% of the target. Such HCR could be written as \\(if \\quad SSB_y &gt; 0.5xB_{MSY} \\quad then \\quad F_{y+1} = F_{MSY} \\quad or \\quad else \\quad F_{y+1} = 0.8xF_{MSY}\\) and coded like # fit model fit00 &lt;- sca(ple4, ple4.indices, srmodel=~geomean()) stk00 &lt;- ple4 + fit00 # create stock recruitment model object sr00 &lt;- as(fit00, &quot;FLSR&quot;) # estimate reference points brp00 &lt;- FLBRP(stk00, sr=sr00) brp00 &lt;- brp(brp00) ftrg &lt;- refpts(brp00)[&#39;msy&#39;,&#39;harvest&#39;] blim &lt;- 0.5*refpts(brp00)[&#39;msy&#39;,&#39;biomass&#39;] # set projection # most recent year in the data maxy &lt;- range(ple4)[&quot;maxyear&quot;] # number of years to project projy &lt;- 2 # last year for projections endpy &lt;- maxy + projy # initial year for projections inipy &lt;- maxy + 1 # extend stock object to store projection&#39;s results stk00 &lt;- fwdWindow(stk00, end = endpy) # set the controls for the projections trg00 &lt;- fwdControl(year = inipy:endpy, quant = &quot;f&quot;, value = ftrg) trg01 &lt;- fwdControl(year = inipy:endpy, quant = &quot;f&quot;, value = 0.8*ftrg) # project if(ssb(stk00)[,ac(maxy)] &gt; blim){ stk10 &lt;- fwd(stk00, control=trg00, sr=sr00) } else { stk10 &lt;- fwd(stk00, control=trg01, sr=sr00) } Figure 12.9: Projection of stock for 2 years following a HCR with a target of FMSY and limit of 50% SSBMSY References "],["reference-points.html", "13 Reference Points 13.1 Yield per recruit reference points 13.2 Stock recruitment relationship based reference points 13.3 Economics reference points 13.4 Computing user specific reference points", " 13 Reference Points One of the primary objectives of stock assessment is the estimation of reference points. These serve as benchmarks for evaluating the outputs of assessment models and determining the status of a fish stock. Reference points are critical for effective fisheries management, guiding decisions on sustainable exploitation. The most common classification of stock status is bidimensional, comparing exploitation levels and biomass sizes against target reference points. This framework allows for the assessment of whether a stock is overfished or experiencing overfishing: Exploitation Levels: Typically represented by fishing mortality (F), overfishing occurs when F exceeds the target reference point. Conversely, if F is below the reference, the stock is considered to be sustainably fished. Biomass Size: Commonly measured by spawning stock biomass (SSB), stocks are deemed overfished if the SSB falls below the reference point. These assessments often utilize tools like the Kobe plot, which visually represents stock status in relation to these metrics. In addition to target reference points, limit reference points (LRPs) are commonly included in stock assessments. These represent thresholds that should not be crossed, as they signal a high risk of stock collapse or significant uncertainty in population dynamics. Effective management aims to maintain fishing pressure and biomass levels within safe biological limits, ensuring long-term sustainability and reducing the risk of adverse outcomes. Advancements in stock assessment science continue to refine these reference points. For example, Maximum Sustainable Yield (MSY) and its proxies, such as B\\(_{MSY}\\) (biomass at MSY) and F\\(_{MSY}\\) (fishing mortality at MSY), remain widely used. These reference points are related with the stock’s productivity, which in itself is a complex interaction between recruitment, growth and mortality processes. Recruitment is the process of input to the population, it defines the number of fish that will enter the population and are vulnerable to fishing. It encompasses the process of spawning, which depends on the reproductive potential of the individuals, and the survivability of the laervae up to entering the fishery, which mostly depends on environmental conditions. Individual growth defines the time needed for an individual to gain weight, grow in length and eventualy mature and spawn. Mortality is commonly split between mortality caused by fishing and mortality caused by natural events. Natural mortality merges together all factors by which an individual may die and are not related to fishing, for example predation from other species. These processes, recruitment, individual growth and natural mortality depend on a mix of interactions between environmental conditions and species’ biology. Fishing mortality on the other hand is mostly dependent on the human factor, it’s related with the choice to fish and the way to fish. It’s the outcome of the effort the fleet deploys, the selectivity of the gear used and the availability of individuals. For example, the productivity of the stock will be different if the fleet fishes in an area with lots of young fish using a small mesh size, from a fleet fishing in an area where young fish are not common and using a large mesh size. For this section we’ll be using the package FLBRP [] from the FLR family of packages. library(FLBRP) library(FLa4a) data(ple4) data(ple4.indices) fit0 &lt;- sca(ple4, ple4.indices) stk0 &lt;- ple4 + fit0 To proceed with the computation of reference points we must start by creating an FLBRP object and afterwards run the fitting process with brp(). The FLBRP class has information on selection pattern, mass at age, and biological parameters. The information is stored in the object’s slots which can be accessed with the usual commands, respectively catch.sel(), discards.sel(), stock.wt(), catch.wt(), discards.wt(), m() and mat(). These quantities are computed by averaging the 3 most recent years of the relevant stock object slots. In the case of the selection pattern it is computed by scaling F-at-age to a maximum of 1. By default FLBRP creates a harvest slot with 100 computations of fishing mortality at age scaled from \\(\\bar{F}=0\\) up to \\(F_{crash}\\) or \\(\\bar{F}=4\\), if the former isn’t possible to compute, which is later used to compute the reference points. A number of parameters can be set by the user to create the FLBRP object: Argument Default value Description fbar seq(0, 4, length.out = 101) nyears 3 biol.nyears nyears fbar.nyears nyears sel.nyears fbar.nyears na.rm TRUE mean “arithmetic” 13.1 Yield per recruit reference points In the case where no stock recruitment relationship exists, or was fitted, brp() will return yield per recruit reference points. By default it computes biomasses in the absence of fishing, also know as virgin biomass, \\(F_{MAX}\\), \\(F_{0.1}\\) and 40% Spawning per recruit reference points. brp0 &lt;- FLBRP(stk0) brp0 &lt;- brp(brp0) summary(brp0) ## An object of class &quot;FLBRP&quot; ## ## Name: ## Description: ## Quant: age ## Dims: age year unit season area iter ## 10 101 1 1 1 1 ## ## Range: min max pgroup minfbar maxfbar ## 1 10 10 2 6 ## ## ## Model: rec ~ a ## params ## iter a ## 1 1 ## ## refpts: calculated The selection pattern and other quantities can be depicted by calling plot() on the specific FLBRP object’s slot. To extract a table with all reference points one uses the method refpts(). Note in this case \\(F_{msy}\\) is the same as \\(F_{max}\\), since the assumed stock recruitment is mean recruitment. refpts(brp0) ## An object of class &quot;FLPar&quot; ## quant ## refpt harvest yield rec ssb biomass revenue cost ## virgin 0.00e+00 0.00e+00 1.00e+00 3.42e+00 3.53e+00 NA NA ## msy 2.10e-01 7.08e-02 1.00e+00 9.44e-01 1.03e+00 NA NA ## crash 1.47e+01 6.02e-06 1.00e+00 4.38e-06 2.87e-02 NA NA ## f0.1 1.58e-01 6.85e-02 1.00e+00 1.28e+00 1.38e+00 NA NA ## fmax 2.10e-01 7.08e-02 1.00e+00 9.44e-01 1.03e+00 NA NA ## spr.30 1.96e-01 7.06e-02 1.00e+00 1.03e+00 1.12e+00 NA NA ## mey NA NA NA NA NA NA NA ## quant ## refpt profit ## virgin NA ## msy NA ## crash NA ## f0.1 NA ## fmax NA ## spr.30 NA ## mey NA ## units: NA refpts(brp0)[c(&#39;msy&#39;, (&#39;fmax&#39;)), ] ## An object of class &quot;FLPar&quot; ## quant ## refpt harvest yield rec ssb biomass revenue cost profit ## msy 0.2099 0.0708 1.0000 0.9436 1.0349 NA NA NA ## fmax 0.2099 0.0708 1.0000 0.9436 1.0349 NA NA NA ## units: NA The depiction of the reference points with the method plot() shows recruitment as constant over all levels of biomass and set to \\(1\\). 13.2 Stock recruitment relationship based reference points An important way to improve reference points is to include stock recruitment dynamics. Yield per recruit, as in previous section, ignores this dynamics and assumes recruitment will be the same no matter SSB’s size, which is obviously wrong although in many cases due to unknown or very uncertain dynamics it’s the best one can do. The stock recruitment model must be fitted before computing reference points and the FLSR object has to be passed to the FLBRP call to create the object that brp() method will use. There’s two ways of fitting stock recruitment models: (i) after fitting the stock assessment model by using its outputs, SSB and recruitment, as data to fit the model; (ii) inside the stock assessment model together with all other quantities. There’s pros and cons on both approaches, we’re not going to dwell on those now though. 13.2.1 Stock recruitment after fitting the stock assessment model In the following example we’ll use a Beverton and Holt stock recruitment reltionship. There are several other relationships that can be used, see Table XX (ver ?bevholt()) Model Formula Function Description Beverton and Holt rec ~ a * ssb/(b + ssb) bevholt() [REF] Ricker [REF] rec ~ a * ssb * exp(-b * ssb) ricker() Segmented regression [REF] rec ~ ifelse(ssb &lt;= b, a * ssb, a * b) segreg Cushing [REF] rec ~ a * ssb^b cushing Shepherd [REF] shepherd Geomean [REF] geomean sr0 &lt;- as.FLSR(stk0, model=bevholt) sr0 &lt;- fmle(sr0) ## Nelder-Mead direct search function minimizer ## function value for initial parameters = -18.229811 ## Scaled convergence tolerance is 2.71645e-07 ## Stepsize computed as 138047.472012 ## BUILD 3 -8.381087 -21.236493 ## REFLECTION 5 -18.229811 -23.986923 ## LO-REDUCTION 7 -21.236493 -23.986923 ## HI-REDUCTION 9 -23.670348 -23.986923 ## REFLECTION 11 -23.689833 -24.357493 ## LO-REDUCTION 13 -23.986923 -24.517332 ## LO-REDUCTION 15 -24.357493 -24.651322 ## LO-REDUCTION 17 -24.517332 -24.735311 ## LO-REDUCTION 19 -24.651322 -24.782483 ## LO-REDUCTION 21 -24.735311 -24.826934 ## LO-REDUCTION 23 -24.782483 -24.842111 ## LO-REDUCTION 25 -24.826934 -24.866395 ## HI-REDUCTION 27 -24.842111 -24.872880 ## HI-REDUCTION 29 -24.866395 -24.884011 ## LO-REDUCTION 31 -24.872880 -24.884011 ## HI-REDUCTION 33 -24.879745 -24.884011 ## EXTENSION 35 -24.882025 -24.888317 ## EXTENSION 37 -24.884011 -24.897425 ## EXTENSION 39 -24.888317 -24.904784 ## EXTENSION 41 -24.897425 -24.937270 ## EXTENSION 43 -24.904784 -24.940973 ## EXTENSION 45 -24.937270 -25.043544 ## LO-REDUCTION 47 -24.940973 -25.043544 ## EXTENSION 49 -25.008191 -25.186152 ## LO-REDUCTION 51 -25.043544 -25.186152 ## EXTENSION 53 -25.161825 -25.335939 ## EXTENSION 55 -25.186152 -25.540561 ## LO-REDUCTION 57 -25.335939 -25.540561 ## EXTENSION 59 -25.501589 -25.922665 ## EXTENSION 61 -25.540561 -26.001469 ## EXTENSION 63 -25.922665 -26.704604 ## LO-REDUCTION 65 -26.001469 -26.704604 ## LO-REDUCTION 67 -26.468110 -26.704604 ## LO-REDUCTION 69 -26.695594 -26.704604 ## HI-REDUCTION 71 -26.703662 -26.727696 ## HI-REDUCTION 73 -26.704604 -26.729632 ## HI-REDUCTION 75 -26.726736 -26.729632 ## HI-REDUCTION 77 -26.727696 -26.731255 ## HI-REDUCTION 79 -26.729632 -26.731255 ## HI-REDUCTION 81 -26.731204 -26.731582 ## HI-REDUCTION 83 -26.731255 -26.731744 ## HI-REDUCTION 85 -26.731582 -26.731772 ## HI-REDUCTION 87 -26.731744 -26.731820 ## HI-REDUCTION 89 -26.731772 -26.731823 ## HI-REDUCTION 91 -26.731820 -26.731835 ## HI-REDUCTION 93 -26.731823 -26.731844 ## LO-REDUCTION 95 -26.731835 -26.731844 ## Exiting from Nelder Mead minimizer ## 97 function evaluations used plot(sr0) We now need to provide the FLSR object, sr0, to the FLBRP call and refit the reference points. brp0 &lt;- FLBRP(stk0, sr=sr0) model(brp0) ## rec ~ a * ssb/(b + ssb) ## &lt;environment: 0x5b9c39ea76c0&gt; params(brp0) ## An object of class &quot;FLPar&quot; ## params ## a b ## 1038832 9829 ## units: NA brp0 &lt;- brp(brp0) The new reference points can now be extracted using the refpts method with the FLBRP object as the main argument, and depict the relationships with plot(). Note this time by setting the flag obs to TRUE the plot will include the estimates of \\(SSB\\) and \\(R\\). refpts(brp0) ## An object of class &quot;FLPar&quot; ## quant ## refpt harvest yield rec ssb biomass revenue cost ## virgin 0.00e+00 0.00e+00 1.04e+06 3.54e+06 3.65e+06 NA NA ## msy 2.07e-01 7.28e+04 1.03e+06 9.87e+05 1.08e+06 NA NA ## crash 2.25e+00 1.11e-06 3.87e-04 3.66e-06 1.83e-05 NA NA ## f0.1 1.58e-01 7.06e+04 1.03e+06 1.32e+06 1.42e+06 NA NA ## fmax 2.10e-01 7.28e+04 1.03e+06 9.70e+05 1.06e+06 NA NA ## spr.30 1.96e-01 7.27e+04 1.03e+06 1.06e+06 1.15e+06 NA NA ## mey NA NA NA NA NA NA NA ## quant ## refpt profit ## virgin NA ## msy NA ## crash NA ## f0.1 NA ## fmax NA ## spr.30 NA ## mey NA ## units: NA Note \\(MSY\\) based reference points are no longer the same as \\(F_{MAX}\\), and recruitment is no longer constant over all \\(SSB\\) levels. 13.2.2 Stock recruitment during fitting the stock assessment model fit1 &lt;- sca(ple4, ple4.indices, srmodel = ~ bevholt(CV = 0.5)) a4aflsr &lt;- as(stkmodel(fit1), &quot;FLSR&quot;) # or a4aflsr &lt;- as(fit1, &quot;FLSR&quot;) plot(a4aflsr, obs = TRUE) a4abrp &lt;- FLBRP(stk0, a4aflsr) 13.3 Economics reference points We can add economic data to the FLBRP object to calculate economic based reference points, like maximum economic yield (MEY). We need to provide information about price, variable costs and fixed costs. The first in value at age per weight of fish, the others in value per unit of fishing mortality. # price price(brp0) &lt;- c(rep(1,3),rep(1.5,2),rep(2,5)) price(brp0)@units &lt;- &quot;1000 euro per ton&quot; # variable costs per F vcost(brp0) &lt;- 100000 vcost(brp0)@units &lt;- &quot;1000 euro per F&quot; # fixed costs per F fcost(brp0) &lt;- 50000 fcost(brp0)@units &lt;- &quot;1000 euro per F&quot; # reference points brp0 &lt;- brp(brp0) refpts(brp0) ## An object of class &quot;FLPar&quot; ## quant ## refpt harvest yield rec ssb biomass revenue cost ## virgin 0.00e+00 0.00e+00 1.04e+06 3.54e+06 3.65e+06 0.00e+00 5.00e+04 ## msy 2.07e-01 7.28e+04 1.03e+06 9.87e+05 1.08e+06 1.21e+05 7.07e+04 ## crash 2.25e+00 1.11e-06 3.87e-04 3.66e-06 1.83e-05 1.15e-06 2.75e+05 ## f0.1 1.58e-01 7.06e+04 1.03e+06 1.32e+06 1.42e+06 1.20e+05 6.58e+04 ## fmax 2.10e-01 7.28e+04 1.03e+06 9.70e+05 1.06e+06 1.21e+05 7.10e+04 ## spr.30 1.96e-01 7.27e+04 1.03e+06 1.06e+06 1.15e+06 1.22e+05 6.96e+04 ## mey 2.19e-01 7.27e+04 1.03e+06 9.22e+05 1.02e+06 1.21e+05 7.19e+04 ## quant ## refpt profit ## virgin -5.00e+04 ## msy 5.07e+04 ## crash -2.75e+05 ## f0.1 5.38e+04 ## fmax 5.03e+04 ## spr.30 5.21e+04 ## mey 4.89e+04 ## units: NA The reference points table is now complete with values for revenue, costs and profit, as well as estimtes for \\(MEY\\) based reference points. The point where profits are maximized, instead of the point where catch is maximized as in the case of MSY. plot(brp0) 13.4 Computing user specific reference points There is an option to calculate user defined reference points given a target F: custom_refs &lt;- FLPar(Ftrgt1 = 0.33, Ftrgt2 = 0.44) brp1 &lt;- brp0 + custom_refs refpts(brp1) ## An object of class &quot;FLPar&quot; ## quant ## refpt harvest yield rec ssb biomass revenue cost ## virgin 0.00e+00 0.00e+00 1.04e+06 3.54e+06 3.65e+06 0.00e+00 5.00e+04 ## msy 2.07e-01 7.28e+04 1.03e+06 9.87e+05 1.08e+06 1.21e+05 7.07e+04 ## crash 2.25e+00 1.11e-06 3.87e-04 3.66e-06 1.83e-05 1.15e-06 2.75e+05 ## f0.1 1.58e-01 7.06e+04 1.03e+06 1.32e+06 1.42e+06 1.20e+05 6.58e+04 ## fmax 2.10e-01 7.28e+04 1.03e+06 9.70e+05 1.06e+06 1.21e+05 7.10e+04 ## spr.30 1.96e-01 7.27e+04 1.03e+06 1.06e+06 1.15e+06 1.22e+05 6.96e+04 ## mey 2.19e-01 7.27e+04 1.03e+06 9.22e+05 1.02e+06 1.21e+05 7.19e+04 ## Ftrgt1 3.30e-01 6.53e+04 1.02e+06 4.95e+05 5.80e+05 1.05e+05 8.30e+04 ## Ftrgt2 4.40e-01 5.37e+04 1.00e+06 2.81e+05 3.58e+05 8.27e+04 9.40e+04 ## quant ## refpt profit ## virgin -5.00e+04 ## msy 5.07e+04 ## crash -2.75e+05 ## f0.1 5.38e+04 ## fmax 5.03e+04 ## spr.30 5.21e+04 ## mey 4.89e+04 ## Ftrgt1 2.16e+04 ## Ftrgt2 -1.13e+04 ## units: NA Or create an empty FLPar with specified reference points and recalculate everything: #brp2 &lt;- FLPar(NA,dimnames=list(refpt=c(&quot;virgin&quot;,&quot;f0.1&quot;,&quot;fmax&quot;,&quot;spr.30&quot;,&quot;spr.35&quot;,&quot;spr.45&quot;), quantity=c(&quot;harvest&quot;,&quot;yield&quot;,&quot;rec&quot;,&quot;ssb&quot;,&quot;biomass&quot;,&quot;revenue&quot;,&quot;cost&quot;,&quot;profit&quot;), iter=1)) brp2 &lt;- brp1 brp2@refpts &lt;- FLPar(NA, dimnames=list(refpt = c(&quot;virgin&quot;, &quot;f0.1&quot;, &quot;fmax&quot;, &quot;spr.30&quot;, &quot;spr.35&quot;,&quot;spr.45&quot;), quantity=c(&quot;harvest&quot;, &quot;yield&quot;, &quot;rec&quot;, &quot;ssb&quot;, &quot;biomass&quot;, &quot;revenue&quot;, &quot;cost&quot;, &quot;profit&quot;), iter=1)) brp2 &lt;- brp(brp2) refpts(brp2) ## An object of class &quot;FLPar&quot; ## quantity ## refpt harvest yield rec ssb biomass revenue cost ## virgin 0.00e+00 0.00e+00 1.04e+06 3.54e+06 3.65e+06 0.00e+00 5.00e+04 ## f0.1 1.58e-01 7.06e+04 1.03e+06 1.32e+06 1.42e+06 1.20e+05 6.58e+04 ## fmax 2.10e-01 7.28e+04 1.03e+06 9.70e+05 1.06e+06 1.21e+05 7.10e+04 ## spr.30 1.96e-01 7.27e+04 1.03e+06 1.06e+06 1.15e+06 1.22e+05 6.96e+04 ## spr.35 1.69e-01 7.16e+04 1.03e+06 1.23e+06 1.33e+06 1.21e+05 6.69e+04 ## spr.45 1.27e-01 6.64e+04 1.03e+06 1.59e+06 1.69e+06 1.13e+05 6.27e+04 ## quantity ## refpt profit ## virgin -5.00e+04 ## f0.1 5.38e+04 ## fmax 5.03e+04 ## spr.30 5.21e+04 ## spr.35 5.38e+04 ## spr.45 5.07e+04 ## units: NA One specific case is to compute \\(F_{MSY}\\) ranges according to Hilborn (2010) and Rindorf et al. (2016) ideas. For this case there’s already the method msyRanges, which takes as argument a fitted FLBRP object and delivers a FLPar object, similar to refpts. rp.rngs &lt;- msyRange(brp0, range=0.05) rp.rngs ## An object of class &quot;FLPar&quot; ## quantity ## refpt harvest yield rec ssb biomass revenue cost profit ## msy 2.07e-01 7.28e+04 1.03e+06 9.87e+05 1.08e+06 1.21e+05 7.07e+04 5.07e+04 ## min 1.45e-01 6.92e+04 1.03e+06 1.43e+06 1.53e+06 1.18e+05 6.45e+04 5.31e+04 ## max 2.87e-01 6.92e+04 1.02e+06 6.26e+05 7.14e+05 1.12e+05 7.87e+04 3.36e+04 ## units: NA Another simple way, although it onl;y works for \\(SPR\\) based reference points, is to include other spr.## points in the refpts table. References "],["propagate-uncertainty-into-stock-assessment.html", "14 Propagate uncertainty into stock assessment", " 14 Propagate uncertainty into stock assessment In a multistage stock assessment process as described in this book, it’s important to be able to propagate uncertainty across the different stages. This section describes methods to propagate uncertainty across stages and compares their outcomes in terms of stock assessment outputs. The idea is to add uncertainty as one moves from one stage to the next. If a stock has uncertainty on it’s growth parameters, or natural mortality, or any other quantity estimated or set during the input data preparation, the model fit uncertainty will be added to it by generating iterations in the input data which are then used to fit the stock assessment model. The suggested workflow is: Add uncertainty in growth or M parameters. Draw from the parameters distribution. Compute metrics for stock assessment. If there’s uncertainty in growth parameters use slicing to created iterations of metrics by age, e.g. catch at age and index at age. If there’s uncertainty in M parameters draw from the distribution and generate iterations of the M matrix. If both draw from growth and M parameters, potentially having into account correlation between those parameters, and generate iterations of age based metrics and M. Fit the stock assessment model to each iteration Simulate from each fit Aggregate results in single FLStock object. In this section we give an example of how uncertainty in natural mortality, set up using the m() method and the class a4aM (see chapter XX), is propagated through the stock assessment. We’ll use the stock of Red Mullet in the Mediterranean GSA 1 (see Introduction for details) and 3 methods to add estimation uncertainty (step 5 above): Take one draw of the fit Take n draws of the fit and summarize with the median Take n draws of the fit and combine all These outcomes will be compared with a fit across M iterations without any sampling from the fit. Using a4a methods we’ll model natural mortality using a negative exponential model by age, Jensen’s estimator for the level and no time trend. We include multivariate normal uncertainty using the mvrnorm() method and create 250 iterations. nits &lt;- 250 shape &lt;- FLModelSim(model=~exp(-age-0.5)) level &lt;- FLModelSim(model=~k^0.66*t^0.57, params = FLPar(k=0.4, t=10), vcov=matrix(c(0.002, 0.01,0.01, 1), ncol=2)) #trend &lt;- FLModelSim(model=~b, params=FLPar(b=0.5), vcov=matrix(0.02)) m4 &lt;- a4aM(shape=shape, level=level) m4 &lt;- mvrnorm(nits, m4) range(m4)[] &lt;- range(stk00)[] range(m4)[c(&quot;minmbar&quot;,&quot;maxmbar&quot;)]&lt;-c(1,1) flq &lt;- m(m4)[] quant(flq) &lt;- &quot;age&quot; stk0 &lt;- propagate(stk00, nits) m(stk0) &lt;- flq The M matrix for this stock is shown in Figure14.1). Figure 14.1: Natural mortality generated from M model’s parameter uncertainty We fit the same model to the new stock object which has uncertainty in the natural mortality and add estimation uncertainty following the methods described above. # create objects to store the results stk01 &lt;- stk0 stk02 &lt;- stk0 stk03 &lt;- propagate(stk00, nits*nits) # run without estimation unceratainty stk04 &lt;- stk00 + sca(stk0, idx00) for(i in 1:nits){ stk &lt;- iter(stk0, i) fit &lt;- sca(stk, idx00) # Method 1 iter(stk01, i) &lt;- stk + simulate(fit, 1) # Method 2 iter(stk02, i) &lt;- qapply(stk + simulate(fit, nits), iterMedians) # Method 3 iter(stk03, (nits*(i-1)+1):(nits*i)) &lt;- stk + simulate(fit, nits) } plot(FLStocks(&quot;M&quot;=stk04, &quot;M + 1 estimation sample&quot;=stk01, &quot;M + estimation median&quot;=stk02, &quot;M + n estimation samples&quot;=stk03)) Figure 14.2: Stock summary. Stock metrics computed over fits including uncertainty in M and estimation uncertainty "],["modelling-fleet-selectivity.html", "15 Modelling fleet selectivity 15.1 Modelling fleets with different proportions of the catch covariate", " 15 Modelling fleet selectivity 15.1 Modelling fleets with different proportions of the catch covariate cp1 &lt;- cp2 &lt;- catch.n(ple4) pa &lt;- c(0.2, 0.3, 0.5, 0.5, 0.6, 0.7, 0.6, 0.6, 0.6, 0.6) na &lt;- nrow(cp1) for(i in 1:na) cp1[i][] &lt;- mean(rbinom(100, 1, pa[i])) for(i in 1:na) cp2[i][] &lt;- mean(rbinom(100, 1, 1-pa[i])) cp2 &lt;- 1-cp1 fmod &lt;- ~ s(year, k = 20, by = cp1) + s(year, k = 20, by = cp2) + s(age, k = 5) fit &lt;- sca(ple4, ple4.indices, fmodel=fmod, covar=FLQuants(cp1=cp1, cp2=cp2)) This approach allows the inclusion of mean length in the catch of each fleet, as an indicator of fleet&#39;s selctivity, to model fishing mortality. This approach wouldn&#39;t model individual fleet selectivity, it would bring information about fleet selectivity to help fitting the fishing mortality. plot(FLStocks(f1=ple4+fit, f0=ple4+fit0)) fit b &lt;- coef(simulate(stkmodel(fit), nsim = 100)) by1 &lt;- grep(&quot;fMod:s[(]age[)]:by1&quot;, dimnames(b)$params) by2 &lt;- grep(&quot;fMod:s[(]age[)]:by2&quot;, dimnames(b)$params) # doesnt work getX(~ s(age, k = 3), data.frame(age = dims(ple4)$min:dims(ple4)$max)) # need covars to make it work df &lt;- data.frame( year = rep(dims(ple4)$minyear:dims(ple4)$maxyear, each = dims(ple4)$age), age = dims(ple4)$min:dims(ple4)$max, cp1 = c(cp1[, 1]), cp2 = c(cp2[, 1]) ) X &lt;- getX(fmodel(fit), df) # just keep first 10 rows (ages 1:10) X &lt;- X[1:10,] sims1 &lt;- exp(X[,by1] %*% b[by1,]) sims2 &lt;- exp(X[,by2] %*% b[by2,]) par(mfrow = c(2, 1)) matplot(sims1, type = &quot;l&quot;) matplot(sims2, type = &quot;l&quot;) fmodel0 &lt;- ~ s(age, k = 5) + s(year, k = 10) fit0 &lt;- sca(ple4, ple4.indices, fmodel = fmodel0) plot(splines::bs(year) %*% c(0, 1, 1)) plot(pnorm(year, 1990, 5), type = &quot;b&quot;) fmodel2 &lt;- ~ s(age, k = 3, by = cbind(pnorm(year, 1990, 5), 1 - pnorm(year, 1990, 5))) + s(year, k = 18) fit2 &lt;- sca(ple4, ple4.indices, fmodel = fmodel2) xyplot(data ~ age | year &gt; 1990, groups = year, data = harvest(fit2), type = &quot;l&quot;) wireframe(harvest(fit2)) b &lt;- coef(simulate(stkmodel(fit2), nsim = 1000)) by1 &lt;- grep(&quot;fMod:s[(]age[).]&quot;, dimnames(b)$params) by2 &lt;- grep(&quot;fMod:s[(]age[)]:&quot;, dimnames(b)$params) X &lt;- getX(fmodel(fit2), as.data.frame(catch.n(ple4))) sims1 &lt;- X[1:10, by1] %*% b[by1, ] sims2 &lt;- X[nrow(X) - (10:1) + 1, by2] %*% b[by2, ] # get quantiles simsqs1 &lt;- t(apply(sims1, 1, quantile, c(0.25, .5))) par(mfrow = c(2, 1)) matplot(sims1, type = &quot;l&quot;) matplot(sims2, type = &quot;l&quot;) dm &lt;- dims(ple4) # simulate a simple model age &lt;- dm$min:dm$max year &lt;- dm$minyear:dm$maxyear s1 &lt;- c(splines::bs(age) %*% c(.2, 1, 1)) s2 &lt;- c(splines::bs(age) %*% c(-1, 1, 0)) s1 &lt;- s1 / max(s1) s2 &lt;- s2 / max(s2) weight &lt;- pnorm(year, 1990, 5) obj &lt;- catch.n(ple4) y &lt;- s1 %o% weight + s2 %o% (1 - weight) yobs &lt;- y + rnorm(length(y), 0, 0.01) dat &lt;- data.frame( year = rep(year, each = length(age)), age = rep(age, length(year)), y = c(yobs) ) L &lt;- cbind(rep(weight, each = dm$age), 1 - rep(weight, each = dm$age)) agem &lt;- cbind(dat$age, dat$age) library(mgcv) fit0 &lt;- gam(y ~ s(age, k = 5) + s(year, k = 20), data = dat) fit1 &lt;- gam(y ~ s(agem, k = 5, by = L), data = dat) obj[] &lt;- fitted(fit1) wireframe(obj) xyplot(data ~ age | year &gt; 1990, groups = year, data = obj, type = &quot;l&quot;) "],["modelling-spatial-effects.html", "16 Modelling spatial effects", " 16 Modelling spatial effects ridx01 &lt;- stk0@stock.n[1]*0.7*0.001 ridx01 &lt;- log(ridx01*rlnorm(ridx01)) ridx02 &lt;- stk0@stock.n[1]*0.3*0.001 ridx02 &lt;- log(ridx02*rlnorm(ridx02)) srmod &lt;- ~ geomean(a~ridx01+ridx02, CV=0.5) cvar &lt;- FLQuants(ridx01 = ridx01, ridx02 = ridx02) fit01 &lt;- sca(stock,tun.sel[c(1)],fmodel=fmod,qmodel=qmod, srmodel=srmod, covar=cvar) coef(fit01) srmod &lt;- ~ geomean(CV=0.1) fit02 &lt;- sca(stock,tun.sel[c(1)],fmodel=fmod,qmodel=qmod, srmodel=srmod) coef(fit02) check situation where the two areas are negatively correlated the two covariates need to be at the same scale, in the sense of representing the same process other examples (ICES, ask in the plenary if we can have access to the data) dan ghotel ask for spatial workshop data "],["sections-to-be-added.html", "17 Sections to be added!?", " 17 Sections to be added!? Assessing the coverage of confidence intervals?? "],["annex---stock-assessment-workflow.html", "18 Annex - stock assessment workflow 18.1 The “mean” model 18.2 The age effects 18.3 Year effect on fishing mortality 18.4 Year effect on catchability 18.5 The initial year population abundance model, aka N1 18.6 The stock recruitment submodel 18.7 The variance submodel 18.8 Final comments", " 18 Annex - stock assessment workflow The following sections describes a potential workflow for fitting a a4a stock assessment model. The idea is to explore the age and year effects in isolation and adjust the model’s smoothness to model those effects. The procedure is heavily supported by residuals’ analysis. In a well specified model residuals should show a random pattern, without any trend or very high values (outliers). 18.1 The “mean” model To start the analysis we’ll fit a “mean” model, where all submodels will be set to an overall average, by using the \\(\\sim 1\\) formula. This will be our reference model to see how adding age and year effects will show up in the diagnostic tools, in particular in the residuals. library(FLa4a) data(hke1567) data(hke1567.idx) fit01 &lt;- sca(hke1567, hke1567.idx, fmod=~1, qmod=list(~1), srmod=~1, vmod=list(~1, ~1), n1mod=~1) res01 &lt;- residuals(fit01, hke1567, hke1567.idx) The common residuals plot clearly shows a time trend for each age (Figure 18.2) for both datasets. Furthermore, inspecting how catch ate age residuals are positioned across ages, by comparing the level of residuals for each age, one can see the pattern of lower than 0 residuals in age 0, reversing the signal for ages 1 and 2, close to 0 in ages 3 and 4, and again below 0 in age 5. plot(res01) Figure 18.1: Mean fit residuals by year) This pattern becomes more apparent when plotting the residuals by age across years. plot(res01, auxline=&quot;l&quot;, by=&quot;age&quot;) Figure 18.2: Mean fit residuals by age 18.2 The age effects The following models will introduce age effects in the fishing mortality submodel and catchability submodel. In the fishing mortality submodel we’ll introduce a factor which means that there will be as many parameters as ages minus 1 and each parameters will be independent of each other. fit02 &lt;- sca(hke1567, hke1567.idx, fmod=~factor(age), qmod=list(~1), srmod=~1, vmod=list(~1, ~1), n1mod=~1) res02 &lt;- residuals(fit02, hke1567, hke1567.idx) The residuals plot now shows catch at age residuals less stagered, reflecting the modelling of the age effect. plot(res02) Figure 18.3: Fishing mortality model with age effect residuals by year The residuals plot by age shows the same outcome. plot(res02, auxline=&quot;l&quot;, by=&quot;age&quot;) Figure 18.4: Fishing mortality model with age effect residuals by age We’ll now proceed adding an age effect to the catchability model while removing the catch at age effect. fit03 &lt;- sca(hke1567, hke1567.idx, fmod=~1, qmod=list(~factor(age)), srmod=~1, vmod=list(~1, ~1), n1mod=~1) res03 &lt;- residuals(fit03, hke1567, hke1567.idx) plot(res03) Figure 18.5: Index catchability model with age effect residuals by year The residuals plot by age shows the same outcome. plot(res03, auxline=&quot;l&quot;, by=&quot;age&quot;) Figure 18.6: Index catchability model with age effect residuals by age Finally both effects are brought together. fit04 &lt;- sca(hke1567, hke1567.idx, fmod=~factor(age), qmod=list(~factor(age)), srmod=~1, vmod=list(~1, ~1), n1mod=~1) res04 &lt;- residuals(fit04, hke1567, hke1567.idx) plot(res04) Figure 18.7: Fishing mortality and index catchability models with age effect residuals by year The residuals plot by age shows the same outcome. plot(res04, auxline=&quot;l&quot;, by=&quot;age&quot;) Figure 18.8: Fishing mortality and index catchability models with age effect residuals by age 18.3 Year effect on fishing mortality This model will introduce an year effect in the fishing mortality submodel on top of the F age effect added before. Inspecting the last set of residuals (Figure 18.8) one can easily see the pattern across years with more positive residuals in the beggining of the time series and more negative in the most recent years. As for age we’re using a factor for years. The new model’s residuals won’t show such a pronounced effect anymore (Figures 18.9 and 18.10). fit05 &lt;- sca(hke1567, hke1567.idx, fmod=~factor(age) + factor(year), qmod=list(~1), srmod=~1, vmod=list(~1, ~1), n1mod=~1) res05 &lt;- residuals(fit05, hke1567, hke1567.idx) plot(res05) Figure 18.9: Fishing mortality model with year effect residuals by year The residuals plot by age shows the same outcome. plot(res05, auxline=&quot;l&quot;, by=&quot;age&quot;) Figure 18.10: Fishing mortality model with year effect residuals by age We can see now that the residuals show a lot less patterns than before. There’s still some issues, the survey catchability seems to have an year trend. However the model is not fully specified yet, stock recruitment is modelled as constant over time, the initial population abundance is also modelled as a constant as well as the variance models. 18.4 Year effect on catchability It’s uncommon to include year trends on the abundance index catchability model. Such decision needs to be considered carefully as the trend in the index, in the case of a well design scientific survey, should result from a change in abundance. Modelling that trend would attribute such change to the survey design and remove it from the abundance. If the survey index is based on a commercial CPUE it becomes more likely that changes in selectivity or fishing behaviour could show up in the index as changes in abundance. Although the common process of standardizing CPUEs should deal with technical issues. In the case of adding year effects to teh catchability submode the same formulas can be used, to include period breaks, trends, etc. 18.5 The initial year population abundance model, aka N1 This model sets the n-at-age in the first year of the time series, which is needed due to the lack of previous data to reconstruct those cohorts. It will affect the population numbers in the lower triangle of the initial population matrix and catches. The following model will introduce an age effect in the population abundance in the first year of the time series. fit06 &lt;- sca(hke1567, hke1567.idx, fmod=~factor(age) + factor(year), qmod=list(~factor(age)), srmod=~1, vmod=list(~1, ~1), n1mod=~factor(age)) res06 &lt;- residuals(fit06, hke1567, hke1567.idx) The best way to inspect the effect of this model is to zoom into the initial years of the time series. Figure 18.11 zooms into the previous model, which used an intercept only model for N1, while Figure 18.12. Figure 18.11: N1 fitted as an intercept only model: 2007 - 2010 residuals by age Figure 18.12: N1 fitted with an age effect model: 2007 - 2010 residuals by age Comparing the two plots it can be seen the effect of modeling abundance in the initial year. Residuals for both catch at age and catchability improved considerably. The following years also improve to different levels. 18.6 The stock recruitment submodel In this example we’ll simply add a model to allow recruitment to vary over time and we’ll see how to track potential improvements in the residuals. fit07 &lt;- sca(hke1567, hke1567.idx, fmod=~factor(age) + factor(year), qmod=list(~factor(age)), srmod=~factor(year), vmod=list(~1, ~1), n1mod=~factor(age)) res07 &lt;- residuals(fit07, hke1567, hke1567.idx) The residuals plot by year are very useful to see the effect of adding a varying stock recruitment model. The year trends present in previous models are not absent. Recruitment variability when left unmodelled was being picked up by trends in the survey catchability and catch at age. And due to the cohort dynamics underlying the catch at age model, where propagating into other ages’ estimates. plot(res07) Figure 18.13: Stock-recruitment model with year effect residuals by year 18.7 The variance submodel Finally, we’re testing the variance submodel, specifically the catch at age variance model. We won’t dig into the catchability variance model though. It’s common to accept that a scientific survey following a well designed sampling protocol will have equal variance across ages since no preferential areas should be sampled sampled. The variance model will use a smoother with k=3. The expectation is that the variance model will have a U-shape, since yourger and older ages are usually less caught and as such estomates of those ages will have larger variances than fully exploited ages. fit08 &lt;- sca(hke1567, hke1567.idx, fmod=~factor(age) + s(year, k=10), qmod=list(~factor(age)), srmod=~s(year, k=10), vmod=list(~s(age, k=3), ~1), n1mod=~factor(age)) We’ll use the pearson residuals for this analysis since those are standardized by the predicted variances of the model and not the residual variance itself, like the more common standardized residuals. Figure 18.15 shows an improve set of residuals when compared to Figure 18.14 which add an intercept only model for the variance model. res07 &lt;- residuals(fit07, hke1567, hke1567.idx, type=&quot;pearson&quot;) res08 &lt;- residuals(fit08, hke1567, hke1567.idx, type=&quot;pearson&quot;) plot(res07) Figure 18.14: Variance model with intercept only age effect pearson residuals plot(res08) Figure 18.15: Variance model with age effect pearson residuals To see what’s happening with the variance model one can use predict to plot the different models fitted. Figure 18.16: Variance models for catch at age To see the effect these models have on the estimated quantities one can look at the variance of the estimates: Figure 18.17: Estimates of population abundance with different variance models 18.8 Final comments The sequence presented here can be changed and eploited in any order the user is interested or prefers to. The approach of not allowing year effects in surveys and variance model can be modified if the user prefers to do so. The (ab)use of factor is for demonstration purposes only. The user is incentivised to explore other model forms, in particular smoothers. "],["references.html", "19 References", " 19 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
