[["the-statistical-catch-at-age-stock-assessment-framework-with-markov-chain-monte-carlo-mcmc.html", "14 The statistical catch-at-age stock assessment framework with Markov Chain Monte Carlo (MCMC) 14.1 The MCMC method for sca 14.2 Diagnostics with CODA", " 14 The statistical catch-at-age stock assessment framework with Markov Chain Monte Carlo (MCMC) The previous methods were demonstrated using maximum likelihood estimation (MLE). However, ADMB also supports Markov Chain Monte Carlo (MCMC) methods, which provide significant advantages, particularly when working with complex models that involve many parameters. The key difference is that while MLE finds a single best estimate of parameters by maximizing the likelihood function, MCMC offers a broader perspective by generating an entire distribution of possible values. This approach is more informative because it does not just give the most likely estimate but also helps us understand the uncertainty surrounding it. With MCMC, researchers can incorporate prior knowledge and obtain results that are often more realistic and reliable (Gelman et al., 2013). This is especially useful when dealing with complicated models where traditional likelihood-based methods struggle, as MCMC allows for efficient exploration of possible solutions without requiring an exact mathematical formulation (Gilks, Richardson, &amp; Spiegelhalter, 1996; Robert &amp; Casella, 2004). One of the biggest advantages of MCMC is its flexibility when working with models that have irregular behavior, such as those with multiple peaks or abrupt changes in likelihood. Standard MLE methods assume that the likelihood function behaves smoothly, like a well-shaped bowl, but this is rarely true in real-world applications. In fisheries, ecology, and other applied sciences, models often have parameters that interact in complex ways, creating likelihood surfaces with ridges and multiple solutions. In these cases, MLE can easily get stuck in a local peak, failing to find the best possible estimate or underestimating the real uncertainty in the system (Neal, 1993). Since MCMC uses a probabilistic sampling approach instead of strict optimization, it moves freely across the entire space of possible values, making it more robust and adaptable to challenging problems (Robert &amp; Casella, 2013). Traditional MLE-based uncertainty estimation relies on the Hessian matrix, which essentially measures how quickly the likelihood function changes as parameters vary. This method assumes that the shape of the likelihood function is roughly the same everywhere—meaning that a quadratic (bowl-like) approximation is valid (Pawitan, 2001). However, this assumption is often unrealistic, especially in models with many parameters or correlations between them, as is common in fisheries stock assessment models. Furthermore, MLE uncertainty estimates require a large sample size for them to be accurate, which is not always available in real-world applications (Wasserman, 2004; van der Vaart, 1998). If these assumptions do not hold, MLE can give misleading confidence intervals, making decision-making riskier. Additionally, MLE assumes that the model is correctly specified—meaning that it accurately represents the real system being studied. If the model is misspecified or overly simplified, the Hessian-based uncertainty estimates may be highly unreliable, requiring alternative approaches like robust standard errors or resampling methods (White, 1982). In fields like fisheries science, where models often involve multiple correlated parameters, MCMC provides a much more flexible and realistic way to estimate uncertainty. Unlike MLE, which assumes uncertainty follows a simple symmetrical pattern, MCMC can handle more complex distributions, giving a better representation of real-world variability. This is especially important when estimating key fisheries management indicators, such as spawning stock biomass (\\(SSB\\)) or fishing mortality (\\(F\\)), which influence critical policy decisions. Because MCMC does not impose strict mathematical assumptions about the shape of uncertainty, it produces estimates that are more reflective of real-world conditions, ultimately leading to more informed and reliable management strategies. ADMB’s approach to Markov Chain Monte Carlo (MCMC) enhances Bayesian analysis by efficiently exploring parameter uncertainty in complex models. Unlike standard MCMC tools, ADMB leverages automatic differentiation to improve sampling efficiency and speed (Fournier et al., 2012). It supports various sampling algorithms, including Metropolis-Hastings and Hamiltonian Monte Carlo, which help navigate high-dimensional parameter spaces and complex likelihood structures more effectively. This makes ADMB particularly useful in applied sciences like fisheries and ecology, where uncertainty estimation is crucial for decision-making. Additionally, ADMB provides built-in diagnostics to assess MCMC convergence and reliability, ensuring that posterior distributions are well-explored and results are robust (Gelman et al., 2013). To evaluate the quality of MCMC sampling, ADMB offers several key diagnostics. Autocorrelation analysis detects dependencies between successive samples, while the effective sample size (ESS) measures the number of independent samples in the chain. The Gelman-Rubin diagnostic (\\(\\hat{R}\\)) helps assess whether multiple chains have converged to the same distribution, with values close to 1 indicating good convergence. Trace plots visually inspect parameter behavior over iterations, revealing trends or poor mixing. Additionally, ADMB monitors the acceptance rate to ensure efficient sampling and provides posterior density estimates to check if the distribution has been properly explored. These tools help users refine their MCMC runs, adjusting sampling length or proposal distributions to improve performance and ensure reliable uncertainty estimates. The manual “A Guide for Bayesian Analysis in AD Model Builder” by Cole C. Monnahan, Melissa L. Muradian and Peter T. Kuriyam describe and explain a larger group of arguments that can be set when running MCMC with ADMB, which the a4a uses. 14.0.1 References: Brooks, S., Gelman, A., Jones, G., &amp; Meng, X.-L. (2011). Handbook of Markov Chain Monte Carlo. CRC Press. Gamerman, D., &amp; Lopes, H. F. (2006). Markov Chain Monte Carlo: Stochastic Simulation for Bayesian Inference. CRC Press. Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., &amp; Rubin, D. B. (2014). Bayesian Data Analysis (3rd ed.). CRC Press. Neal, R. M. (1993). Probabilistic inference using Markov Chain Monte Carlo methods. Department of Computer Science, University of Toronto. Robert, C., &amp; Casella, G. (2013). Monte Carlo Statistical Methods (2nd ed.). Springer. Casella, G., &amp; Berger, R. L. (2002). Statistical Inference (2nd ed.). Duxbury. Lehmann, E. L., &amp; Casella, G. (1998). Theory of Point Estimation (2nd ed.). Springer. Pawitan, Y. (2001). In All Likelihood: Statistical Modelling and Inference Using Likelihood. Oxford University Press. van der Vaart, A. W. (1998). Asymptotic Statistics. Cambridge University Press. Wasserman, L. (2004). All of Statistics: A Concise Course in Statistical Inference. Springer. White, H. (1982). “Maximum Likelihood Estimation of Misspecified Models.” Econometrica, 50(1), 1-25. Fournier, D. A., Skaug, H. J., Ancheta, J., Ianelli, J., Magnusson, A., Maunder, M. N., Nielsen, A., &amp; Sibert, J. (2012). AD Model Builder: Using automatic differentiation for statistical inference of highly parameterized complex nonlinear models. Optimization Methods and Software, 27(2), 233–249. Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., &amp; Rubin, D. B. (2013). Bayesian Data Analysis (3rd ed.). CRC Press. 14.1 The MCMC method for sca This section shows how the sca methods interface with ADMB to use the MCMC fits. For this section we’ll use the hake assessment in mediterranean areas (gsa) 1, 5, 6 and 7. We’ll start buy fitting the MLE model and afterwards call the MCMC methods. The outcomes of the MCMC fit need to be inspected to make sure the chain converged and the results are robust. A set of diagnostics are available to do this work. [TO CHECK] For many Bayesian software platforms, the MCMC algorithms are started at user-specified or arbitrary places. ADMB has the advantage that it can robustly estimate the posterior mode and the covariance at that point. This information is very valuable in initializing the MCMC chain. Specifically, an MCMC chain starts from the posterior mode and uses the estimated covariance matrix in its proposed jumps (see the algorithm sections below). As such, ADMB chains typically do not need a long period to reach areas of high density. However, we caution the user to always check the MCMC output as other issues may lead to a chain that needs a longer burn-in. (REF TO Monnahan) # load libraries and data library(FLa4a) library(ggplotFL) data(hke1567) data(hke1567.idx) nsim &lt;- 250 # MLE estimate fmod &lt;- ~s(age, k = 4) + s(year, k = 8) + s(year, k = 8, by = as.numeric(age == 0)) + s(year, k = 8, by = as.numeric(age == 4)) qmod &lt;- list(~I(1/(1 + exp(-age)))) fit &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod) fit &lt;- simulate(fit, nsim) To run the MCMC method, one needs to configure a set of arguments, which is done by creating a SCAMCMC object. Table 14.1 describes the arguments available to run the MCMC method, extracted from Monnahan [ref]. For more details on the MCMC configuration in ADMB visit the ADMB website. Table 14.1: ADMB MCMC arguments Argument Description mcmc N Run N MCMC iterations mcsave N Save every N th MCMC iteration mcscale N Rescale step size for first N iterations mcmult N Rescale the covariance matrix mcrb N Reduce high parameter correlations mcprobe X Use a fat-tailed proposal distribution mcdiag Use a diagonal covariance matrix mcnoscale Do not scale the algorithm during mcu Use a uniform distribution as proposal distribution hybrid Use the hybrid method hynstep N Mean number of steps for the leapfrog method hyeps X The stepsize for the leapfrog method [X numeric and &gt; 0] # mcmc mc &lt;- SCAMCMC() # check the default pars mc ## An object of class &quot;SCAMCMC&quot; ## Slot &quot;mcmc&quot;: ## [1] 10000 ## ## Slot &quot;mcsave&quot;: ## [1] 100 ## ## Slot &quot;mcscale&quot;: ## [1] NaN ## ## Slot &quot;mcmult&quot;: ## [1] NaN ## ## Slot &quot;mcrb&quot;: ## [1] NaN ## ## Slot &quot;mcprobe&quot;: ## [1] NaN ## ## Slot &quot;mcseed&quot;: ## [1] NaN ## ## Slot &quot;mcdiag&quot;: ## [1] FALSE ## ## Slot &quot;mcnoscale&quot;: ## [1] FALSE ## ## Slot &quot;mcu&quot;: ## [1] FALSE ## ## Slot &quot;hybrid&quot;: ## [1] FALSE ## ## Slot &quot;hynstep&quot;: ## [1] NaN ## ## Slot &quot;hyeps&quot;: ## [1] NaN Defaults for now are ok, so lets fit the model. Note that the argument fit must be set to MCMC and the argument mcmc takes the SCAMCMC object. # fit the model fitmc00 &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) # check acceptance rate fitSumm(fitmc00) ## iters ## 1 ## nopar 52.0000 ## nlogl NA ## maxgrad NA ## nobs 176.0000 ## gcv NA ## convergence NA ## accrate 0.3271 As usual fitSumm store relevant information about the model fit. In the case of MCMC fit the information stored is the number of model paramters (nopar), the number of observations (nobs) and the acceptance rate (accrate). plot(hke1567 + fitmc00) 14.2 Diagnostics with CODA In essence, the diagnostics are used to give the analyst confidence that the posterior distributions of the parameters is unbiased, with simetric non correlated distributions of each parameter, over which one can make inference. There’s a large body of literature about MCMC convergence. In this section we’ll focus on convergence, stationarity and mixing, which are essential for ensuring that the chain provides reliable samples from the target posterior distribution. [, are: (i) stationarity, the chain should converge to its stationary distribution; (ii) mixing, the chain should mix well, meaning it should explore the sample space efficiently without being overly correlated; and (iii) convergence, the chain should converge to the posterior distribution. Mixing in MCMC refers to how well the chain explores the parameter space. Poor mixing means the chain is getting stuck in certain regions, leading to high autocorrelation and slow convergence.] We use the package CODA to run the diagnostics on MCMC fits. One needs to convert the sca output into a mcmc CODA object over which several diagostics can be ran. The mcmc object is a matrix with the parameters (row = iters, cols= pars). library(coda) The out-of-the-box methods available to the stock assessment scientist are trace plots, autocorrelation analysis, geweke diagnostic, acceptance rate, distribution density and cumulative means. A trace plot shows the sampled values of a parameter over iterations. A plot that looks like a random, stable “cloud” of points with no trends or drifts, with rapid fluctuactions, is a signal of convergence, meaning the chain mixes well and is stationary. If the trace plot shows a strong trend or periodicity, drifts, or long autocorrelated stretches, it means the chain hasn’t converged. For demonstration purposes we’ll create a chain with 1000 samples (mcmc=1000) and save every iter (mcsave=1), which will create a highly correlated and unstable chain, and update the initial MCMC fit to also have 1000 samples (mcmc=10000, mcsave=100). # update initial fit mc &lt;- SCAMCMC(mcmc=10000, mcsave=100) fitmc01 &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) fitmc01.mc &lt;- FLa4a::as.mcmc(fitmc01) # highly correlated fit mc &lt;- SCAMCMC(mcmc=1000, mcsave=1) fitmc02 &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) fitmc02.mc &lt;- FLa4a::as.mcmc(fitmc02) Figures ?? and ?? cleary depicts this difference. Ploting the chain for the parameter clearly shows autocorrelation for the first parameter but also the initial phase where the chain seems to be stuck in a single value (Figure ??). plot(fitmc01.mc[,1]) plot(fitmc02.mc[,1]) &lt;img src=“_main_files/figure-html/chain01, “MCMC chain trace with high autocorrelation and burnin period (left panel). Parameter density showing a double mode distribution (right panel).”-1.png” width=“50%” /&gt;&lt;img src=“_main_files/figure-html/chain01, “MCMC chain trace with high autocorrelation and burnin period (left panel). Parameter density showing a double mode distribution (right panel).”-2.png” width=“50%” /&gt; "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
