[["index.html", "Fish stock assessment with R The a4a Initiative 1 Before starting 1.1 Installing and loading libraries 1.2 How to read this document 1.3 How to get help 1.4 Notation 1.5 Acknowledgements 1.6 License, documentation and development status", " Fish stock assessment with R The a4a Initiative Ernesto Jardim, Colin Millar, Danai Mantopoulou Palouka and Iago Mosqueira 2025-04-29 1 Before starting 1.1 Installing and loading libraries To run the methods in this book the reader will need to install the FLa4a package (C. Millar and Jardim 2025) and its dependencies. Some datasets are distributed with the package and as such need to be loaded too. # from CRAN install.packages(c(&quot;copula&quot;,&quot;triangle&quot;, &quot;coda&quot;, &quot;grid&quot;, &quot;gridExtra&quot;, &quot;latticeExtra&quot;)) # from FLR install.packages(c(&quot;FLCore&quot;, &quot;FLa4a&quot;), repos=&quot;http://flr-project.org/R&quot;) # libraries library(devtools) library(FLa4a) library(XML) library(reshape2) library(ggplotFL) # datasets data(ple4) data(ple4.indices) data(ple4.index) data(rfLen) packageVersion(&quot;FLCore&quot;) ## [1] &#39;2.6.20.9333&#39; packageVersion(&quot;FLa4a&quot;) ## [1] &#39;1.9.2&#39; 1.2 How to read this document The target audience for this document are readers with some experience in R and some background on stock assessment. The document explains the approach being developed by the Assessment for All Initiative (a4a) for fish stock assessment and scientific advice. It presents a mixture of text and code, where the first explains the concepts behind the methods, while the last shows how these can be run with the software provided. Moreover, having the code allows the reader to copy/paste and replicate the analysis presented here. The sections and subsections are as independent as possible, so they can be used as a reference document for the FLa4a. 1.3 How to get help FLa4a is build using R’s object oriented implementation with S4 classes, and FLCore’s (L. T. Kell et al. 2007) class structures and methods. In order to access S4 methods and classes documentation the user needs to use specific terminology. For example, FLStock is one of our main components in order to run our stock assessment model. We can check the structure of an FLStock object as follows: showClass(&quot;FLStock&quot;) ## Class &quot;FLStock&quot; [package &quot;FLCore&quot;] ## ## Slots: ## ## Name: catch catch.n catch.wt discards discards.n ## Class: FLQuant FLQuant FLQuant FLQuant FLQuant ## ## Name: discards.wt landings landings.n landings.wt stock ## Class: FLQuant FLQuant FLQuant FLQuant FLQuant ## ## Name: stock.n stock.wt m mat harvest ## Class: FLQuant FLQuant FLQuant FLQuant FLQuant ## ## Name: harvest.spwn m.spwn name desc range ## Class: FLQuant FLQuant character character numeric ## ## Extends: ## Class &quot;FLS&quot;, directly ## Class &quot;FLComp&quot;, by class &quot;FLS&quot;, distance 2 ## ## Known Subclasses: ## Class &quot;FLStockR&quot;, directly, with explicit coerce The object oriented structure of FLa4a gives the opportunity to change the behavior of a function according to the object that is applied to. For example we can check the available methods of the function plot showMethods(&quot;plot&quot;) ## Function: plot (package base) ## x=&quot;a4aFit&quot;, y=&quot;FLIndices&quot; ## x=&quot;a4aFit&quot;, y=&quot;FLStock&quot; ## x=&quot;a4aFitCatchDiagn&quot;, y=&quot;missing&quot; ## x=&quot;a4aFitMCMCs&quot;, y=&quot;missing&quot; ## x=&quot;a4aFitResiduals&quot;, y=&quot;missing&quot; ## x=&quot;a4aFits&quot;, y=&quot;missing&quot; ## x=&quot;ANY&quot;, y=&quot;ANY&quot; ## x=&quot;color&quot;, y=&quot;ANY&quot; ## x=&quot;Copula&quot;, y=&quot;ANY&quot; ## x=&quot;FLBiol&quot;, y=&quot;missing&quot; ## x=&quot;FLBiols&quot;, y=&quot;missing&quot; ## x=&quot;FLCohort&quot;, y=&quot;missing&quot; ## x=&quot;FLIndex&quot;, y=&quot;missing&quot; ## x=&quot;FLIndexBiomass&quot;, y=&quot;missing&quot; ## x=&quot;FLIndices&quot;, y=&quot;missing&quot; ## x=&quot;FLPar&quot;, y=&quot;missing&quot; ## x=&quot;FLQuant&quot;, y=&quot;FLQuant&quot; ## x=&quot;FLQuant&quot;, y=&quot;missing&quot; ## x=&quot;FLQuantPoint&quot;, y=&quot;FLQuant&quot; ## x=&quot;FLQuantPoint&quot;, y=&quot;FLQuants&quot; ## x=&quot;FLQuantPoint&quot;, y=&quot;missing&quot; ## x=&quot;FLQuants&quot;, y=&quot;FLPar&quot; ## x=&quot;FLQuants&quot;, y=&quot;FLPars&quot; ## x=&quot;FLQuants&quot;, y=&quot;missing&quot; ## x=&quot;FLSR&quot;, y=&quot;missing&quot; ## x=&quot;FLSRs&quot;, y=&quot;ANY&quot; ## x=&quot;FLStock&quot;, y=&quot;FLPar&quot; ## x=&quot;FLStock&quot;, y=&quot;FLStock&quot; ## x=&quot;FLStock&quot;, y=&quot;FLStocks&quot; ## x=&quot;FLStock&quot;, y=&quot;missing&quot; ## x=&quot;FLStocks&quot;, y=&quot;FLPar&quot; ## x=&quot;FLStocks&quot;, y=&quot;missing&quot; ## x=&quot;mvdc&quot;, y=&quot;ANY&quot; ## x=&quot;profile.mle&quot;, y=&quot;missing&quot; by calling showMethods R prints all the possible uses of the plot function. We want to see what it does when it is called on an FLStock object with no other object. We observe that plot takes two arguments, x and y. So, in the signature of the getMethod function we are going to use, we need to define both x and y. getMethod(&#39;plot&#39;, signature = list(&quot;FLStock&quot;,&quot;missing&quot;)) ## Method Definition: ## ## function (x, y, ...) ## { ## .local &lt;- function (x, metrics = list(Rec = rec, SSB = ssb, ## Catch = catch, F = fbar), na.rm = TRUE, ...) ## { ## metrics &lt;- metrics(x, metrics = metrics) ## if (&quot;F&quot; %in% names(metrics)) ## units(metrics$F) &lt;- paste0(range(x, c(&quot;minfbar&quot;, ## &quot;maxfbar&quot;)), collapse = &quot;-&quot;) ## if (&quot;SSB&quot; %in% names(metrics)) { ## if (all(dimnames(metrics$SSB)$unit %in% c(&quot;F&quot;, &quot;M&quot;))) { ## metrics$SSB &lt;- metrics$SSB[, , &quot;F&quot;] + metrics$SSB[, ## , &quot;M&quot;] ## if (&quot;Rec&quot; %in% names(metrics)) ## metrics$Rec &lt;- unitSums(metrics$Rec) ## } ## } ## if (&quot;Rec&quot; %in% names(metrics)) { ## if (dim(metrics$Rec)[4] &gt; 1) { ## metrics$Rec[metrics$Rec == 0] &lt;- NA ## } ## } ## p &lt;- plot(metrics, na.rm = na.rm, ...) + ylim(c(0, NA)) ## if (&quot;SSB&quot; %in% names(metrics)) ## if (all(dimnames(metrics$SSB)$unit %in% c(&quot;F&quot;, &quot;M&quot;))) { ## return(p + theme(legend.position = &quot;bottom&quot;, ## legend.key = element_blank()) + labs(color = &quot;Sex&quot;) + ## scale_color_manual(name = &quot;&quot;, labels = c(&quot;Both&quot;, ## &quot;F&quot;, &quot;M&quot;), values = flpalette_colours(3))) ## } ## return(p) ## } ## .local(x, ...) ## } ## &lt;bytecode: 0x5e2b5de38108&gt; ## &lt;environment: namespace:ggplotFL&gt; ## ## Signatures: ## x y ## target &quot;FLStock&quot; &quot;missing&quot; ## defined &quot;FLStock&quot; &quot;missing&quot; More information can be found in R’s documentation (https://www.r-project.org/). 1.4 Notation Along this chapter the notation presented in Table 1.1 will be used. Mathematical descriptions will be kept as simple as possible for readability. Table 1.1: Mathematical notation Type Symbol Description variables \\(C\\) catches \\(F\\) fishing mortality \\(M\\) natural mortality \\(R\\) recruitment \\(Q\\) vessel or fleet catchability \\(w\\) weights \\(l\\) likelihood \\(I\\) abundance index \\(S\\) spawning stock biomass \\(CV\\) coefficient of variation \\(D\\) residuals or deviances \\(N\\) normal distribution \\(\\beta\\) parameter \\(a\\) stock-recruitment parameter \\(b\\) stock-recruitment parameter \\(\\sigma^2\\) variance of catch \\(\\tau^2\\) variance of index \\(\\phi^2\\) variance of predicted recruitment \\(\\upsilon^2\\) variance of residuals subscripts \\(a\\) age \\(y\\) year \\(C\\) catch \\(I\\) abundance index \\(N\\) normal distribution \\(s\\) survey \\(SR\\) stock recruitment relationship superscripts and accents \\(\\hat{}\\) observation \\(\\tilde{}\\) prediction \\(c\\) catches \\(s\\) abundance index 1.5 Acknowledgements 1.6 License, documentation and development status The software is released under the EUPL 1.1. For more information on the a4a methodologies refer to Jardim et al. (2014), C. P. Millar et al. (2014) and Scott et al. (2016). Documentation can be found at http://flr-project.org/FLa4a. You are welcome to: Submit suggestions and bug-reports at: https://github.com/flr/FLa4a/issues Send a pull request on: https://github.com/flr/FLa4a/ Compose a friendly e-mail to the maintainer, see packageDescription('FLa4a') References "],["introduction.html", "2 Introduction 2.1 The “Assessment for All” Initiative (a4a) 2.2 Multi-stage modelling approach 2.3 Stock Assessment Process 2.4 Stock assessment as a linear model 2.5 Data used in the book", " 2 Introduction 2.1 The “Assessment for All” Initiative (a4a) The European Commission Joint Research Centre’s (JRC) “Assessment for All” Initiative (a4a) was launched to simplify and standardize the complex methodologies often employed in fisheries science, a4a focuses on creating flexible, modular frameworks that can accommodate varying data availability, regional needs, and stakeholder objectives. The JRC started its ‘Assessment for All’ Initiative (a4a), with the aim to develop, test, and distribute methods to assess a large numbers of stocks in an operational time frame, and to build the necessary expertise on stock assessment and advice provision. According to Jardim et al. (2014), the long-term strategy of a4a is to increase the number of stock assessments by reducing the workload required to run each analysis and by bringing more scientists into fisheries management advice. The first is achieved by developing a working framework with the methods required to run all the analyses a stock assessment needs. Such approach should make the model exploration and selection processes easier, as well as decreasing the burden of moving between software platforms. The second can be achieved by making the analysis more intuitive, thereby attracting more experts to join stock assessment teams. One major step to achieve the a4a goals was the development of a stock assessment model that could be applied rapidly to a large number of stocks and for a wide range of applications: traditional stock assessment, conditioning of operating models, forecasting, or informing harvest control rules in MSE algorithms. The modular nature of a4a allows for the integration of data from diverse sources, including biological, environmental, and socioeconomic datasets, ensuring comprehensive assessments. This inclusivity enhances the ability to predict stock dynamics and evaluate the impacts of fishing and environmental changes. While a4a simplifies traditional assessment approaches, it faces challenges such as ensuring the quality and consistency of input data, especially in regions with limited monitoring infrastructure. To address this, the initiative incorporates uncertainty into its models, leveraging MCMC frameworks and other statistical tools to account for variability in data quality and ecosystem processes. The a4a framework has been applied in various European fisheries to improve stock assessment practices, only in the Mediterranean Sea has been used for more than 200 stock assessments, as of 2024 in (GFCM)] and (STECF). Moreover, its use in small pelagic fisheries demonstrated the utility of simple linear models in capturing key population dynamics without the need for data-intensive methods (Jardim et al. 2014). Some of the key elements of stock assessment are the quantity, quality and aggregation level of the data available. As in many other models the data will condition the type of models that can be used. In a4a the minimum set of data, loosely defined as a “moderate data” level, consists of: volume of catches in weight (which should include landings and discards); length structure of the catches (based on selectivity studies or direct observations); natural mortality by length; proportion of mature individuals by length; age-length key or growth model; length-weight relationship; index of abundance and its length structure, or index of biomass (the type of index is left open, it could be from a scientifc survey or a commercial CPUE series); 2.2 Multi-stage modelling approach In ecological and population dynamics modeling, one can choose between integrated models, which estimate correlated parameters together, and two-stage models, which separate estimation into distinct steps. These approaches differ in complexity, data requirements, interpretability, and their ability to address uncertainties. The selection depends largely on the study objectives, available data, and the system’s ecological complexity. Integrated models estimate all parameters within a unified framework, accounting for correlations and interactions between variables such as growth, natural mortality, recruitment, and environmental factors. This approach can provide a realistic depiction of biological systems by preserving dependencies and feedback loops, which are crucial for understanding processes like density dependence or predator-prey interactions (Hamel et al. 2023). Integrated models are particularly advantageous for ecosystem-based management, where interactions among multiple factors need to be captured. However, the complexity of these models makes them computationally intensive and sensitive to data quality. On the other hand, two-stage models estimate parameters such as growth or natural mortality independently before incorporating them into broader models. This step-wise approach simplifies estimation, reducing computational demands and mitigating issues like parameter confounding. For example, fisheries often use empirical relationships to estimate natural mortality (M) based on growth parameters or life history traits before including M in stock assessment models (Maceina and Sammons 2016). However, this decoupling may overlook dynamic interactions, such as how growth influences mortality, potentially leading to biased or incomplete inferences about ecosystem dynamics (Jacobsen and Essington 2018). Dealing with uncertainty is a critical aspect of both approaches. Integrated models explicitly quantify and propagate uncertainties across correlated parameters. These models incorporate multiple sources of variability, including observation, process, and structural uncertainties, enhancing the robustness of predictions (López Quintero, Contreras-Reyes, and Wiff 2017). Conversely, two-stage models often treat parameter estimates as fixed values, which can underestimate uncertainty propagation in subsequent analyses. However, by treating first-stage estimates as distributions rather than point estimates, two-stage models can partially address this limitation. For fisheries science, the choice between these models often depends on management goals and data availability. Integrated models are better suited for forecasting fish abundance or evaluating complex ecological interactions, such as predator-prey dynamics or responses to environmental variability (Robertson, Regular, and Cadigan 2022). Meanwhile, two-stage models are practical for stock assessments, where simplicity and interpretability are prioritized. Two-stage models are advantageous for practical applications, such as fisheries stock assessments, where simplicity and robustness take precedence over ecological nuance. Empirical estimates of \\(M\\), derived from life-history traits, provide reliable inputs for subsequent models, avoiding the parameter confounding that often occurs in integrated frameworks. Despite the intuitive advantages of integrated models, it is not a panacea for poor quality data or model structure uncertainty in stock assessments. There are several disadvantages, mostly related to model misspecification, the complexity of the resulting models, and the associated, often considerable, computational requirements (e.g. the use of remotely sensed environmental information). Consequently, in some situations, the traditional two-stage approach remains a better approach (Mounder and Punt, 2013). 2.3 Stock Assessment Process The following table breaks down the stock assessment process into three stages: (i) input data preparation, (ii) stock assessment model fitting, and (iii) provision of scientific advice. This breakdown is designed to explain the a4a approach, offering a general framework that outlines the sequence of analyses in the stock assessment process. Each stage includes a plethora of analyses and procedures tailored to the specific stock, considering the available data, time, and resources. Table 2.1: Stock assessment process stages Stage Description Input data preparation Preparation of catch data, potentially separating landings and discards. Preparation of biological data, including maturity, length-weight relationships, natural mortality, and individual growth. Conversion of length data into age data. Stock assessment model fit Fitting the model to data, inspecting diagnostics such as residuals, retrospective analyses, and hindcasts. Fitting the stock-recruitment model to recruitment and spawning stock biomass (SSB) estimates from the stock assessment model fit or within the model itself. Scientific advice provision Estimation of reference points. Assessment of stock status based on reference points and model estimates of SSB and fishing mortality. Running projections under different scenarios. Providing reports with policy outcome evaluations. The main purpose of the above table is to clarify a4a’s multi-stage approach to stock assessment. For instance, parameters like natural mortality and individual growth are estimated outside the stock assessment model fitting process, unlike integrated analyses. This is done for reasons discussed earlier. Nevertheless, the stock-recruitment relationship can be estimated within the model, as is typical in integrated analyses. This mixed approach seeks to exclude highly correlated processes from the model while incorporating those that can enhance the robustness of the stock assessment model fit. When data are limited, the stock assessment model requires more structure, but this constraint reduces the information available to manage parameter correlations. Striking this balance is challenging. The a4a approach incorporates stock-recruitment relationships, as these are believed to exhibit lower correlation with other model parameters compared to natural mortality or individual growth. On the other hand, since natural mortality and individual growth are very important processes acting at a very low level in terms of data processing, there are specific methods to deal with conditioning those processes, in case there’s no data or limited data available, and to provide the opportuinity to propagate their uncertainty into stock assessment. 2.4 Stock assessment as a linear model The submodels formulation uses linear models, which opens the possibility of using the linear modelling tools available in R. For example, mgcv (mgcv2017?) gam formulas or factorial design formulas using lm(). The ‘language’ of linear models has been developing within the statistical community for many years, and constitutes an elegant way of defining models without going through the complexity of mathematical representations. This approach makes it also easier to communicate among scientists: (Nelder 1965), notation for randomized block design (Wilkinson and Rogers 1973), symbolic description for factorial designs (Hastie and Tibshirani 1990), introduced notation for smoothers (Chambers and Hastie 1991), further developed for use in S 2.5 Data used in the book 2.5.1 Plaice in area FAO 27, ICES area IV [ToDo] 2.5.2 European hake in FAO 37, GSAs 1,5,6,7 European hake is an important demersal species targeted by Mediterranean fishing fleets in several regions, mainly by bottom trawling, with In GSA 5 (Balearic Islands), bottom trawlers use different fishing tactics depending on the depth, with hake being targeted mainly on the deep shelf and the upper slope. In GSA 6, the fishery is also dominated by trawlers, with a large fleet operating on the shelf and slope and showing relatively stable catches. In GSA 7 (Gulf of Lions), hake is targeted by both French and Spanish vessels using a variety of gear types, including trawlers, gillnets and longlines. 2.5.3 Red mullet in FAO 27, GSA 1 Red mullets is a key commercial species in GSA 1.They can be found in sandy and muddy areas, and most are caught by bottom trawlers. Small scale fisheries also catch some using nets. The amount of discards reported is very low and considered to be negligible. Trawl fisheries developed along the continental shelf and upper slope. Smaller vessels operate almost exclusively on the continental shelf. Red mullet is intensively exploited during its recruitment from September to November 2.5.4 Redfish simulated length data This is a length based dataset simulated with GADGET (REF). References "],["growth.html", "3 Modelling Individual Growth and Using Stochastic Slicing to Convert Length-based Data Into Age-based Data 3.1 a4aGr - The growth class 3.2 Adding uncertainty to growth parameters with a multivariate normal distribution 3.3 Adding uncertainty to growth parameters with a multivariate triangle distribution 3.4 Adding uncertainty to growth parameters with statistical copulas 3.5 Converting from length to age based data - the l2a() method", " 3 Modelling Individual Growth and Using Stochastic Slicing to Convert Length-based Data Into Age-based Data The a4a stock assessment framework is based on age dynamics. Therefore, length information must be processed before running the stock assessment model. The methods in this section provide the analyst flexibility to use a large range of information sources, e.g. literature or online databases, to collect information about the species growth model and uncertainty about the model parameters. The framework allows the analyst to parametrize individual growth, set the assumptions about it and condition the stock assessment model on those decisions. It incentivizes the uptake of estimation uncertainty, as well as exploring several parameterizations and/or growth models to deal with structural uncertainty, finally propagating uncertainty into stock assessment. Within the a4a framework this is handled using the a4aGr class and its methods. This class stores information about the growth model and it’s parameters, including parameters’ uncertainty and the distributions governing it. The class’s main method is l2a that converts length to ages based on a length based stock object and using the model defined in the a4aGr instance. 3.1 a4aGr - The growth class The conversion of length data to age is performed through the use of a growth model. The implementation is done through the a4aGr class . showClass(&quot;a4aGr&quot;) ## Class &quot;a4aGr&quot; [package &quot;FLa4a&quot;] ## ## Slots: ## ## Name: grMod grInvMod params vcov distr name desc ## Class: formula formula FLPar array character character character ## ## Name: range ## Class: numeric ## ## Extends: &quot;FLComp&quot; To construct an a4aGr object, the growth model and parameters must be provided. Here we show an example using the von Bertalanffy growth model. To create the a4aGr object it’s necessary to pass the model equation (\\(length \\sim time\\)), the inverse model equation (\\(time \\sim length\\)) and the parameters. Any growth model can be used as long as it’s possible to write the model (and the inverse) as an R formula. vbObj &lt;- a4aGr( grMod=~linf*(1-exp(-k*(t-t0))), grInvMod=~t0-1/k*log(1-len/linf), params=FLPar(linf=58.5, k=0.086, t0=0.001, units=c(&quot;cm&quot;,&quot;year-1&quot;,&quot;year&quot;)) ) Check the model and its inverse: lc=20 predict(vbObj, len=lc) ## iter ## 1 ## 1 4.86575 predict(vbObj, t=predict(vbObj, len=lc)) ## iter ## 1 ## 1 20 The predict method allows the transformation between lengths and ages, and vice-versa, using the growth model. predict(vbObj, len=5:10+0.5) ## iter ## 1 ## 1 1.149080 ## 2 1.370570 ## 3 1.596362 ## 4 1.826625 ## 5 2.061540 ## 6 2.301299 predict(vbObj, t=5:10+0.5) ## iter ## 1 ## 1 22.04376 ## 2 25.04796 ## 3 27.80460 ## 4 30.33408 ## 5 32.65511 ## 6 34.78488 3.2 Adding uncertainty to growth parameters with a multivariate normal distribution Uncertainty in the growth model is introduced through the inclusion of parameter uncertainty. This is done by making use of the parameter variance-covariance matrix (the vcov slot of the a4aGr class) and setting a distribution for the parameters. The variance-covariance matrix could come from the parameter uncertainty from fitting the growth model parameters, or a meta analysis of correlation between parameters. Here we set the variance-covariance matrix by scaling a correlation matrix, using a cv of 0.2. Based on \\[\\rho_{x,y}=\\frac{\\Sigma_{x,y}}{\\sigma_x \\sigma_y}\\] and \\[CV_x=\\frac{\\sigma_x}{\\mu_x}\\] # Make an empty cor matrix cm &lt;- diag(c(1,1,1)) # k and linf are negatively correlated while t0 is independent cm[1,2] &lt;- cm[2,1] &lt;- -0.5 # scale cor to var using CV=0.2 cv &lt;- 0.2 p &lt;- c(linf=60, k=0.09, t0=-0.01) vc &lt;- matrix(1, ncol=3, nrow=3) l &lt;- vc l[1,] &lt;- l[,1] &lt;- p[1]*cv k &lt;- vc k[,2] &lt;- k[2,] &lt;- p[2]*cv t &lt;- vc t[3,] &lt;- t[,3] &lt;- p[3]*cv mm &lt;- t*k*l diag(mm) &lt;- diag(mm)^2 mm &lt;- mm*cm # check that we have the intended correlation all.equal(cm, cov2cor(mm)) ## [1] TRUE Create the a4aGr object as before but now we also include the vcov argument for the variance-covariance matrix. vbObj &lt;- a4aGr( grMod=~linf*(1-exp(-k*(t-t0))), grInvMod=~t0-1/k*log(1-len/linf), params=FLPar( linf=p[&quot;linf&quot;], k=p[&quot;k&quot;], t0=p[&quot;t0&quot;], units=c(&quot;cm&quot;,&quot;year-1&quot;,&quot;year&quot;)), vcov=mm ) First we show a simple example where we assume that the parameters are represented using a multivariate normal distribution. Note that the object we have just created has a single iteration of each parameter. vbObj@params ## An object of class &quot;FLPar&quot; ## params ## linf k t0 ## 60.00 0.09 -0.01 ## units: cm year-1 year dim(vbObj@params) ## [1] 3 1 We simulate 250 iterations from the a4aGr object by calling mvrnorm() using the variance-covariance matrix we created earlier. The object will now have 250 iterations of each parameter, randomly sampled from the multivariate normal distribution. vbNorm &lt;- mvrnorm(250,vbObj) vbNorm@params ## An object of class &quot;FLPar&quot; ## iters: 250 ## ## params ## linf k t0 ## 59.7757667(11.16855) 0.0908875( 0.01768) -0.0098667( 0.00201) ## units: cm year-1 year dim(vbNorm@params) ## [1] 3 250 We can now convert from length to ages data based on the 250 parameter iterations, which will produce 250 sets of age data. For example, the next code will convert a single length vector using each of the 250 parameter iterations. lvec &lt;- 5:10+0.5 ages &lt;- predict(vbNorm, len=lvec) dim(ages) ## [1] 6 250 The marginal distributions of the parameters can be seen in Figure 3.1. Figure 3.1: The marginal distributions of each of the parameters from using a multivariate normal distribution. Pairwise plots show the covariance between each pair of parameters and the shape of their correlation (Figure 3.2). Figure 3.2: Scatter plot of the 10000 samples parameter from the multivariate normal distribution. Using the new generated age vectors one can depict the growth curves for the 250 iterations, which displays individual growth uncertainty (Figure 3.3). Figure 3.3: Growth curves using parameters simulated from a multivariate normal distribution. 3.3 Adding uncertainty to growth parameters with a multivariate triangle distribution One alternative to using a normal distribution is to use a triangle distribution. We use the package triangle (Carnell 2022) where this distribution is parametrized using the minimum, maximum and median values. This can be very attractive if the analyst needs to scrape information from the web or literature, and use a meta-analysis to build the parameters’ distribution. The triangle distribution has the advantage of setting hard tail limits, avoiding to generate extreme values. Here we show an example of setting a triangle distribution with values taken from Fishbase (Froese and Pauly 2000). The following shows a method to extract data from fishbase. However, due to potential changes in the way one gets data from fishbase from whithin R, we’ve downloaded the data beforehand and load it for this example. # The web address for the growth parameters for redfish (Sebastes norvegicus) addr &lt;- &quot;https://fishbase.se/PopDyn/PopGrowthList.php?ID=501&quot; # Scrape the data tab &lt;- try(readHTMLTable(addr)) # Interrogate the data table and get vectors of the values linf &lt;- as.numeric(as.character(tab$dataTable[,2])) k &lt;- as.numeric(as.character(tab$dataTable[,4])) t0 &lt;- as.numeric(as.character(tab$dataTable[,5])) # Set the min (a), max (b) and median (c) values for the parameter as a list of lists # Note that t0 has no &#39;c&#39; (median) value. This makes the distribution symmetrical triPars &lt;- list( linf=list(a=min(linf), b=max(linf), c=median(linf)), k=list(a=min(k), b=max(k), c=median(k)), t0=list(a=median(t0, na.rm=T)-IQR(t0, na.rm=T)/2, b=median(t0, na.rm=T)+IQR(t0, na.rm=T)/2)) # Draw 250 samples using mvrtriangle vbTri &lt;- mvrtriangle(250, vbObj, paramMargins=triPars) Note that in this case we’re not building a new object with all the parameters’ information. We’re using the argument paramMargins to pass the parameters’ information to the method. The marginals will reflect the uncertainty on the parameter values that were scraped from Froese and Pauly (2000) but, as we don’t really believe the parameters are multivariate normal, here we adopted a distribution based on a t copula with triangle marginals. The marginal distributions can be seen in Figure 3.4 and the shape of the correlation can be seen in Figure 3.5. Figure 3.4: The marginal distributions of each of the parameters from using a multivariate triangle distribution. Figure 3.5: Scatter plot of the 10000 samples parameter from the multivariate triangle distribution. We can still use predict() to see the growth model uncertainty (Figure 3.6). Comparing with Figure 3.3 one can see that using triangle distribution generates a lot less outliers, or values outside the central range of the growth curve. Figure 3.6: Growth curves using parameters simulated from a multivariate triangle distribution. Remember that the above examples use a variance-covariance matrix that we essentially made up. An alternative would be to scrape the entire growth parameters dataset from Fishbase and compute the shape of the variance-covariance matrix yourself. 3.4 Adding uncertainty to growth parameters with statistical copulas A more general approach to adding parameter uncertainty is to make use of statistical copulas (Sklar 1959). Genest, Okhrin, and Bodnar (2024) describes statistical “copula” as a multivariate cumulative distribution function with uniform margins on the unit interval. Sklar (1959) highlighted the fact that any multivariate distribution can be expressed as a function of its margins and a copula. The idea is very actractive, one can simulate any multivariate distribution by setting a multivariate function in the unit interval which describes how the margins relate to each other, and scale up the univariate uniform margin with any continuos univariate distribution. In our case this is possible with the mvrcop() function, borrowed from the package copula (Jun Yan 2007). The example below keeps the same parameters and changes only the copula type and family but a lot more can be done. Check the package copula for more information. vbCop &lt;- mvrcop(250, vbObj, copula=&quot;archmCopula&quot;, family=&quot;clayton&quot;, param=2, margins=&quot;triangle&quot;, paramMargins=triPars) The shape of the correlation as well as the resulting growth curves are shown in Figures 3.7 and 3.8. Figure 3.7: Scatter plot of the 250 samples parameter from the using an archmCopula copula with triangle margins. Figure 3.8: Growth curves using parameters simulated from an archmCopula copula with triangle margins. 3.5 Converting from length to age based data - the l2a() method After introducing uncertainty in the growth model through the parameters it’s time to transform the length-based dataset into an age-based dataset. The method that deals with this process is l2a(). The implementation of this method for the FLQuant class is the main workhorse. There are two other implementations, for the FLStock and FLIndex classes, which are mainly wrappers that call the FLQuant method several times. When converting from length-based data to age-based data you need to be aware of how the aggregation of length classes is performed. For example, individuals in length classes 1-2, 2-3, and 3-4 cm may all be considered as being of age 1 (obviously depending on the growth model). How should the values in those length classes be combined? If the values are abundances then the values should be summed. Summing other types of values, such as mean weight, does not make sense. Instead these values are averaged over the length classes (possibly weighted by the abundance). This is controlled using the stat argument which can be either mean or sum (the default). Fishing mortality is not computed to avoid making wrong assumptions about the meaning of F at length. We demonstrate the method by converting a catch-at-length FLQuant to a catch-at-age FLQuant. First we make an a4aGr object with a multivariate triangle distribution using parameters extracted from an AI agent. We use 10 iterations as an example, and call l2a() by passing in the length-based FLQuant and the a4aGr object. triPars &lt;- list( linf=list(a=55, b=60), k=list(a=0.05, b=0.06), t0=list(a=-3, b=-2)) # Draw 10 samples using mvrtriangle vbTriSmall &lt;- mvrtriangle(10, vbObj, paramMargins=triPars) # slice catch numbers at lengths to ages by summing catches cth.n &lt;- l2a(catch.n(rfLen.stk), vbTriSmall) # note there&#39;s a lot of 0 catches so we&#39;ll set the plus group at 21 cth.n &lt;- setPlusGroup(cth.n, 21) # there&#39;s also negative ages. The simulated data included individuals in lengths that won&#39;t show in the catches, like 1 cm. We&#39;ll trim those ages cth.n &lt;- cth.n[ac(0:21)] # slice catch weights at lengths to ages by averaging catches cth.wt &lt;- l2a(catch.wt(rfLen.stk), vbTriSmall, stat=&quot;mean&quot;) # same process to deal with negative ages cth.wt &lt;- cth.wt[ac(0:21)] In the previous example, the FLQuant object that was sliced (catch.n(rfLen.stk)) had only one iteration. This iteration was sliced by each of the iterations in the growth model. It is possible for the FLQuant object to have the same number of iterations as the growth model, in which case each iteration of the FLQuant and the growth model are used together. It is also possible for the growth model to have only one iteration while the FLQuant object has many iterations. The same growth model is then used for each of the FLQuant iterations. As with all FLR objects, the general rule is one or n iterations. As well as converting one FLQuant at a time, we can convert entire FLStock and FLIndex objects. In these cases the individual FLQuant slots of those classes are converted from length-based to age-based. As mentioned above, the aggregation method depends on the type of values the slots contain. The abundance slots (*.n, such as stock.n) are summed. The *.wt, m, mat, harvest.spwn and m.spwn slots of an FLStock object are averaged. The catch.wt and sel.pattern slots of an FLIndex object are averaged, while the index, index.var and catch.n slots are summed. The method for FLStock classes takes an additional argument for the plusgroup. aStk &lt;- l2a(rfLen.stk, vbTriSmall, plusgroup=21) ## [1] &quot;maxfbar has been changed to accomodate new plusgroup&quot; aIdx &lt;- l2a(rfTrawl.idx, vbTriSmall) When converting with l2a() all lengths above Linf are converted to the maximum age, as there is no information in the growth model about how to deal with individuals larger than Linf. References "],["modelling-natural-mortality.html", "4 Modelling Natural Mortality 4.1 a4aM - The M class 4.2 Adding uncertainty to natural mortality parameters with a multivariate normal distribution 4.3 Adding uncertainty to natural mortality parameters with statistical copulas 4.4 Computing natural mortality time series - the “m” method", " 4 Modelling Natural Mortality Natural mortality (\\(M\\)) is a critical parameter in stock assessment models, representing all sources of mortality not related to fishing or harvest (\\(F\\)). Combined, these two sources constitute the total mortality (\\(Z\\)) that individuals experience, with \\(Z = F + M\\). However, natural mortality is notoriously difficult to observe and estimate. Only a few methods, such as mark-recapture studies, provide direct estimates of \\(M\\). However, these methods are not applicable to all species and are often costly. As an alternative, life-history theory is commonly used to derive values for \\(M\\) that are consistent with individual growth and reproduction. There is an extensive body of literature on natural mortality. Works by Pauly (1980), Gislason et al. (2010), Charnov (1993), Maunder et al. (2023) and Quinn and Deriso (1999), as well as a dedicated special issue in Fisheries Research (Hamel et al. 2023), offer valuable insights and serve as excellent starting points. Given the parameter’s importance and the difficulty of obtaining direct observations, natural mortality is widely regarded as one of the most significant sources of uncertainty in stock assessments. Within the a4a framework, natural mortality is treated as an externally fixed parameter in the stock assessment model. Our aim is to develop a system that enables analysts to explore alternative models for \\(M\\) and compare the resulting assessment outcomes. This approach provides a more comprehensive information base to support informed decision-making throughout the stock assessment process. Within the a4a framework, the general method for adding natural mortality in the stock assessment model is to: Create an object of class a4aM which holds the natural mortality model and parameters. Add uncertainty to the parameters in the a4aM object. Apply the m() method to the a4aM object to create an age or length based FLQuant object of the required dimensions. The resulting FLQuant object can then be directly inserted into an FLStock object to be used for the assessment. In this section we go through each of the steps in detail using a variety of different models. 4.1 a4aM - The M class Natural mortality is implemented in a class named a4aM. This class is made up of three objects of the class FLModelSim. Each object is a model that represents one effect: an age or length effect, a scaling (level) effect and a time trend, named shape, level and trend, respectively. The impact of the models is multiplicative, i.e. the overal natural mortality is given by shape x level x trend. showClass(&quot;a4aM&quot;) ## Class &quot;a4aM&quot; [package &quot;FLa4a&quot;] ## ## Slots: ## ## Name: shape level trend name desc range ## Class: FLModelSim FLModelSim FLModelSim character character numeric ## ## Extends: &quot;FLComp&quot; The a4aM constructor requires that the models and parameters are provided. The default method will build each of these models as a constant value of 1. As a simple example, the usual “0.2” guessestimate could be set up by setting the level model to have a single parameter with a fixed value, while the other two models, shape and trend, have a default value of 1 (meaning that they have no effect). mod02 &lt;- FLModelSim(model=~a, params=FLPar(a=0.2)) m1 &lt;- a4aM(level=mod02) m1 ## a4aM object: ## shape: ~1 ## level: ~a ## trend: ~1 More interesting natural mortality shapes can be set up using biological knowledge. The following example uses an exponential decay over ages (implying that the resulting FLQuant} generated by the m() method will be age based). We also use Jensen’s second estimator (Kenchington 2014) as a scaling level model, which is based on the von Bertalanffy \\(K\\) parameter, \\(M=1.5K\\). shape2 &lt;- FLModelSim(model=~exp(-age-0.5)) level2 &lt;- FLModelSim(model=~1.5*k, params=FLPar(k=0.4)) m2 &lt;- a4aM(shape=shape2, level=level2) m2 ## a4aM object: ## shape: ~exp(-age - 0.5) ## level: ~1.5 * k ## trend: ~1 Note that the shape model has age as a parameter of the model but is not set using the params argument. The shape model does not have to be age-based. For example, here we set up a shape model using Gislason’s second estimator (Kenchington 2014): \\(M_l=K(\\frac{L_{\\inf}}{l})^{1.5}\\). We use the default level and trend models. shape_len &lt;- FLModelSim(model=~K*(linf/len)^1.5, params=FLPar(linf=60, K=0.4)) m_len &lt;- a4aM(shape=shape_len) Another option is to model how an external factor may impact natural mortality. This can be added through the trend model. Suppose natural mortality can be modelled with a dependency on the NAO index, due to some mechanism that results in having lower mortality when NAO is negative and higher when it’s positive. In this example, the impact is represented by the NAO value on the quarter before spawning, which occurs in the second quarter. We use this to make a complex natural mortality model with an age based shape model, a level model based on \\(K\\) and a trend model driven by NAO, where mortality increases by 50% if NAO is positive on the first quarter. # Get NAO nao &lt;- read.table(&quot;https://www.cpc.ncep.noaa.gov/products/precip/CWlink/pna/norm.nao.monthly.b5001.current.ascii.table&quot;, skip=1, fill=TRUE, na.strings=&quot;-99.90&quot;) dnms &lt;- list(quant=&quot;nao&quot;, year=1950:2024, unit=&quot;unique&quot;, season=1:12, area=&quot;unique&quot;) # Build an FLQuant from the NAO data nao.flq &lt;- FLQuant(unlist(nao[,-1]), dimnames=dnms, units=&quot;nao&quot;) # Build covar by calculating mean over the first 3 months nao &lt;- seasonMeans(trim(nao.flq, year=dimnames(stock.n(ple4))$year)) # Turn into Boolean nao &lt;- (nao&gt;0) # Constructor trend3 &lt;- FLModelSim(model=~1+b*nao, params=FLPar(b=0.5)) shape3 &lt;- FLModelSim(model=~exp(-age-0.5)) level3 &lt;- FLModelSim(model=~1.5*k, params=FLPar(k=0.4)) m3 &lt;- a4aM(shape=shape3, level=level3, trend=trend3) m3 ## a4aM object: ## shape: ~exp(-age - 0.5) ## level: ~1.5 * k ## trend: ~1 + b * nao 4.2 Adding uncertainty to natural mortality parameters with a multivariate normal distribution Uncertainty on natural mortality is added through uncertainty on the parameters. In this section we’ll’ show how to add multivariate normal uncertainty. We make use of the class FLModelSim method mvrnorm(), which is a wrapper for the method mvrnorm() distributed by the package MASS (Venables and Ripley 2002). We’ll create an a4aM object with an exponential shape, a level model based on \\(k\\) and temperature, Jensen’s third estimator (Kenchington 2014), and a trend model driven by the NAO (as above). Afterwards a variance-covariance matrix for the level and trend models will be included. Finally, create an object with 100 iterations using the mvrnorm() method. Create the object: shape4 &lt;- FLModelSim(model=~exp(-age-0.5)) level4 &lt;- FLModelSim(model=~k^0.66*t^0.57, params=FLPar(k=0.4, t=10), vcov=array(c(0.002, 0.01,0.01, 1), dim=c(2,2))) trend4 &lt;- FLModelSim(model=~1+b*nao, params=FLPar(b=0.5), vcov=matrix(0.02)) m4 &lt;- a4aM(shape=shape4, level=level4, trend=trend4) # Call mvrnorm() m4 &lt;- mvrnorm(100, m4) m4 ## a4aM object: ## shape: ~exp(-age - 0.5) ## level: ~k^0.66 * t^0.57 ## trend: ~1 + b * nao Inspect the level model (for example): level(m4) ## An object of class &quot;FLModelSim&quot; ## Slot &quot;model&quot;: ## ~k^0.66 * t^0.57 ## ## Slot &quot;params&quot;: ## An object of class &quot;FLPar&quot; ## iters: 100 ## ## params ## k t ## 0.3961(0.0493) 9.8284(1.0785) ## units: NA ## ## Slot &quot;vcov&quot;: ## [,1] [,2] ## [1,] 0.002 0.01 ## [2,] 0.010 1.00 ## ## Slot &quot;distr&quot;: ## [1] &quot;norm&quot; Note the variance in the parameters: params(trend(m4)) ## An object of class &quot;FLPar&quot; ## iters: 100 ## ## params ## b ## 0.48382(0.113) ## units: NA Note the shape model has no parameters and no uncertainty: params(shape(m4)) ## An object of class &quot;FLPar&quot; ## params ## ## NA ## units: NA In this particular case, the shape model will not be randomized because it doesn’t have a variance-covariance matrix. Also note that because there is only one parameter in the trend model, the randomization will use a univariate normal distribution. The same model could be achieved by using mnrnorm() on each model component: m4 &lt;- a4aM(shape=shape4, level=mvrnorm(100, level4), trend=mvrnorm(100, trend4)) an exact match would require to control the random seed so that the draws would be exactly the same. 4.3 Adding uncertainty to natural mortality parameters with statistical copulas We can also use copulas to add parameter uncertainty to the natural mortality model, similar to the way we use them for the growth model in Section 3. As stated above these processes make use of the methods implemented for the FLModelSim class. In the following example we’ll use again Gislason’s second estimator, \\(M_l=K(\\frac{L_{\\inf}}{l})^{1.5}\\) and a triangle copula to model parameter uncertainty. The method mvrtriangle() is used to create 1000 iterations. linf &lt;- 60 k &lt;- 0.4 # vcov matrix (make up some values) mm &lt;- matrix(NA, ncol=2, nrow=2) # 10% cv diag(mm) &lt;- c((linf*0.1)^2, (k*0.1)^2) # 0.2 correlation mm[upper.tri(mm)] &lt;- mm[lower.tri(mm)] &lt;- c(0.05) # a good way to check is using cov2cor cov2cor(mm) ## [,1] [,2] ## [1,] 1.0000000 0.2083333 ## [2,] 0.2083333 1.0000000 # create object mgis2 &lt;- FLModelSim(model=~k*(linf/len)^1.5, params=FLPar(linf=linf, k=k), vcov=mm) # set the lower, upper and centre of the parameters pars &lt;- list(list(a=55,b=65), list(a=0.3, b=0.6, c=0.35)) mgis2 &lt;- mvrtriangle(1000, mgis2, paramMargins=pars) mgis2 ## An object of class &quot;FLModelSim&quot; ## Slot &quot;model&quot;: ## ~k * (linf/len)^1.5 ## ## Slot &quot;params&quot;: ## An object of class &quot;FLPar&quot; ## iters: 1000 ## ## params ## linf k ## 60.0279(2.1735) 0.4054(0.0699) ## units: NA ## ## Slot &quot;vcov&quot;: ## [,1] [,2] ## [1,] 36.00 0.0500 ## [2,] 0.05 0.0016 ## ## Slot &quot;distr&quot;: ## [1] &quot;un &lt;deprecated slot&gt; triangle&quot; The resulting parameter estimates and marginal distributions can be seen in Figures 4.1 and 4.2. By default the method uses an elliptical copula of t family (see ?ellipCopula for more information). Figure 4.1: Pairwise depiction of Gislason’s second natural mortality model estimates using a ‘t’ family elliptic copula and triangle distribution margins. Figure 4.2: Marginal distributions of the parameters for Gislason’s second natural mortality model using triangle distributions. We now have a new model that can be used for the shape model. You can use the constructor or the set method to add the new model. Note that we have a quite complex method now for M. A length based shape model from Gislason’s work, Jensen’s third model based on temperature level and a time trend depending on NAO. All of the component models have uncertainty in their parameters. m5 &lt;- a4aM(shape=mgis2, level=level4, trend=trend4) # or m5 &lt;- m4 shape(m5) &lt;- mgis2 4.4 Computing natural mortality time series - the “m” method Now that we have set up the natural mortality a4aM model and added parameter uncertainty to each component, we are ready to generate the FLQuant of natural mortality with m(). The m() method is the workhorse method for computing natural mortality. The method returns a FLQuant that can be inserted in an FLStock to be used in the assessment method. The size of the FLQuant object is determined by the min, max, minyear and maxyear elements of the range slot of the a4aM object. By default the values of these elements are set to 0, which generates a FLQuant with length 1 in the quant and year dimension. The range slot can be set by hand, or by using the rngquant() and rngyear() methods. The name of the first dimension of the output FLQuant (e.g. ‘age’ or ‘len’) is determined by the parameters of the shape model. If it is not clear what the name should be then the name is set to ‘quant’. Here we demonstrate m() using the simple a4aM object we created above that has constant natural mortality. Start with the simplest model: m1 ## a4aM object: ## shape: ~1 ## level: ~a ## trend: ~1 Check the range: range(m1) ## min max plusgroup minyear maxyear minmbar maxmbar ## 0 0 0 0 0 0 0 The \\(M\\) FLQuant won’t have ages or years: m(m1) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## quant 0 ## 0 0.2 ## ## units: NA To have a more useful matrix of values that cover the ages and years in the FLStock object, the analyst needs to set the quant and year ranges. # set the quant range range(m1, c(&quot;min&quot;,&quot;max&quot;)) &lt;- c(0,7) # set the year range range(m1, c(&quot;minyear&quot;,&quot;maxyear&quot;)) &lt;- c(2000, 2010) range(m1) ## min max plusgroup minyear maxyear minmbar maxmbar ## 0 7 0 2000 2010 0 0 Create the object with the M estimates by age and year, note the name of the first dimension is ‘quant’. m(m1) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## quant 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 ## 0 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## 1 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## 2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## 3 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## 4 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## 5 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## 6 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## 7 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## ## units: NA The next example has an age-based shape (model “m2” from above). As the shape model has ‘age’ as a variable which is not included in the FLPar slot it is used as the name of the first dimension of the resulting FLQuant. An important feature of the m() method is the use of the level model. The outcome of the level model will be applied to a range of ages or lengths, set by the mbar information in the range slot. In this example the level model is \\(1.5*K\\) and since \\(K=0.4\\), the level predicted by the model will be \\(0.6\\). The m() model will use the information in the range, minmbar and maxmbar to compute the mean level. This mean level will match the value given by the level model. The mbar range can be changed with the rngmbar() method. We illustrate this by making an FLQuant with age varying natural mortality. Check the model and set the ranges: m2 ## a4aM object: ## shape: ~exp(-age - 0.5) ## level: ~1.5 * k ## trend: ~1 # set the quant range range(m2, c(&quot;min&quot;,&quot;max&quot;)) &lt;- c(0,7) # set the year range range(m2, c(&quot;minyear&quot;,&quot;maxyear&quot;)) &lt;- c(2000, 2003) range(m2) ## min max plusgroup minyear maxyear minmbar maxmbar ## 0 7 0 2000 2003 0 0 m(m2) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 2000 2001 2002 2003 ## 0 0.600000 0.600000 0.600000 0.600000 ## 1 0.220728 0.220728 0.220728 0.220728 ## 2 0.081201 0.081201 0.081201 0.081201 ## 3 0.029872 0.029872 0.029872 0.029872 ## 4 0.010989 0.010989 0.010989 0.010989 ## 5 0.004043 0.004043 0.004043 0.004043 ## 6 0.001487 0.001487 0.001487 0.001487 ## 7 0.000547 0.000547 0.000547 0.000547 ## ## units: NA Note that the level value is: predict(level(m2)) ## iter ## 1 ## 1 0.6 Which is the same as: m(m2)[&quot;0&quot;] ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 2000 2001 2002 2003 ## 0 0.6 0.6 0.6 0.6 ## ## units: NA This is because the mbar range is currently set to “0” and “0” (see above) and the mean natural mortality value over this range is given by the level model. We can change the mbar range: range(m2, c(&quot;minmbar&quot;,&quot;maxmbar&quot;)) &lt;- c(0,5) range(m2) ## min max plusgroup minyear maxyear minmbar maxmbar ## 0 7 0 2000 2003 0 5 Which rescales the the natural mortality at age: m(m2) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 2000 2001 2002 2003 ## 0 2.28129 2.28129 2.28129 2.28129 ## 1 0.83924 0.83924 0.83924 0.83924 ## 2 0.30874 0.30874 0.30874 0.30874 ## 3 0.11358 0.11358 0.11358 0.11358 ## 4 0.04178 0.04178 0.04178 0.04178 ## 5 0.01537 0.01537 0.01537 0.01537 ## 6 0.00565 0.00565 0.00565 0.00565 ## 7 0.00208 0.00208 0.00208 0.00208 ## ## units: NA Check that the mortality over the mean range is the same as the level model: quantMeans(m(m2)[ac(0:5)]) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 2000 2001 2002 2003 ## all 0.6 0.6 0.6 0.6 ## ## units: NA The next example uses a time trend for the trend model. We use the m3 model we made earlier. The trend model for this model has a covariate, ‘nao’. This needs to be passed to the m() method. The year range of the ‘nao’ covariate should match that of the range slot. Simple, pass in a single nao value (only one year): m(m3, nao=1) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 0 ## 0 0.9 ## ## units: NA Set ages: range(m3, c(&quot;min&quot;,&quot;max&quot;)) &lt;- c(0,7) m(m3, nao=0) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 0 ## 0 0.600000 ## 1 0.220728 ## 2 0.081201 ## 3 0.029872 ## 4 0.010989 ## 5 0.004043 ## 6 0.001487 ## 7 0.000547 ## ## units: NA With ages and years - passing in the NAO data as numeric (1,0,1,0) range(m3, c(&quot;minyear&quot;,&quot;maxyear&quot;)) &lt;- c(2000, 2003) m(m3, nao=as.numeric(nao[,as.character(2000:2003)])) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 2000 2001 2002 2003 ## 0 0.600000 0.900000 0.600000 0.900000 ## 1 0.220728 0.331091 0.220728 0.331091 ## 2 0.081201 0.121802 0.081201 0.121802 ## 3 0.029872 0.044808 0.029872 0.044808 ## 4 0.010989 0.016484 0.010989 0.016484 ## 5 0.004043 0.006064 0.004043 0.006064 ## 6 0.001487 0.002231 0.001487 0.002231 ## 7 0.000547 0.000821 0.000547 0.000821 ## ## units: NA The final example show how m() can be used to make an FLQuant with uncertainty (see Figure 4.3). We use the m4 object from earlier with uncertainty on the level and trend parameters. range(m4, c(&quot;min&quot;,&quot;max&quot;)) &lt;- c(0,7) range(m4, c(&quot;minyear&quot;,&quot;maxyear&quot;)) &lt;- c(2000, 2003) flq &lt;- m(m4, nao=c(nao[,ac(2000:2003)])) flq ## An object of class &quot;FLQuant&quot; ## iters: 100 ## ## , , unit = unique, season = all, area = unique ## ## year ## age 2000 2001 2002 2003 ## 0 2.05289(0.180266) 3.07810(0.408376) 2.05289(0.180266) 3.07810(0.408376) ## 1 0.75521(0.066316) 1.13237(0.150233) 0.75521(0.066316) 1.13237(0.150233) ## 2 0.27783(0.024396) 0.41658(0.055268) 0.27783(0.024396) 0.41658(0.055268) ## 3 0.10221(0.008975) 0.15325(0.020332) 0.10221(0.008975) 0.15325(0.020332) ## 4 0.03760(0.003302) 0.05638(0.007480) 0.03760(0.003302) 0.05638(0.007480) ## 5 0.01383(0.001215) 0.02074(0.002752) 0.01383(0.001215) 0.02074(0.002752) ## 6 0.00509(0.000447) 0.00763(0.001012) 0.00509(0.000447) 0.00763(0.001012) ## 7 0.00187(0.000164) 0.00281(0.000372) 0.00187(0.000164) 0.00281(0.000372) ## ## units: NA dim(flq) ## [1] 8 4 1 1 1 100 Figure 4.3: Natural mortality with age and year trend. Notably, the last example created a M model that varies with time, based on an environmental variable, and adds estimation uncertainty, showing the huge flexibility this method can deal with. References "],["stock-assessment-framework.html", "5 Stock assessment framework 5.1 Maths description 5.2 Model structure 5.3 Submodel building blocks and fundamental R formulas 5.4 The major effects available for modelling 5.5 Classes Description ", " 5 Stock assessment framework 5.1 Maths description The stock assessment model is based on two types of observations: catches, \\(\\hat{C}\\), and abundance indices, \\(\\hat{I}\\). The model predicts catches at age \\(C_{ay}\\) and indices of abundance \\(I_{ays}\\) for each age \\(a\\), year \\(y\\) and survey \\(s\\) in the input dataset. To predict catches and survey indices, the model uses the standard population dynamics model \\[\\begin{equation} N_{a+1,y+1} = N_{ay} \\exp \\left( - F_{ay} - M_{ay} \\right) \\end{equation}\\] where \\(N_{ay}\\) is the number of individuals at age \\(a\\) in year \\(y\\), \\(F_{ay}\\) is the fishing mortality at age \\(a\\) in year \\(y\\), and \\(M_{ay}\\) is the natural mortality at age \\(a\\) in year \\(y\\). Any fish that survived beyond the oldest age \\(A\\) in the model are accumulated in the oldest age group and are assumed to be fished at a common rate \\(F_{A,y}\\). \\[\\begin{align} N_{A,y+1} = &amp;N_{A-1,y} \\exp \\left( - F_{A-1,y} - M_{A-1,y} \\right) \\\\ &amp;+ N_{A,y} \\exp \\left( - F_{A,y} - M_{A,y} \\right) \\end{align}\\] The numbers \\(N_{a,y}\\) are initiated in the first year, \\(y=1\\) and at the youngest age, \\(a=1\\), and the matrix of numbers at age are filled in according to the population dynamics model stated above (see Figure 5.1). Figure 5.1: Statistical catch at age population dynamics model Defining \\(R_y = N_{1,y}\\), the numbers at age can be written (ignoring the plus group) as: \\[\\begin{equation} N_{a,y} = \\left\\{ \\begin{matrix} R_{y-a+1} \\exp \\left( - \\sum^a_{i=1} F_{a-i,y-i} + M_{a-i,y-i} \\right) &amp; y \\geq a \\\\ N_{a-y+1,1} \\exp \\left( - \\sum^{a-y}_{i=1} F_{a-i,y-i} + M_{a-i,y-i} \\right) &amp; y \\lt a \\end{matrix} \\right. \\end{equation}\\] Catches in numbers by age and year are defined in terms of the three quantities: natural mortality, fishing mortality and recruitment; using a modified form of the well known Baranov catch equation: \\[\\begin{equation} C_{ay} = \\frac{F_{ay}}{F_{ay}+M_{ay}}\\left(1 - e^{-(F_{ay}+M_{ay})}\\right) R_{y}e^{-\\sum (F_{ay} + M_{ay})} \\end{equation}\\] Survey indices by age and year are defined in terms of the same three quantities with the addition of survey catchability: \\[\\begin{equation} I_{ays} = Q_{ays} R_{y}e^{-\\sum (F_{ay} + M_{ay})} \\end{equation}\\] It is assumed that the observed catches are normally distributed about the model predictions on the log scale with observation variance \\(\\sigma^2_{ay}\\). that is: \\[\\begin{equation} \\log \\hat{C}_{ay} \\sim \\text{Normal} \\Big( \\log C_{ay}, \\sigma^2_{ay}\\Big) \\end{equation}\\] \\[\\begin{equation} \\log \\hat{I}_{ays} \\sim \\text{Normal} \\Big( \\log I_{ays}, \\tau^2_{ays} \\Big) \\end{equation}\\] The log-likelihood can now be defined as the sum of the log-likelihood of the observed catches: \\[\\begin{equation} \\ell_C = \\sum_{ay} w^{(c)}_{ay}\\ \\ell_N \\Big( \\log C_{ay}, \\sigma^2_{ay} ;\\ \\log \\hat{C}_{ay} \\Big) \\end{equation}\\] and the log-likelihood of the observed survey indices as: \\[\\begin{equation} \\ell_I = \\sum_s \\sum_{ay} w^{(s)}_{ays}\\ \\ell_N \\Big( \\log I_{ays}, \\tau_{ays}^2 ;\\ \\log \\hat{I}_{ays} \\Big) \\end{equation}\\] giving the total log-likelihood \\[\\begin{equation} \\ell = \\ell_C + \\ell_I \\end{equation}\\] which is defined in terms of the strictly positive quantites, \\(M_{ay}\\), \\(F_{ay}\\), \\(Q_{ays}\\) and \\(R_{y}\\), and the observation variances \\(\\sigma_{ay}\\) and \\(\\tau_{ays}\\). As such, the log-likelihood is over-parameterised as there are many more parameters than observations. In order to reduce the number of parameters, \\(M_{ay}\\) is assumed known (as is common). The remaining parameters are written in terms of a linear combination of covariates \\(x_{ayk}\\), e.g. \\[\\begin{equation} \\log F_{ay} = \\sum_k \\beta_k x_{ayk} \\end{equation}\\] where \\(k\\) is the number of parameters to be estimated and is sufficiently small. Using this tecnique the quantities \\(\\log F\\), \\(\\log Q\\), \\(\\log \\sigma\\) and \\(\\log \\tau\\) %\\(\\log \\text{initial\\,age\\,structure}\\) % this is not present in the above (in bold in the equations above) can be described by a reduced number of parameters. The following section has more discussion on the use of linear models in a4a. The a4a statistical catch-at-age model can addionally allow for a functional relationship that links predicted recruitment \\(\\tilde{R}\\) based on spawning stock biomass and modelled recruitment \\(R\\), to be imposed as a fixed variance random effect. [NEEDS REVISION, sentence not clear] Options for the relationship are the hard coded models Ricker, Beverton Holt, smooth hockeystick or geometric mean. This is implemented by including a third component in the log-likelihood: \\[\\begin{equation} \\ell_{SR} = \\sum_y \\ell_N \\Big( \\log \\tilde{R}_y(a, b), \\phi_y^2 ;\\ \\log R_y \\Big) \\end{equation}\\] giving the total log-likelihood \\[\\begin{equation} \\ell = \\ell_C + \\ell_I + \\ell_{SR} \\end{equation}\\] Using the (time varying) Ricker model as an example, predicted recruitment is \\[\\begin{equation} \\tilde{R}_y(a_y,b_y) = a_y S_{y-1} e^{-b_y S_{y-1}} \\end{equation}\\] where \\(S\\) is spawning stock biomass derived from the model parameters \\(F\\) and \\(R\\), and the fixed quantites \\(M\\) and mean weights by year and age. It is assumed that \\(R\\) is log-normally distributed, or equivalently, normally distributed on the log-scale about the (log) recruitment predicted by the SR model \\(\\tilde{R}\\), with known variance \\(\\phi^2\\), i.e. \\[\\begin{equation} \\log R_y \\sim \\text{Normal} \\Big( \\log \\tilde{R}_y, \\phi_y^2 \\Big) \\end{equation}\\] which leads to the definition of \\(\\ell_{SR}\\) given above. In all cases \\(a\\) and \\(b\\) are strictly positive, and with the quantities \\(F\\), \\(R\\), etc. linear models are used to parameterise \\(\\log a\\) and/or \\(\\log b\\), where relevant. By default, recruitment \\(R\\) as apposed to the reruitment predicted from a stock recruitment model \\(\\tilde{R}\\), is specified as a linear model with a parameter for each year, i.e. \\[\\begin{equation} \\log R_y = \\gamma_y \\end{equation}\\] This is to allow modelled recruitment \\(R_y\\) to be shrunk towards the stock recruitment model. However, if it is considered appropriate that recruitment can be determined exactly by a relationship with covariates, it is possible, to instead define \\(\\log R\\) in terms of a linear model in the same way as \\(\\log F\\), \\(\\log Q\\), \\(\\log \\sigma\\) and \\(\\log \\tau\\). %But this is pretty much the same as taking a geometric mean, with a model on log a, and making the variance very small. 5.2 Model structure The a4a stock assessment framework allows the user to set up a large number of different models. The mechanics which provide this flexibility are designed around the concept of submodels. The user has to define the model structure using R formulas, including mgcv (R-mgcv?) gam formulas, for each unknown variable that must be estimated. By using R formulas the stock assessment framework gives lot’s of flexibility to explore models and combinations of submodels. There are 5 submodels in operation: a model for F-at-age (\\(F_{ay}\\)) a (list) of model(s) for abundance indices catchability-at-age (\\(Q_{ays}\\)), a model for recruitment (\\(R_y\\)) a list of models for the observation variance of catch-at-age and abundance indices (\\(\\{\\sigma^2_{ay}, \\tau^2_{ays}\\}\\)) a model for the initial age structure (\\(N_{a,y=1}\\)) When setting the structure of each submodel the user is in fact building the predictive model and its parameters. The optimization process, done through ADMB, estimates the parameters and their variance-covariance matrix, allowing further analysis to be carried out, like simulation, prediction, diagnostics, etc. All the statistical machinery will be at the user’s reach. The framework’s workhorse is the method sca, which is called over FLStock and FLIndex or FLIndices objects. The following code shows an example of a call to sca(). # fit a model with a single index fit &lt;- sca(ple4, ple4.index, fmodel = ~ s(age, k = 5), qmodel = list(~ s(age, k = 4)), srmodel = ~ 1, n1model = ~ s(age, k = 5), vmodel = list( ~ 1, ~ 1)) # check output fit ## a4a model fit for: PLE ## ## Call: ## .local(stock = stock, indices = indices, fmodel = ..1, qmodel = ..2, ## srmodel = ..3, n1model = ..4, vmodel = ..5) ## ## Time used: ## Pre-processing Running a4a Post-processing Total ## 0.2739515 0.2425783 0.1034045 0.6199343 ## ## Submodels: ## fmodel: ~s(age, k = 5) ## srmodel: ~1 ## n1model: ~s(age, k = 5) ## qmodel: ## IND: ~s(age, k = 4) ## vmodel: ## catch: ~1 ## IND: ~1 The sca() method arguments are shown in Table 5.1 Table 5.1: sca() arguments Argument Default Description stock missing ‘FLStock’ object containing catch and stock information indices missing ‘FLIndices’ object containing survey indices fmodel missing a formula object depicting the model for log fishing mortality at age qmodel missing a list of formula objects depicting the models for log survey catchability at age srmodel missing a formula object depicting the model for log recruitment n1model missing a formula object depicting the model for the population in the first year of the time series vmodel missing a list of formula objects depicting the model for the variance of fishing mortality and the indices covar missing a list with covariates to be used by the submodels. The formula must have an element with the same name as the list element wkdir missing used to set a working directory for the admb optimiser; if wkdir is set, all admb files are saved to this folder, otherwise they are deleted verbose FALSE if true, admb fitting information is printed to the screen fit ‘assessment’ character with type of fit: ‘assessment’ (ML estimation, returns covariance matrix), ‘MP’ (ML estimation, doesn’t compute covariance matrix) or ‘MCMC’ (MCMC estimation) center TRUE logical defining if the data should be centered before fitting mcmc missing a ‘SCAMCMC’ object with the arguments to run MCMC fits (fit=’MCMC) 5.3 Submodel building blocks and fundamental R formulas The elements available to build submodels formulas are ‘age’ and ‘year’, which can be used to build models with different structures. In R’s modelling language, a constant model is coded as \\(\\sim 1\\), while a slope over time would simply be \\(\\sim year\\), a smoother over time \\(\\sim s(year, k=10)\\), a model with a coefficient for each year would be \\(\\sim factor(year)\\). Transformations of the variables are as usual, e.g. \\(\\sim sqrt(year)\\), etc; while combinations of all the above can be done although non-convergence will limit the possibilities. Using the \\(F\\) submodel as example the following specifies the models described in the previous paragraph, see Figure 5.2. # models m1 &lt;- ~1 m2 &lt;- ~ year m3 &lt;- ~ s(year, k=10) m4 &lt;- ~ factor(year) m5 &lt;- ~ sqrt(year) # fits fit1 &lt;- sca(ple4, ple4.indices, fmodel=m1, fit=&quot;MP&quot;) fit2 &lt;- sca(ple4, ple4.indices, fmodel=m2, fit=&quot;MP&quot;) fit3 &lt;- sca(ple4, ple4.indices, fmodel=m3, fit=&quot;MP&quot;) fit4 &lt;- sca(ple4, ple4.indices, fmodel=m4, fit=&quot;MP&quot;) fit5 &lt;- sca(ple4, ple4.indices, fmodel=m5, fit=&quot;MP&quot;) # plot lst &lt;- FLStocks(constant=ple4+fit1, linear=ple4+fit2, smooth=ple4+fit3, factor=ple4+fit4, sqrt=ple4+fit5) lst &lt;- lapply(lst, fbar) lgnd &lt;- list(points=FALSE, lines=TRUE, space=&#39;right&#39;) xyplot(data~year, groups=qname, lst, auto.key=lgnd, type=&#39;l&#39;, ylab=&#39;fishing mortality&#39;) Figure 5.2: Example of fundamental R formulas used to model the year effect The models above and their combinations can be used to model both ‘age’ and ‘year’. The corresponding fits for age are show in the code below and Figure 5.3. # models m1 &lt;- ~1 m2 &lt;- ~ age m3 &lt;- ~ s(age, k=3) m4 &lt;- ~ factor(age) m5 &lt;- ~ sqrt(age) # fits fit1 &lt;- sca(ple4, ple4.indices, fmodel=m1, fit=&quot;MP&quot;) fit2 &lt;- sca(ple4, ple4.indices, fmodel=m2, fit=&quot;MP&quot;) fit3 &lt;- sca(ple4, ple4.indices, fmodel=m3, fit=&quot;MP&quot;) fit4 &lt;- sca(ple4, ple4.indices, fmodel=m4, fit=&quot;MP&quot;) fit5 &lt;- sca(ple4, ple4.indices, fmodel=m5, fit=&quot;MP&quot;) # plot lst &lt;- FLStocks(constant=ple4+fit1, linear=ple4+fit2, smooth=ple4+fit3, factor=ple4+fit4, sqrt=ple4+fit5) lst &lt;- lapply(lst, function(x) harvest(x)[,&#39;2000&#39;]) xyplot(data~age, groups=qname, lst, auto.key=lgnd, type=&#39;l&#39;, ylab=&#39;fishing mortality in 2000&#39;) Figure 5.3: Example of fundamental R formulas used to model the age effect 5.4 The major effects available for modelling Although the building blocks for formulas are ‘age’ and ‘year’, in fact there are three effects that can be modelled for each submodel: ‘age’, ‘year’ and ‘cohort’. As examples note the following models for fishing mortality. # the age effect ageeffect &lt;- ~ factor(age) # the year effect yeareffect &lt;- ~ factor(year) # the cohort cohorteffect &lt;- ~ factor(year-age) # the fits fit1 &lt;- sca(ple4, ple4.indices, fmodel=yeareffect) fit2 &lt;- sca(ple4, ple4.indices, fmodel=ageeffect) fit3 &lt;- sca(ple4, ple4.indices, fmodel=cohorteffect) and the graphical representation of the three models in Figures 5.4 to 5.6. Figure 5.4: Major effects: the year effect (~ factor(year)) Figure 5.5: Major effects: the age effect (~ factor(age)) Figure 5.6: Major effects: the cohort effect (~ factor(year-age)) 5.5 Classes Description The data structure used to store and report the fitting process follows an object-oriented paradigm (e.g. the S4 system in R) and is hierarchically organized. The type argument in the sca method defines the fitting method - either maximum likelihood or MCMC - and specifies whether the variance-covariance matrix of the parameters is calculated and returned in the case of maximum likelihood. The resulting object belongs to a specific class, depending on the selected option. Figure 5.7 illustrates the input/output model of the statistical stock assessment method based on catch-at-age data. Figure 5.7: The fit process input/output model Table 5.2 provides details about the type of fit approach and computation of variance covariance information. Table 5.2: Fit Types and Associated Classes Type of Fit Fit Method Variance-Covariance Matrix Output Object Class MP Maximum Likelihood No a4aFit Assessment Maximum Likelihood Yes a4aFitSA MCMC MCMC No a4aFitMCMC Type MP, for “Management Procedure,” returns an a4aFit class object designed for use in MSEs (Management Strategy Evaluations) with full feedback models (Punt et al. 2016). Inverting the Jacobian to compute the variance-covariance matrix is computationally intensive in maximum likelihood models, and as MSE analyses often involve thousands of iterations, using type=\"MP\" significantly speeds up the process. This option is advantageous for scenarios requiring multiple model fits. However, the lack of immediate feedback on model convergence is a drawback, as convergence is assessed by inverting the Jacobian. A failed inversion indicates non-convergence. Type Assessment, the default, reports both the parameters and their variance-covariance, and as such requires the invertion of the jacobian. The method takes longer to run but returns a much richer dataset which allows the compution confidence intervals, simulate, etc. When the jacobian can’t be inverted this method flags potential non-convergence and returns an empty object. Type MCMC uses the Markov Chain Monte-Carlo approach, in which case it doesn’t compute likelihoods or variance-covariance matrices. It returns the full draws of the chain, which allows the computation of credible intervals, simulation kind of studies and so on. Table 5.3 describes the composition of the a4aFit class. Table 5.3: a4aFit Class Description Class Slot Slot’s Class Description a4aFit call call Code used to run the analysis catch.n FLQuant Catch numbers at age and year clock numeric Time to run the analysis desc character Description of the stock and/or analysis fitSumm array Summary statistics of the fit (e.g., number of data points) harvest FLQuant Fishing mortality at age and year index FLQuants Indices of abundance (age/biomass, by year) name character Stock name range numeric Age and year range of the data stock.n FLQuant Population in numbers (age and year) The a4aFitSA and a4aFitMCMC classes extend a4aFit, retaining all its slots while adding a pars slot of class SCAPars. Table 5.4 outlines these classes. Table 5.4: a4aFitSA and a4aFitMCMC Class Description Class Slot Slot’s Class Description a4aFitSA All a4aFit Inherited from a4aFit pars SCAPars Parameter information a4aFitMCMC All a4aFit Inherited from a4aFit pars SCAPars Parameter information The SCAPars class stores details about submodel parameters, such as formulas and distributions, and includes three slots: stkmodel for stock model parameters, qmodel for catchability parameters, and vmodel for variance parameters. Table 5.5 describes the SCAPars class. Table 5.5: SCAPars Class Description Class Slot Slot’s Class Description SCAPars stkmodel a4aStkParams Details of fishing mortality, stock recruitment, and initial stock numbers qmodel submodel Details of catchability parameters vmodel submodel Details of variance parameters The stkmodel slot encompasses parameters for fishing mortality, stock recruitment, and initial stock numbers. Due to potential correlations among these parameters, their variance-covariance matrix is reported collectively. Table 5.6 describes the slots of the a4aStkParams class. Table 5.6: a4aStkParams Class Description Class Slot Slot’s Class Description a4aStkParams centering FLPar Centering parameters coefficients FLPar Model coefficients desc character Description distr character Distributions fMod formula Fishing mortality model link function Link function linkinv function Inverse link function m FLQuant Mortality parameters mat FLQuant Maturity parameters n1Mod formula Initial stock numbers model name character Stock name range numeric Age and year range srMod formula Stock-recruitment model units character Units of measurement vcov array Variance-covariance matrix wt FLQuant Weights The qmodel and vmodel slots share the submodel class, which describes single submodels. Table 5.7 provides details. Table 5.7: submodel Class Description Class Slot Slot’s Class Description submodel centering FLPar Centering parameters coefficients FLPar Model coefficients desc character Description distr character Distributions formula formula Submodel formula link function Link function linkinv function Inverse link function name character Stock name range numeric Age and year range vcov array Variance-covariance matrix References "],["introduction-to-splines.html", "6 Introduction to Splines 6.1 Generalize to polynomials 6.2 Thin plate spline 6.3 The mgcv package inside a4a 6.4 On the number of knots \\(k\\)", " 6 Introduction to Splines Splines are a specific case of smoothers. A smoother is a method or algorithm designed to estimate a smooth function that fits data, capturing underlying trends without overfitting noise. Splines are a powerful tool for modeling complex, non-linear relationships between variables in a flexible and interpretable way. A common way to use splines is to break a function into smooth, continuous polynomial segments, each called a piece or basis function, joined at specific points called knots. This piecewise approach allows us to capture the non-linearity in the data without overfitting. Basis functions consist the main building block of splines.The key concept of “basis functions” is that they transform the input variable (or vector) \\(\\mathbf{x}\\) into a set of new variables, which are then used as inputs in the model. This allows the model to remain linear in these transformed variables, even though it can capture complex, non-linear relationships in the original variable. Splines build their functionality through the core concepts of linear models. Linear models assume that the relationship between data can be described by a straight line, in the case of only one predictor \\(\\mathbf{x}\\). We denote linear model as: \\[\\mathbf{y} = \\beta_0 + \\beta_1\\mathbf{x} + \\mathbf{\\epsilon}\\] Where \\(\\mathbf{y}\\) are the observations, the parameters \\(\\beta_0\\) and \\(\\beta_1\\) uniquely determine a straight line and \\(\\mathbf{\\epsilon}\\) is the observation error. Simple in its construction and representation, linear models can be limited when trying to capture the complexity of a real-world data set. The simplest form of a linear model is the mean or average of a data set: \\[\\mathbf{y} = \\beta_0 + \\epsilon\\] In the following graph of non linear data (\\(\\mathbf{y}\\)), we fit a simple mean value of \\(\\beta_0\\) and a straight line model \\(\\beta_0 + \\beta_1\\mathbf{x}\\): Figure 6.1: Generated data with two linear fits, left a simple mean and right a straight line In this case our parameters are a \\(\\beta_0 =\\) 3.09 for the mean (left) model, while for the straight line model, \\(\\beta_0 =\\) 0.85 and \\(\\beta_1 =\\) 0.46. In Figure 6.1 we see how by adding to the constant model \\(\\beta_0\\) a multiple (\\(\\beta_1\\)) of the variable \\(\\mathbf{x}\\), the model becomes a little more complex but it can now follow the general upward trend compared to the first line, although it fails to follow the peaks and the lows of the data set. One of the mechanisms that splines use is to split the set of values of the predictor, in our case \\(\\mathbf{x}\\), in smaller compartments and fit in those compartments a specified model. The points where the splitting occurs are called knots. In Figure 6.2 we take a stepwise approach following the logic behind the use of piecewise polynomials in smoothing splines. First, we split our range of \\(\\mathbf{x}\\) values in 4 subsets, by defining our knots, then we take the average of \\(\\mathbf{y}\\) data for each of these compartments and in the final step we add a bit of complexity by fitting a straight line model in each of these subsets of our original dataset. Figure X2 demonstrates a naive approach to try and follow better the trends of the data, where in each step we manage to capture a bit more. Figure 6.2: Effect of breaking the data in sections, by knots, and fitting the mean or a linear model to each section. 6.1 Generalize to polynomials We explored the concept of splitting the \\(\\mathbf{x}\\) variable space into compartments and developing a solution by locally fitting linear models that better capture the global trajectory of the data. Smooth functions rely on two fundamental mechanisms. The first, as mentioned earlier, involves how the domain of the function is divided for estimation. The second, which we will focus on here, involves using slightly more complex functions than linear ones, such as polynomials. By combining polynomial pieces and ensuring smoothness at their junctions, i.e. knots, we can create flexible models that adapt to the data. In regression, splines are a powerful tool for fitting complex shapes by introducing non-linear trends while maintaining control over the smoothness of the overall function. This approach allows us to balance flexibility and precision in modeling. The polynomials are build by transforming the predictor variable or the sets of variables into higher order polynomials (usually 2nd or 3rd grade polynomials). These polynomials need to have matching values at the knots. Let \\(S\\) our spline function, that is defined in an interval \\([a,b]\\). We seek to construct \\(S\\) by combining \\(k-1\\) polynomials \\(P\\), where \\(k\\) is the number of knots. Let also \\(t_{i}, i = 0, ..., k-1\\) the positions of the knots in the interval \\([a,b]\\). \\(S\\) is going to be defined as: \\[ S(x) = \\begin{cases} P_1(x) &amp; \\text{if } a \\leq x &lt; t_1, \\\\ P_2(x) &amp; \\text{if } t_1 \\leq x &lt; t_2, \\\\ \\quad\\vdots \\\\ P_{k-1}(x) &amp; \\text{if } t_{k-1} \\leq x \\leq b, \\end{cases} \\] The above definition is a simplified version of how splines work and can help as an intuitive approach. In reality splines need to satisfy some extra conditions like continuity, i.e. \\(P_{i-1}(t_{i}) = P_{i}(t_{i})\\) on the points of junction, and of the first and second derivative. Depending on the basis functions the conditions may differ. In Figure 6.3 we demonstrate the fitting of cubic regression splines. In this case the highest order of the polynomials \\(P_i(x)\\) are 3rd degree polynomials and it comprises 4 cubic segments, i.e basis functions. The spline would be then as follows: \\[\\mathbf{y} = \\beta_0 + \\beta_1P_1(\\mathbf{x}) + \\beta_2P_2(\\mathbf{x}) + \\beta_3P_3(\\mathbf{x}) + \\beta_4P_4(\\mathbf{x})+\\beta_5P_5(\\mathbf{x}) \\] Figure 6.3: A B-spline with 5 knots consists of 5 cubic polynomials The B-spline fit above is constrained at the boundaries, by putting two of the five knots there, resulting in a linear behavior at the ends of the data range. This approach is helpful for data that has an approximately linear trend at the boundaries but exhibits non-linearity in the center. The knots in this regression spline are placed by quantiles through the variable space, so in the case where the data are evenly spread across the variable space the knots will be placed evenly. For the spline fitted above, there are \\(k\\) polynomials plus an intercept (not shown) based on the knots (dashed lines). See Figure 6.4 for a depiction of the basis functions. Figure 6.4: basis functions for a B-spline with 5 knots [A bit confused here … should it be 5 polinomials? are you sure what you’re calling intercet is correct?] The polynomials transform the initial variable \\(\\mathbf{x}\\) and create a model matrix \\(\\mathbf{X}\\) with \\(k\\) columns and \\(n\\) rows, where \\(n\\) is the number of data points. This new transformation is being then used to fit the model and estimate the \\(\\beta_{0}, ... ,\\beta_{k}\\) coefficients, \\(\\beta_0\\) is required for the intercept. The fitted model results from \\(\\mathbf{X} \\mathbf{\\beta}\\) (Figure 6.5). Figure 6.5: basis functions for a B-spline with 5 knots 6.2 Thin plate spline Thin plate splines are particularly useful for smoothing in multiple dimensions. However, they also work well with univariate data, as they offer flexibility and control over smoothness they are the default choice of the mgcv package. Thin plate splines work differently from the splines we have shown so far. They are not composed of a sequence of local polynomials but from basis functions that are smooth across the entire range of the data, and capture increasing amounts of flexibility (Figure 6.6). # Fit a thin plate spline with gam() tps_model &lt;- gam(y ~ s(x, k = 5, bs = &quot;tp&quot;), data = data) # Predict data$tps_fit &lt;- predict(tps_model) Figure 6.6: Thin plate splines fit with k = 5 Thin plate splines are ideal when you need a smooth fit without predefined knots. The notion of knots in thin plate splines does not have the same interpretation as for B-splines and other piecewise functions, and infact it is likely not useful to think of knots when using thin plate spines. Instead it is better to think of the number of basis functions used to represent the smooth term (Figure 6.7). Figure 6.7: basis functions for thin plate splines As in the example before, the fitted model results from \\(X\\beta\\) (Figure 6.8). Figure 6.8: Basis functions for thin plate splines and the fitted model Figure 6.9 shows both models fitted to the dataset, both fits use the same number of knots. [Can we say a bit more here?] Figure 6.9: Thin plate splines and cubic regression splines fit 6.3 The mgcv package inside a4a The mgcv package provides various user input options to define the smoother functions used to construct the submodels. Under the a4a framework, the mgcv package is used to construct the model matrices of the submodels, which are then passed to ADMB where the model fitting takes place. The default option for the basis functions of the splines is bs = tp (thin plate splines) and is considered the optimal option. The user can define other smoothing basis functions using the bs argument. The user can refer to the smooth.terms of the mgcv package for a full description. There are many equivalent basis functions for the splines, and some of them have little or no effect in the context of a4a, since they differ only in the penalty term, which is not used in a4a. Examples for different smoothing terms: fmod00 &lt;- ~s(age)+s(year, bs = &#39;tp&#39;, k = 10) # thin plate splines fmod01 &lt;- ~s(age)+s(year, bs = &#39;cr&#39;, k = 10) # regression cubic splines fmod02 &lt;- ~s(age)+s(year, bs = &#39;bs&#39;, k = 10) # b-splines fmod03 &lt;- ~s(age)+s(year, bs = &#39;ps&#39;, k = 10) # p-splines fmod04 &lt;- ~s(age)+s(year, bs = &#39;ad&#39;, k = 10) # Adaptive smoothers fit00 &lt;- sca(stk, idx, fmodel = fmod00) fit01 &lt;- sca(stk, idx, fmodel = fmod01) fit02 &lt;- sca(stk, idx, fmodel = fmod02) fit03 &lt;- sca(stk, idx, fmodel = fmod03) fit04 &lt;- sca(stk, idx, fmodel = fmod04) In this example we are using the thin plate regression splines, cubic splines, b-splines, p-splines and adaptive smoothers. Figure 6.10 shows the different fits together, where it’s clear the differences are very small. Figure 6.10: Multiple fits of thin plate splines, cubic splines, b-splines, p-splines and adaptive smoothers Functionality of mgcv package provides also the option to work with interactions. Although s(age, year) can be defined, it uses a common bivariate spline for the two variables which are very different in scale. It is preferable if interactions are assumed to use te(age, year). Again is up to the user to define the basis functions for the smoothers. Figure 6.11 shows the differences when using different basis for the tensor product. [The paragraph above needs a bit more explanation about what a tensor is] fmod03 &lt;- ~te(age, year, k = c(3,10)) fmod04 &lt;- ~te(age, year, k = c(3,10), bs = &#39;cr&#39;) fmod05 &lt;- ~te(age, year, bs = &#39;bs&#39;) fit03 &lt;- sca(stk, idx, fmodel = fmod03) fit04 &lt;- sca(stk, idx, fmodel = fmod04) fit05 &lt;- sca(stk, idx, fmodel = fmod05) Figure 6.11: Multiple basis for a tensor and their effects in the fit 6.4 On the number of knots \\(k\\) \\(k\\) is the dimension of the basis used to represent the smooth term \\(s\\). The default depends on the number of variables that the smooth is a function of. In practice k-1 (or k) sets the upper limit on the degrees of freedom associated with an s smooth (1 degree of freedom is usually lost to the intercept of the smooth). For the smooths the upper limit of the degrees of freedom is given by the product of the k values provided for each marginal smooth less one, for the constraint. The choice of the k is not critical and in general it must be large enough to allow to have enough degrees of freedom capturing the underlying process and small enough to prevent overfitting (Figure 6.12). A strong pattern in the residuals can be a sign of low \\(k\\). fmod00 &lt;- ~s(age)+s(year, k = 5) fmod01 &lt;- ~s(age)+s(year, k = 10) # cubic splines [Why is this cubic] fmod02 &lt;- ~s(age)+s(year, k = 20) # b-splines [and this b?] fit00 &lt;- sca(stk, idx, fmodel = fmod00) fit01 &lt;- sca(stk, idx, fmodel = fmod01) fit02 &lt;- sca(stk, idx, fmodel = fmod02) Figure 6.12: Multiple ks and their effects in the fit We can check the result of choosing different \\(k\\) values on BIC (Bayesian Information Criteria) and GCV (Generalized Cross Validation score) by running the stock assessment model with different k and looking at the values of those stastics (Figure 6.13). fmodsk &lt;- list() for(i in 10:20) { fmodsk[[paste0(i)]] &lt;- as.formula(paste0(&quot;~s(age)+s(year, k=&quot;,i,&quot;)&quot;)) } myFits &lt;- scas(FLStocks(stk), list(idx), fmodel = fmodsk) Figure 6.13: BIC (Bayesian Information Criteria) and GCV (Generalized Cross Validation score) profiles based on changing the value of k. "],["fitting.html", "7 Fitting 7.1 Fishing mortality submodel (\\(F_{ay}\\)) 7.2 Abundance indices catchability submodel (\\(Q_{ays}\\)) 7.3 Stock-recruitment submodel (\\(R_y\\)) 7.4 Observation variance submodel (\\(\\{\\sigma^2_{ay}, \\tau^2_{ays}\\}\\)) 7.5 Initial year abundance submodel (\\(N_{a,y=1}\\)) 7.6 More models 7.7 Working with covariates 7.8 Assessing files 7.9 Missing observations in the catch matrix or index", " 7 Fitting The a4a stock assessment framework is implemented in R through the method sca(). The method call requires as a minimum a FLStock object and a FLIndices or FLindex object, in which case the default submodels will be set by the method. Having described building blocks, basic formulations and effects available to build a submodel’s model, it’s important to look into specific formulations and relate them to commonly known representations. Note that although a large number of formulations are available for each submodel, the user must carefully decide on the full stock assessment model being build and avoid over-parameterize. Over-parametrization may lead to non-convergence, but may also end up not being very useful for prediction/forecasting, which is one of the main objectives of stock assessment. library(FLa4a) data(ple4) data(ple4.indices) data(ple4.index) fmod &lt;- ~ s(age, k=8) + s(year, k=30) + te(age, year, k = c(5, 15)) fit &lt;- sca(ple4, ple4.indices, fmodel=fmod) stk &lt;- ple4 + fit Figure 7.1: Stock summary Submodels that are not explicitly defined will be set by default using the relevant call to defaultFmod(), defaultQmod, defaultSRmod(), defaultN1mod or defaultVmod(). These methods will use the length of the time series and number of age groups to define the models. The show method for a4aFit objects display the models used for the fit. ## a4a model fit for: PLE ## ## Call: ## .local(stock = stock, indices = indices, fmodel = ..1) ## ## Time used: ## Pre-processing Running a4a Post-processing Total ## 0.7885687 5.9960084 0.1161988 6.9007759 ## ## Submodels: ## fmodel: ~s(age, k = 8) + s(year, k = 30) + te(age, year, k = c(5, 15)) ## srmodel: ~factor(year) ## n1model: ~s(age, k = 3) ## qmodel: ## BTS-Isis-early: ~s(age, k = 6) ## BTS-Combined (ISIS and TRIDENS): ~s(age, k = 6) ## SNS: ~s(age, k = 5) ## BTS-Combined (all): ~s(age, k = 6) ## IBTS_Q3: ~s(age, k = 6) ## IBTS_Q1: ~s(age, k = 5) ## vmodel: ## catch: ~s(age, k = 3) ## BTS-Isis-early: ~1 ## BTS-Combined (ISIS and TRIDENS): ~1 ## SNS: ~1 ## BTS-Combined (all): ~1 ## IBTS_Q3: ~1 ## IBTS_Q1: ~1 To set specific submodels the user has to write the relevant R formula and include it in the call. The arguments for each submodel are self-explanatory: fishing mortality is ‘fmodel’, indices’ catchability is ‘qmodel’, stock-recruitment is ‘srmodel’, observation variance is ‘vmodel’ and for initial year’s abundance is ‘n1model’. The following model comes closer to the official stock assessment of North Sea plaice, as such we’ll name it fit0 and keep it for future comparisons. fmod0 &lt;- ~s(age, k=6)+s(year, k=10)+te(age, year, k=c(3,8)) qmod0 &lt;- list(~s(age, k = 4), ~s(age, k = 3), ~s(age, k = 3) + year, ~s(age, k = 3), ~s(age, k = 4), ~s(age, k = 6)) srmod0 &lt;- ~ s(year, k=20) vmod0 &lt;- list(~s(age, k=4), ~1, ~1, ~1, ~1, ~1, ~1, ~1) n1mod0 &lt;- ~ s(age, k=3) fit0 &lt;- sca(ple4, ple4.indices, fmodel=fmod0, qmodel=qmod0, srmodel=srmod0, n1model=n1mod0, vmodel=vmod0) stk0 &lt;- ple4 + fit0 Figure 7.2: Stock summary - close to official assessment As before by calling the fitted object, the submodels’ formulas are printed in the console: ## a4a model fit for: PLE ## ## Call: ## .local(stock = stock, indices = indices, fmodel = ..1, qmodel = ..2, ## srmodel = ..3, n1model = ..4, vmodel = ..5) ## ## Time used: ## Pre-processing Running a4a Post-processing Total ## 0.74133968 2.36244512 0.07750773 3.18129253 ## ## Submodels: ## fmodel: ~s(age, k = 6) + s(year, k = 10) + te(age, year, k = c(3, 8)) ## srmodel: ~s(year, k = 20) ## n1model: ~s(age, k = 3) ## qmodel: ## BTS-Isis-early: ~s(age, k = 4) ## BTS-Combined (ISIS and TRIDENS): ~s(age, k = 3) ## SNS: ~s(age, k = 3) + year ## BTS-Combined (all): ~s(age, k = 3) ## IBTS_Q3: ~s(age, k = 4) ## IBTS_Q1: ~s(age, k = 6) ## vmodel: ## catch: ~s(age, k = 4) ## BTS-Isis-early: ~1 ## BTS-Combined (ISIS and TRIDENS): ~1 ## SNS: ~1 ## BTS-Combined (all): ~1 ## IBTS_Q3: ~1 ## IBTS_Q1: ~1 There are a set of methods for a4a fit objects which help manipulating sca() results, namely +, that updates the stock object with the fitted fishing mortalities, population abundance and catch in numbers at age. This method can be applied to FLStocks and a4aFits objects as well. The following subsections describe common formulations used to define submodels. Although the formulas are tailored to specific submodels — e.g., a separable model for \\(F\\) — they can, in principle, be applied to any submodel. The sca method is agnostic to the model setup and will attempt to fit the model regardless of its specification. However, from a statistical standpoint, convergence may fail if the model is not well specified. From a fisheries modeling perspective, limitations arise in how the model is interpreted. For example, if a scientific survey is modeled with a year effect, the user is implicitly assuming that the survey’s selectivity has changed over time. Consequently, the model may attribute part of the observed trend in the survey data to changes in selectivity rather than to changes in abundance. 7.1 Fishing mortality submodel (\\(F_{ay}\\)) 7.1.1 Separable model One of the most useful models for fishing mortality is one in which ‘age’ and ‘year’ effects are independent, that is, where the shape of the selection pattern does not change over time, but the overall level of fishing mortality do. Commonly called a ‘separable model’. A full separable model in a4a is written using the factor function which converts age and year effects into categorical values, forcing a different coefficient to be estimated for each level of both effects. This model has \\(age x year\\) number of parameters. fmod1 &lt;- ~ factor(age) + factor(year) fit1 &lt;- sca(ple4, ple4.indices, fmodel=fmod1, fit=&quot;MP&quot;) One can reduce the number of parameters and add dependency along both effects, although still keeping independence of each other, by using smoothers rather than factor. We’re using the North Sea Plaice data, and since it has 10 ages we will use a simple rule of thumb that the spline should have fewer than \\(\\frac{10}{2} = 5\\) degrees of freedom, and so we opt for 4 degrees of freedom. We will also do the same for year and model the change in \\(F\\) through time as a smoother with 20 degrees of freedom. fmod2 &lt;- ~ s(age, k=4) + s(year, k=20) fit2 &lt;- sca(ple4, ple4.indices, fmodel=fmod2, fit=&quot;MP&quot;) An interesting extension of the separable model is the ‘double separable’ where a third factor or smoother is added for the cohort effect. fmod3 &lt;- ~ s(age, k=4) + s(year, k=20) + s(as.numeric(year-age), k=10) fit3 &lt;- sca(ple4, ple4.indices, fmodel=fmod3, fit=&quot;MP&quot;) Figures 7.3 and 7.4 depicts the three models selectivities for each year. Each separable model has a single selectivity that changes it’s overall scale in each year, while the double separable introduces some variability over time by modeling the cohort factor. Figure 7.3: Selection pattern of separable models. Each line represents the selection pattern in a specific year. Independent age and year effects (factor), internally dependent age and year (smooth), double separable (double). Figure 7.4: Fishing mortality of separable models. Independent age and year effects (factor), internally dependent age and year (smooth), double separable (double). 7.1.2 Model with age-year interaction A non-separable model, where we consider age and year to interact can be modeled by a smooth interaction term with a tensor product of cubic splines, the te method (Figure 7.5), again borrowed from mgcv ((R-mgcv?)). fmod &lt;- ~ te(age, year, k = c(4,20)) fit &lt;- sca(ple4, ple4.indices, fmodel=fmod) Figure 7.5: Fishing mortality smoothed non-separable model In this example fishing mortalities are linked across age and time. What if we want to free up a specific age class because in the residuals we see a consistent pattern. This can happen, for example, if the spatial distribution of juveniles is disconnected to the distribution of adults. The fishery focuses on the adult fish, and therefore the \\(F\\) on young fish is a function of the distribution of the juveniles and could deserve a specific model. This can be achieved by adding a component for the year effect on age 1 (Figure 7.6). We’ll use s’s argument by to define the ages that the model will apply to. The as.numeric method over age==1, will result in a matrix that will be \\(1\\) for ages 1 and \\(0\\) for the other ages, effectively removing those ages from the s model. Furthermore, by not removing age 1 from the te component we’re in effect adding the two estimates for age 1. fmod &lt;- ~ te(age, year, k = c(4,20)) + s(year, k = 5, by = as.numeric(age==1)) fit2 &lt;- sca(ple4, ple4.indices, fmodel=fmod) Figure 7.6: Fishing mortality age-year interaction model with extra age 1 smoother. 7.1.3 Constant selectivity for contiguous ages or years To set these models we’ll use the method replace() to define which ages or years will be modelled together. The following example shows replace() in operation. The dependent variables used in the model will be changed and attributed the same age or year, as such during the fit observations of those ages or years with will be seen as replicates. One can think of it as sharing the same mean value, which will be estimated by the model. age &lt;- 1:10 # last age same as previous replace(age, age&gt;9, 9) ## [1] 1 2 3 4 5 6 7 8 9 9 # all ages after age 6 replace(age, age&gt;6, 6) ## [1] 1 2 3 4 5 6 6 6 6 6 year &lt;- 1950:2010 replace(year, year&gt;2005, 2005) ## [1] 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 ## [16] 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 ## [31] 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 ## [46] 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2005 2005 2005 2005 ## [61] 2005 In the \\(F\\) submodel one can use this method to fix the estimation of \\(F\\) in the plus group to be the same as in the last non-aggregated age. fmod &lt;- ~ s(replace(age, age&gt;9, 9), k=4) + s(year, k=20) fit &lt;- sca(ple4, ple4.indices, fmod) Figure 7.7: F-at-age fixed above age 9 Or estimate the average \\(F\\) in the most recent years, instead of averaging after the assessment to compute the statu quo selection pattern. fmod &lt;- ~ s(age, k=4) + s(replace(year, year&gt;2013, 2013), k=20) fit &lt;- sca(ple4, ple4.indices, fmod) Figure 7.8: F-at-age fixed for the most recent 5 years 7.1.4 Time blocks selectivity To define blocks of data sca() uses the method breakpts(), which creates a factor from a vector with levels defined by the second argument. year &lt;- 1950:2010 # two levels separated in 2000 breakpts(year, 2000) ## [1] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] ## [7] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] ## [13] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] ## [19] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] ## [25] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] ## [31] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] ## [37] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] ## [43] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] ## [49] (1949,2000] (1949,2000] (1949,2000] (2000,2010] (2000,2010] (2000,2010] ## [55] (2000,2010] (2000,2010] (2000,2010] (2000,2010] (2000,2010] (2000,2010] ## [61] (2000,2010] ## Levels: (1949,2000] (2000,2010] # five periods with equal interval breakpts(year, seq(1949, 2010, length=6)) ## [1] (1949,1961.2] (1949,1961.2] (1949,1961.2] (1949,1961.2] ## [5] (1949,1961.2] (1949,1961.2] (1949,1961.2] (1949,1961.2] ## [9] (1949,1961.2] (1949,1961.2] (1949,1961.2] (1949,1961.2] ## [13] (1961.2,1973.4] (1961.2,1973.4] (1961.2,1973.4] (1961.2,1973.4] ## [17] (1961.2,1973.4] (1961.2,1973.4] (1961.2,1973.4] (1961.2,1973.4] ## [21] (1961.2,1973.4] (1961.2,1973.4] (1961.2,1973.4] (1961.2,1973.4] ## [25] (1973.4,1985.6] (1973.4,1985.6] (1973.4,1985.6] (1973.4,1985.6] ## [29] (1973.4,1985.6] (1973.4,1985.6] (1973.4,1985.6] (1973.4,1985.6] ## [33] (1973.4,1985.6] (1973.4,1985.6] (1973.4,1985.6] (1973.4,1985.6] ## [37] (1985.6,1997.8] (1985.6,1997.8] (1985.6,1997.8] (1985.6,1997.8] ## [41] (1985.6,1997.8] (1985.6,1997.8] (1985.6,1997.8] (1985.6,1997.8] ## [45] (1985.6,1997.8] (1985.6,1997.8] (1985.6,1997.8] (1985.6,1997.8] ## [49] (1997.8,2010] (1997.8,2010] (1997.8,2010] (1997.8,2010] ## [53] (1997.8,2010] (1997.8,2010] (1997.8,2010] (1997.8,2010] ## [57] (1997.8,2010] (1997.8,2010] (1997.8,2010] (1997.8,2010] ## [61] (1997.8,2010] ## 5 Levels: (1949,1961.2] (1961.2,1973.4] (1973.4,1985.6] ... (1997.8,2010] Note seq() computes ‘left-open’ intervals, which means that to include 1950 the sequence must start one year earlier. These methods can be used to create discrete time series, for which a different selection pattern is allowed in each block. This is called an interaction in statistical modelling parlance, and typically a * denotes an interaction term, for smoothers an interaction is achieved using the by argument. When this argument is a factor a replicate of the smooth is produced for each factor level. In the next case we’ll use the breakpts() to split the time series at 1990, although keeping the same shape in both periods, a thin plate spline with 3 knots (Figure 7.9). fmod &lt;- ~s(age, k = 3, by = breakpts(year, 1990)) fit &lt;- sca(ple4, ple4.indices, fmod) Figure 7.9: F-at-age in two periods using in both cases a thin plate spline with 3 knots 7.1.5 Time changing selectivity In many cases, it may be desirable to allow the selection pattern to evolve over time, from year to year. Again there are several ways to do this, one way is to estimate a mean selection pattern, while also allowing \\(F\\) to vary over time for each age. This is like a separable smoother over year, with ‘age blocks’ so, looking back at previous examples, we have: fmodel &lt;- ~ s(year, k = 15, by = factor(age)) + s(age, k = 4) This is a type of interaction between age and year, but the only connection (or correlation) across ages is via the smoother on age, however there are still 15 degrees of freedom for each age, so the model 10 x 15 + 4 = 154 degrees of freedom. To include correlation across ages and years together the tensor product (te()) is used, this has the effect of restricting the flexibility of the model for \\(F\\). In the following, there is a smoother in 2 dimensions (age and year) where there is 5 degrees of freedom in the age direction, and 15 in the year dimension, resulting in a total of 5 x 15 = 65 degrees of freedom fmodel &lt;- ~ te(age, year, k = c(5, 15)) Often the above formulations provide too much flexibility, and a more complicated specification, but simpler model is preferable: fmodel &lt;- ~ s(age, k = 4) + s(year, k = 15) + te(age, year, k = c(3, 5)) in the above model, the main effects for age and year still have similar flexibility to the full tensor model, however, the interaction (or the change in F at age over time) has been restricted, so that the full model now has 4 + 15 + 3 x 5 = 34 degrees of freedom. 7.1.6 Closed form selection pattern One can use a closed form for the selection pattern. The only requirement is to be able to write it as a R formula, the example below uses a logistic form. fmod &lt;- ~ I(1/(1+exp(-age))) fit &lt;- sca(ple4, ple4.indices, fmod) Figure 7.10: F-at-age logistic 7.2 Abundance indices catchability submodel (\\(Q_{ays}\\)) The catchability submodel is set up the same way as the \\(F\\) submodel. The only difference is that the submodel is set up as a list of formulas, where each formula relates with one abundance index. There’s no limitation in the number of indices or type that can be used for a fit. It’s the analyst that has to decide based on her/his expertise and knowledge of the stock and fleet dynamics. In the following examples we’ll use a single index instead of all available indices for plaice in ICES area 4, to simplify the code and examples. 7.2.1 Catchability submodel for age based indices The first model shown is simply a dummy effect on age, which means that one coefficient will be estimated for each age. Note this kind of model considers each level of the factor to be independent from the others levels (Figure 7.11). qmod &lt;- list(~factor(age)) fit &lt;- sca(ple4, ple4.index, qmodel=qmod) Figure 7.11: Catchability age independent model If one considers catchability at a specific age to be dependent on catchability on the other ages, similar to a selectivity modelling approach, one option is to use a smoother at age, and let the data ‘speak’ regarding the shape (Figure 7.12). qmod &lt;- list(~ s(age, k=4)) fit &lt;- sca(ple4, ple4.indices[1], qmodel=qmod) Figure 7.12: Catchability smoother age model Finally, one may want to investigate a trend in catchability with time, very common in indices built from CPUE data. In the example given here we’ll use a linear trend in time, set up by a simple linear model (Figure 7.13). qmod &lt;- list( ~ s(age, k=4) + year) fit &lt;- sca(ple4, ple4.indices[1], qmodel=qmod) Figure 7.13: Catchability with a linear trend in year 7.2.2 Catchability submodel for age aggregated biomass indices The previous section focused on age-disaggregated indices, which are most often reported as standardized number of individuals, e.g. number of individuals caught per hour. Age-aggregated indices (such as CPUE, biomass, DEPM, etc.) may also be used to tune the population’s biomass in terms of weight. These indices are linked either to the total biomass or to the weight of a specific group of age classes, defined by the age range set in the object. In such cases, a different index class must be used: FLIndexBiomass. This class uses a vector named index with an age dimension labeled as ‘all’. The qmodel should be specified without age-specific factors, although it may still include a ‘year’ component and relevant covariates, if needed. # simulating a biomass index (note the name of the first dimension element) using # the ple4 biomass and an arbritary catchability of 0.001 plus a lognormal error. dnms &lt;- list(age=&quot;all&quot;, year=range(ple4)[&quot;minyear&quot;]:range(ple4)[&quot;maxyear&quot;]) bioidx &lt;- FLIndexBiomass(FLQuant(NA, dimnames=dnms)) index(bioidx) &lt;- stock(ple4)*0.001 index(bioidx) &lt;- index(bioidx)*exp(rnorm(index(bioidx), sd=0.1)) range(bioidx)[c(&quot;startf&quot;,&quot;endf&quot;)] &lt;- c(0,0) # note the name of the first dimension element index(bioidx) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 ## all 410 452 471 524 600 643 535 491 530 520 636 450 463 472 ## year ## age 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 ## all 445 422 386 451 508 515 600 532 469 436 570 511 506 590 ## year ## age 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 ## all 589 709 690 698 651 498 365 426 388 350 331 328 342 416 ## year ## age 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 ## all 386 343 399 396 397 400 389 470 455 565 562 708 871 711 ## year ## age 2013 2014 2015 2016 2017 ## all 928 1069 982 918 1056 ## ## units: t # fitting the model fit &lt;- sca(ple4, FLIndices(bioidx), qmodel=list(~1)) To estimate a constant selectivity over time one used the model \\(\\sim 1\\), resulting in the following estimate: predict(fit)$qmodel[[1]][1,drop=TRUE] ## [1] 0.001006285 The next code shows an example where the biomass index refers to age groups 2 to 4, e.g. the CPUE of a fleet that targets these particular ages. # creating the index dnms &lt;- list(age=&quot;all&quot;, year=range(ple4)[&quot;minyear&quot;]:range(ple4)[&quot;maxyear&quot;]) bioidx &lt;- FLIndexBiomass(FLQuant(NA, dimnames=dnms)) # but now use only ages 2:4 index(bioidx) &lt;- tsb(ple4[ac(2:4)])*0.001 index(bioidx) &lt;- index(bioidx)*exp(rnorm(index(bioidx), sd=0.1)) range(bioidx)[c(&quot;startf&quot;,&quot;endf&quot;)] &lt;- c(0,0) # to pass this information to the model one needs to specify an age range range(bioidx)[c(&quot;min&quot;,&quot;max&quot;)] &lt;- c(2,4) # fitting the model fit &lt;- sca(ple4, FLIndices(bioidx), qmodel=list(~1)) Once more the estimate value is not very far from the simulated one, 0.001. predict(fit)$qmodel[[1]][1,drop=TRUE] ## [1] 0.00101068 7.2.3 Catchability submodel for single age indices Similar to age aggregated indices one may have an index that relates only to one age, like a recruitment index. In this case the FLIndex object must have in the first dimension the age it refers to. The fit uses the index to tune the population abundance for the specific age. As for biomass indices, the qmodel should be set without age factors, although it can have a ‘year’ component and covariates if needed. idx &lt;- ple4.index[1] fit &lt;- sca(ple4, FLIndices(recidx=idx), qmodel=list(~1)) # the estimated catchability is predict(fit)$qmodel[[1]][1,drop=TRUE] ## [1] 0.06007767 7.3 Stock-recruitment submodel (\\(R_y\\)) The S/R submodel is a special case, in the sense that it can be set up with the same linear tools as the \\(F\\) and \\(Q\\) models, but it can also use some hard coded models. The example shows how to set up a simple dummy model with factor(), a smooth model with s(), a Ricker model (ricker()), a Beverton and Holt model (bevholt()), a hockey stick model (hockey()), and a geometric mean model (geomean()). See Figure 7.14 for results. As mentioned before, the ‘structural’ models have a fixed variance, which must be set by defining the coefficient of variation. srmod &lt;- ~ factor(year) fit &lt;- sca(ple4, ple4.indices, srmodel=srmod) srmod &lt;- ~ s(year, k=15) fit1 &lt;- sca(ple4, ple4.indices, srmodel=srmod) srmod &lt;- ~ ricker(CV=0.1) fit2 &lt;- sca(ple4, ple4.indices, srmodel=srmod) srmod &lt;- ~ bevholt(CV=0.1) fit3 &lt;- sca(ple4, ple4.indices, srmodel=srmod) srmod &lt;- ~ hockey(CV=0.1) fit4 &lt;- sca(ple4, ple4.indices, srmodel=srmod) srmod &lt;- ~ geomean(CV=0.1) fit5 &lt;- sca(ple4, ple4.indices, srmodel=srmod) Figure 7.14: Recruitment estimates since 1960 by each stock-recruitment model. 7.4 Observation variance submodel (\\(\\{\\sigma^2_{ay}, \\tau^2_{ays}\\}\\)) The variance model allows the user to set up the shape of the observation variances \\(\\sigma^2_{ay}\\) and \\(\\tau^2_{ays}\\). This is an important subject for fisheries data used as input to stock assessment models. The defaults assume a U-shape like model for catch-at-age and constant variance for abundance indices. The first relies on the fact that it’s common to have more precision on the most represented ages and less precision on the less frequent ages which tend to be the younger and older individuals. These sizes are less caught by the fleets and as such do not appear as often at the auction markets samples. With regards to the abundance indices, one assumes a scientific survey to have a well designed sampling scheme and protocols which keep observation error at similar levels across ages. # reference model with constant variance for the survey index vmod &lt;- list(~s(age, k=3), ~1) fit1 &lt;- sca(ple4, ple4.index, vmodel=vmod) # to compare - survey index variance modelled has a U-shape smoother vmod &lt;- list(~s(age, k=3), ~s(age, k=3)) fit2 &lt;- sca(ple4, ple4.index, vmodel=vmod) Variance estimated for the survey is constant at 0.476 while for catches using the U-shape model, fitted with a smoother, changes with ages (Figure 7.15). Figure 7.15: Abundance index observation variance estimate Observation variance options have an impact in the final estimates of population abundance, which can be seen in Figure 7.16. Figure 7.16: Population estimates using two different variance models for the survey 7.5 Initial year abundance submodel (\\(N_{a,y=1}\\)) The submodel for the stock number at age in the first year of the time series is set with the usual tools. The model deals with the shape of the population abundance in a single year and as such the year effect shouldn’t be included (Figure 7.17). This model has its influence limited to the initial lower triangle of the population matrix, which in assessments with long time series doesn’t make much difference. Nevertheless, when modelling stocks with short time series in relation to the number of ages present, it becomes more important and should be given proper attention. # model with smoother n1mod &lt;- ~s(age, k=4) fit1 &lt;- sca(ple4, ple4.indices, n1model=n1mod) # model with factor n1mod &lt;- ~factor(age) fit2 &lt;- sca(ple4, ple4.indices, n1model=n1mod) Figure 7.17: Nay=1 models The impact in the overall perspective of the stock status is depicted in Figure 7.18. Most of the changes happen in the beggining of the time series, although due to the impact on the estimates of other submodels’ parameters it can have an impact over the full time series. Figure 7.18: Population estimates using two different variance models 7.6 More models More complicated models can be built with these tools. The limitation is going to be the potential overparametrization of the model and the failure to fit if the data isn’t informative enough. For example, Figure 7.19 shows a model where the age effect is modelled as a smoother throughout years independent from each other, with the exception of ages 9 and 10 which share their parameters. fmod &lt;- ~ factor(age) + s(year, k=10, by = breakpts(age, c(0:8))) fit &lt;- sca(ple4, ple4.indices, fmod) Figure 7.19: F-at-age as thin plate spline with 3 knots for each age A quite complex model that implements a cohort effect can be set through the following formula. Figure 7.20 shows the resulting fishing mortality. Note that in this case we end up with a variable \\(F\\) pattern over time, but rather than using 4 * 10 = 40 parameters, it uses, 4 + 10 + 10 = 24. fmodel &lt;- ~ s(age, k = 4) + s(pmax(year - age, 1957), k = 10) + s(year, k = 10) fit &lt;- sca(ple4, ple4.indices, fmodel=fmodel) Figure 7.20: F-at-age with a cohort effect. The following model is applied to the vmodel and it introduces an time trend to reflect the increase in precision in more recent years with improvements in sampling design and increase in sampling effort. vmod &lt;- list( ~ s(age, k = 3) + year, ~1, ~1, ~1, ~1, ~1, ~1 ) fit &lt;- sca(ple4, ple4.indices, vmodel=vmod) Figure 7.21: Catch at age variance model with a year effect. This model fits two smoothers to different sets of ages. fmod &lt;- ~s(age, k = 3, by = breakpts(age, 5)) + s(year, k = 10) fit &lt;- sca(ple4, ple4.indices, fmodel = fmod) Figure 7.22: Smoothers fitted to two sets of ages, 1 to 4 and 5 to 10. flsts &lt;- FLStocks(nowgt=ple4+fit0, wgt=ple4 + fit1) plot(flsts) 7.7 Working with covariates In linear model one can use covariates to explain part of the variance observed on the data that the ‘core’ model does not explain. The same can be done in the a4a framework. The example below uses the North Atlantic Oscillation (NAO) index to model recruitment. nao &lt;- read.table(&quot;https://www.cpc.ncep.noaa.gov/products/precip/CWlink/pna/norm.nao.monthly.b5001.current.ascii.table&quot;, skip=1, fill=TRUE, na.strings=&quot;-99.90&quot;) dnms &lt;- list(quant=&quot;nao&quot;, year=1950:2024, unit=&quot;unique&quot;, season=1:12, area=&quot;unique&quot;) nao &lt;- FLQuant(unlist(nao[,-1]), dimnames=dnms, units=&quot;nao&quot;) nao &lt;- seasonMeans(trim(nao, year=dimnames(stock.n(ple4))$year)) First by simply assuming that the NAO index drives recruitment (Figure 7.23). srmod &lt;- ~ s(nao, k=10) fit2 &lt;- sca(ple4, ple4.indices[1], qmodel=list(~s(age, k=4)), srmodel=srmod, covar=FLQuants(nao=nao)) Figure 7.23: Recruitment model with covariates. Using the NAO index as a recruitment index. In a second model we’re using the NAO index not to model recruitment directly but to model one of the parameters of the S/R function (Figure 7.24). srmod &lt;- ~ ricker(a=~nao, CV=0.25) fit3 &lt;- sca(ple4, ple4.indices[1], qmodel=list(~s(age, k=4)), srmodel=srmod, covar=FLQuants(nao=nao)) Figure 7.24: Recruitment model with covariates. Using the NAO index as a covariate for the stock-recruitment model parameters. Note that covariates can be added to any submodel using the linear model capabilities of R. 7.8 Assessing files The framework gives access to all files produced to run the ADMB fitting routine through the argument wkdir. When set up, all the ADMB files will be left in the directory. Note that the ADMB tpl file is distributed with the FLa4a. One can get it from your R library, under the folder [myRlib]/FLa4a/admb/. fit1 &lt;- sca(ple4, ple4.indices, wkdir=&quot;fit1run&quot;) 7.9 Missing observations in the catch matrix or index Missing observations are encoded as NA, and usually occur if there was no sampling for a year, or, since we model observations on the log scale, if the observation was zero. The a4a framework can deal with missing observations in the catches and indices. The example below shows how to set up a model with missing observations in the catch matrix, to demonstrate the effect of missing observations, using the default model settings. fit &lt;- sca(ple4, ple4.indices) ple4_missing &lt;- ple4 catch.n(ple4_missing)[ac(1:2), &quot;2013&quot;] &lt;- NA fit_missing &lt;- sca(ple4_missing, ple4.indices) In effect, the information on \\(F\\) and \\(Q\\) for the missing observations is inferred from the structural assumptions of the model. If a separable \\(F\\) model is used, the value of \\(F\\) at a given age is derived from its relationship with \\(F\\) at other ages in the same year, as well as from the temporal relationship of \\(F\\) across years for ages with available data. The same principle applies to any other submodel. The impact of missing observations is illustrated in Figure 7.25, which shows box plots of the predicted catch at age, incorporating estimation error. When observations are missing, the resulting estimates for those ages are both different and more uncertain. Additionally, the estimates for nearby years are affected, although the influence of the missing data diminishes with time—estimates from years further away converge toward those obtained using the full dataset. Figure 7.25: Stock estimates with missing observations. This is a simple example, but the same principle applies to more complex models. However, if there are many missing observations, the model cannot be too flexible; otherwise, it won’t be able to reliably estimate the missing data. In any case, one can always add more structure to the model to help address missing information. A common approach is to include a stock-recruitment relationship, which links the spawning stock biomass to recruitment. The example above would definitely benefit from this approach, as the missing information pertains to the first age group. See Figure ??, estimates are much more similar, although estimates from the fit to the missing data dataset show more uncertainty, as expected. # bevholt s/r CV was tweaked to give best results for the example fit2 &lt;- sca(ple4, ple4.indices, srmodel=~bevholt(CV=0.16)) fit_missing2 &lt;- sca(ple4_missing, ple4.indices, srmodel=~bevholt(CV=0.16)) Figure 7.26: Stock estimates with missing observations. Another point to note, is that if observations are systematically missing, for example due to the actual observation being below a detection limit, or zero, then the model may overestimate the true catch at age. This is a common problem in stock assessment models, and is not unique to the a4a framework. Proposed solutions to this issue are to replace zeros with a small number, or half of the smallest observed value. "],["diagnostics.html", "8 Diagnostics 8.1 Residuals 8.2 Predictive skill 8.3 Aggregated catch in weight 8.4 Fit summary, information and cross-validation metrics 8.5 Retrospective analysis 8.6 Hindcast", " 8 Diagnostics Statistical model diagnostics are essential to identify potential issues in the model, such as violations of assumptions, outliers, and influential data points. Without proper diagnostics, fitted models may provide misleading conclusions due to violated assumptions or undetected anomalies. For instance, residual analysis is a widely used diagnostic method that assesses the discrepancy between observed and fitted values, helping to validate underlying model assumptions (Hickey et al. 2019) The a4a framework implements several analysis of residuals, visualizations and statistics, that can be used to evaluate the fit quality and chose across multiple fits. For demonstration purposes we’ll use the plaice in ICES area 4 stock with a plus group at age 9, a single index “BTS-Combined (all)”, and trimmed time series to the most recent 15 years. # use single indices and set plus group at 9 idx &lt;- ple4.indices[c(&quot;BTS-Combined (all)&quot;)] idx[[1]] &lt;- idx[[1]][1:9] stk &lt;- setPlusGroup(ple4, 9) iy &lt;- 2003 # fit fmod &lt;- ~ s(age, k = 8) + s(year, k = 29) + te(age, year, k = c(6, 15)) qmod &lt;- list(~factor(age)) n1mod &lt;- ~s(age, k=5) vmod &lt;- list(~s(age, k=3), ~s(age, k=3)) fit &lt;- sca(stk, idx, fmodel=fmod, qmodel=qmod, n1model=n1mod, vmodel=vmod) 8.1 Residuals Residuals are a ubiquos metrics to check quality of a fit. For sca() fits there are out-of-the-box methods to compute in the log scale, raw residuals (aka deviances), standardized residuals and pearson residuals. A set of plots to inspect residuals and evaluate fit quality and assumptions are implemented. Consider \\(x_{ay}\\) to be either a catch-at-age matrix (\\(C_{ay}\\)) or one abundance index (\\(I_{ay}\\)) and \\(d_{ay}\\) to represent residuals. Raw residuals are compute by \\(d_{ay} = \\log{x_{ay}} - \\log{\\tilde{x}_{ay}}\\) and have distribution \\(N(0,\\upsilon^2_{a})\\). Standardized residuals will be compute with \\(d^s_{ay} = \\frac{d_{ay}}{\\hat{\\upsilon}^2_{a}}\\) where \\(\\hat{\\upsilon}^2_{a} = (n-1)^{-1} \\sum_y(d_{ay})^2\\). Pearson residuals scale raw residuals by the estimates of \\(\\sigma^2\\) or \\(\\tau^2\\), as such \\(d^p_{ay} = \\frac{d_{ay}}{\\tilde{\\upsilon}^2_{a}}\\) where \\(\\tilde{\\upsilon}^2_{a} = \\tilde{\\sigma}^2_{a}\\) for catches, or \\(\\tilde{\\upsilon}^2_{a} = \\tilde{\\tau}^2_{a}\\) for each index of abundance. The residuals() method computes these residuals and generate a object which can be plotted using a set of out-of-the-box methods. The argument type will allow the user to chose which residuals will be computed. By default the method computes standardized residuals. # residuals d_s &lt;- residuals(fit, stk, idx) # shorten time series d_s &lt;- window(d_s, start=iy) Figure 8.1 shows a scatterplot of standardized residuals with a smoother to guide (or mis-guide …) your visual analysis. Note that the standardization should produce normal residuals with variance=1, which means that most residual values should roughly be between \\(-2\\) and \\(2\\). plot(d_s) Figure 8.1: Standardized residuals for abundance indices and catch numbers (catch.n). Each panel is coded by age class, dots represent standardized residuals and lines a simple smoother. When plotting residuals, by default the auxiliar line is a smoother. However it’s possible to use other type of lines by setting the argument auxline in plot (check panel.xyplot help page for more information), of which the relevant ones for residuals are shown in Table 8.1. If type has more than one element, an attempt is made to combine the effect of each of the components. Table 8.1: Values for the argument auxline of residual plots Argument Description l lines h draws vertical (or horizontal if horizontal = TRUE) line segments from the points to the origin s like “l” in the sense that they join consecutive points, but instead of being joined by a straight line, points are connected by a vertical and a horizontal segment forming a ‘step’, with the vertical segment coming first S same as s but the horizontal segment coming first g adds a reference grid r adds a linear regression line smooth adds a loess fit spline adds a cubic smoothing spline fit a draws line segments joining the average y value for each distinct x value Figure 8.2 shows a regression line over the residuals instead of the loess smooother with a grid on the background. plot(d_s, auxline=c(&quot;r&quot;,&quot;g&quot;)) Figure 8.2: Standardized residuals for abundance indices and catch numbers (catch.n). Each panel is coded by age class, dots represent standardized residuals and lines a regression fit. Pearson residuals can be computed and plotted the same way as standardized residuals by setting type='pearson' (Figure 8.3). d_p &lt;- residuals(fit, stk, idx, type=&#39;pearson&#39;) # shorten time series d_p &lt;- window(d_p, start=iy) plot(d_p) Figure 8.3: Pearson residuals for abundance indices and catch numbers (catch.n). Each panel is coded by age class, dots represent standardized residuals and lines a simple smoother. Finally, the raw residuals are computed by setting type='deviances' and plotted the same way as before (Figure 8.4). These residuals are usefull to identify which data points are not well modelled, showing a large dispersion of the residuals and requiring more attention from the analyst. d_r &lt;- residuals(fit, stk, idx, type=&#39;deviances&#39;) # shorten time series d_r &lt;- window(d_r, start=iy) plot(d_r) Figure 8.4: Raw residuals for abundance indices and catch numbers (catch.n). Each panel is coded by age class, dots represent standardized residuals and lines a simple smoother. The above plots can be done by age while grouping by year, instead of by year grouping by ages, the default, in which case it can help distinguish non-modeled structural year effects. The plot argument by needs to be set as age. plot(d_s, by=&#39;age&#39;, auxline=c(&quot;h&quot;, &quot;g&quot;)) Figure 8.5: Standardized residuals for abundance indices and catch numbers (catch.n). Each panel is coded by year, dots represent standardized residuals and lines a simple smoother. The common bubble plot (bubble()) are shown in Figure 8.6. It shows the same information as Figure 8.1 but in a multivariate perspective. bubbles(d_s) Figure 8.6: Bubbles plot of standardized residuals for abundance indices and for catch numbers (catch.n). Figure 8.7 shows a quantile-quantile plot to assess how well standardized residuals match a normal distribution. qqmath(d_s) Figure 8.7: Quantile-quantile plot of standardized residuals for abundance indices and catch numbers (catch.n). Each panel is coded by age class, dots represent standardized residuals and lines the normal distribution quantiles. 8.2 Predictive skill An important feature of stock assessment model fits is the capacity to predict, since one of the most important analysis done with these fits is forecasting future fishing opportunities under pre-defined conditions. The a4a framework implements a visualization of the fit’s predictive skill for both catch-at-age and abundance indices. These are generated by the method plot() with the fit object and a FLStock (Figure 8.8) or FLIndices (Figure 8.9) object as arguments. As before the objects will be shortened to the most recent 15 years for demonstration purposes. plot(fit, stk) Figure 8.8: Predict and observed catch-at-age plot(fit, idx) Figure 8.9: Predict and observed abundance-at-age 8.3 Aggregated catch in weight Although a statistical catch-at-age model assumes errors in catch-at-age and, as such, errors in the total catch in weight, there’s still interest to evaluate how close the model estimates are of the observed catch in weight, even if reported catch in weight is one of the less reliable pieces of information available for stock assessment. The implementation of this diagnopstics is done through the method computeCatchDiagnostics(), which can be visualized with plot() (Figure 8.10). fmod &lt;- ~ factor(age) + factor(year) + te(age, year, k = c(5, 15)) fit &lt;- sca(ple4, ple4.indices, fmodel=fmod) c_d &lt;- computeCatchDiagnostics(fit, ple4) plot(c_d) Figure 8.10: Diagnostics for age aggregated catch in weight The plot method takes 2 important arguments in this case, type and probs. Thre first allows the analyst to choose between all, the plot in Figure 8.10, and prediction (Figure 8.11), which reports prediction error, median estimates and observations. The latter, a vector of 2 values, refers to the confidence intervals to be computed. plot(c_d, type=&quot;prediction&quot;, probs=c(0.025, 0.975)) Figure 8.11: Prediction of aggregated catch in weight 8.4 Fit summary, information and cross-validation metrics To get information about the likelihood fit the method fitSumm() can be used to report number of parameters (npar), negative log-likelkihood (nlogl), ADMB maximum gradient par (maxgrad), number of observations (nobs), generalized cross validation score (gcv), convergence flag (convergence) and acceptance rate (accrate) relevant for MCMC fits only. The GCV is implemented as described by Wood (2017), where the author explains that minimizing the GCV score helps balance the trade-off between model fit and smoothness, effectively preventing overfitting by penalizing excessive complexity. For more information on the other metrics check Project (2013) and Cole C. Monnahan, Muradian, and Kuriyama (2014). The second part refers to the likelihood value for each component. The first component is catch-at-age, components after the first are for indices and the last component is for the recruitment model, if set. fit &lt;- sca(ple4, ple4.indices, srmodel=~bevholt(CV=0.2)) fitSumm(fit) ## iters ## 1 ## nopar 2.890000e+02 ## nlogl -2.458172e+02 ## maxgrad 1.382240e-04 ## nobs 1.728000e+03 ## gcv 1.240412e-01 ## convergence 0.000000e+00 ## accrate NA ## nlogl_comp1 -1.054070e+03 ## nlogl_comp2 6.733170e+01 ## nlogl_comp3 5.910930e+01 ## nlogl_comp4 3.965180e+02 ## nlogl_comp5 6.138520e+01 ## nlogl_comp6 6.335820e+01 ## nlogl_comp7 2.962780e+01 ## nlogl_comp8 1.309180e+02 Information criteria based metrics are reported with the methods AIC and BIC, check Ding (2023) for a primer on these metrics. According to Ding (2023) the AIC can lead to the selection of more complex models that may fit the data better, especially in smaller sample sizes, while the BIC incorporates a stronger penalty for model complexity. In both cases, when comparing models, the lower the score the better the model fits according to these information criteria. AIC(fit) ## [1] 86.36567 BIC(fit) ## [1] 1662.78 8.5 Retrospective analysis Retrospective analysis involves sequentially removing the most recent year of data, refitting the model, and comparing key metric estimates, e.g. the estimate of fishing mortality in year \\(y_{-1}\\) across different fits, using a statistic like Mohn’s rho (Mohn 1999). The rationale is that a well-fitted, stable model should yield consistent estimates despite changes in the data. This method originated from Virtual Population Analysis (VPA) (Pope 1972), which estimates fishing mortality and abundance by working backward from the most recent years. In contrast, retrospective analysis is less directly applicable to statistical catch-at-age models, as these models typically start from the beginning of the time series and the youngest age class, working forward through time. As noted by Cadrin (2025), there is a risk of circular reasoning when this method is used to both diagnose and validate stock assessment models. Nevertheless, many experts still rely on retrospective analysis to evaluate model performance. fit0 &lt;- sca(ple4, ple4.indices) n &lt;- 5 nret &lt;- as.list(1:n) stks &lt;- FLStocks(lapply(nret, function(x){window(ple4, end=(range(ple4)[&quot;maxyear&quot;]-x))})) idxs &lt;- lapply(nret, function(x){window(ple4.indices, end=(range(ple4)[&quot;maxyear&quot;]-x))}) fits &lt;- scas(stks, idxs, fmodel=list(fmodel(fit0))) stks &lt;- stks + fits stks[[6]] &lt;- ple4 + simulate(fit0, 250) Note fmodel doesn’t change: ## $fit1 ## ~te(age, year, k = c(6, 30), bs = &quot;tp&quot;) + s(age, k = 6) ## &lt;environment: 0x5e2b5868bf38&gt; ## ## $fit2 ## ~te(age, year, k = c(6, 30), bs = &quot;tp&quot;) + s(age, k = 6) ## &lt;environment: 0x5e2b5868bf38&gt; ## ## $fit3 ## ~te(age, year, k = c(6, 30), bs = &quot;tp&quot;) + s(age, k = 6) ## &lt;environment: 0x5e2b5868bf38&gt; ## ## $fit4 ## ~te(age, year, k = c(6, 30), bs = &quot;tp&quot;) + s(age, k = 6) ## &lt;environment: 0x5e2b5868bf38&gt; ## ## $fit5 ## ~te(age, year, k = c(6, 30), bs = &quot;tp&quot;) + s(age, k = 6) ## &lt;environment: 0x5e2b5868bf38&gt; The retrospective plot shown below presents the current fit with uncertainty and each retrospective fit on top. If the retrospective fit is not within the confidence interval of the current fit the analyst can argue that the estimate is different and as such reflecting a “poor” fit. plot(window(stks, start=2005)) Figure 8.12: Retrospective analysis of the plaice in ICES area IV stock. Fixed F model. One could use specific submodels and pass them to the fitting function scas, including with some adjustments to take into account the data reduction. In the next example the fishing mortality model is set reducing the smoothness taking into account the length of the dataset. Not considering the adjustment of the model to the new dataset may result in comparisons across models which are very different due to the relationship between information contained in the data and the number of parameters in the model. This issue is more relevant for stocks with shorter time series. n &lt;- 5 nret &lt;- as.list(1:n) stks &lt;- FLStocks(lapply(nret, function(x){window(ple4, end=(range(ple4)[&quot;maxyear&quot;]-x))})) idxs &lt;- lapply(nret, function(x){window(ple4.indices, end=(range(ple4)[&quot;maxyear&quot;]-x))}) # each model will have smootheness scaled to length of time series fmod &lt;- lapply(stks, defaultFmod) fits &lt;- scas(stks, idxs, fmodel=fmod) stks &lt;- stks + fits stks[[6]] &lt;- ple4 + simulate(fit0, 250) Note fmodel changes: ## $fit1 ## ~te(age, year, k = c(6, 30), bs = &quot;tp&quot;) + s(age, k = 6) ## &lt;environment: 0x5e2b537df570&gt; ## ## $fit2 ## ~te(age, year, k = c(6, 29), bs = &quot;tp&quot;) + s(age, k = 6) ## &lt;environment: 0x5e2b53633a68&gt; ## ## $fit3 ## ~te(age, year, k = c(6, 29), bs = &quot;tp&quot;) + s(age, k = 6) ## &lt;environment: 0x5e2b6690a450&gt; ## ## $fit4 ## ~te(age, year, k = c(6, 28), bs = &quot;tp&quot;) + s(age, k = 6) ## &lt;environment: 0x5e2b670253b8&gt; ## ## $fit5 ## ~te(age, year, k = c(6, 28), bs = &quot;tp&quot;) + s(age, k = 6) ## &lt;environment: 0x5e2b67a869e0&gt; And the retrospective plot plot(window(stks, start=2005)) Figure 8.13: Retrospective analysis of the plaice in ICES area IV stock. Updating F model. 8.6 Hindcast A hindcast is a method used in modeling and simulation where historical data is used to test and validate predictive models. In a hindcast, known outcomes from the past are compared with the model’s predictions to assess the model’s accuracy and performance. The primary goal of hindcasting is to improve the reliability and accuracy of predictive models by identifying discrepancies between predicted and actual outcomes and adjusting model parameters accordingly (Mason and Mimmack 2002). The term retroactive forecasting is used by Mason and Mimmack (2002) to denote the form of hindcasting in which forecasts are made for past years (e.g. 2006–2010) using data prior to those years (perhaps 1970–2005). The terminology ex-post is used in business forecasting, referring to predictions for historical periods for which verification data are already available at the time of forecast. For this exercise we’ll use the package a4adiags hindcast method, which follows the suggestions by Carvalho et al. (2021) and Laurence T. Kell, Kimoto, and Kitakado (2016). The hindacast is carried out by sequentially removing the most recent year in the data, similar to a retrospective analysis, refit the stock assessment model and project one year ahead. The Mean Absolute Scale Error (MASE) (Carvalho et al. 2021) is used to assess the predictive skill, a score higher than 1 indicates that the model forecasts have less skill than a random walk. library(a4adiags) theme_set(theme_bw()) nyears &lt;- 5 # set number of year for average biology and selectivity nsq &lt;- 3 hc &lt;- a4ahcxval(ple4, ple4.indices, nyears = nyears, nsq = nsq) Figure 8.14 depicts the hincast results for the abudance indices used in the assessment. The MASE value is included in the strip above the plot. In this case one can see that 3 out of the 5 surveys are not better predictors than a random walk. Figure 8.14: Survey predictions of year ahead indices in hindcast process. The MASE is presented in the strip about the index and is related to the predictive skill of the index. References "],["uncertainty.html", "9 Uncertainty 9.1 The simulate and predict methods 9.2 Prediction uncertainty 9.3 confidence interval coverage 9.4 Propagate uncertainty into stock assessment", " 9 Uncertainty Uncertainty is a fundamental aspect of scientific advice, serving as a reflection of the inherent limitations within the knowledge base used to construct evidence and support scientific opinions. It highlights the gaps, variability, and potential biases present in data, methods, and modeling assumptions that underlie scientific conclusions. In fisheries science, a field that has evolved primarily to provide rigorous, evidence-based advice for the sustainable exploitation of marine and freshwater resources, acknowledging and addressing uncertainty is especially critical. Given the dynamic, complex, and partially observable nature of aquatic ecosystems, the need to systematically characterize and communicate uncertainty is paramount to ensuring robust and credible assessments (Privitera-Johnson &amp; Punt, 2020). Uncertainty is not a weakness in fisheries science; rather, it is a hallmark of scientific integrity and a crucial factor in effective management. Fisheries operate within dynamic, complex ecosystems where variability in recruitment, environmental conditions, and socio-economic pressures make complete predictability impossible. Incorporating uncertainty explicitly into stock assessments and management frameworks strengthens the credibility and resilience of advice. In practice, failing to address uncertainty in fisheries advice can lead to overexploitation, stock collapses, and reduced economic returns. Modern fisheries governance must therefore integrate uncertainty not only at the scientific level but also within decision-making and policy frameworks. Embracing uncertainty promotes a culture of precaution, transparency, and resilience, ensuring that fisheries remain productive and viable for future generations (Folkesson, 2010). In this section we’ll address two important elements of quantifying uncertainty in stock assessment results, prediction error and propagation of uncertainty across modelling stages. The two workhorses for this topic are predict() and simulate() which are implemented in FLa4a, adapted to work with several classes. For sca fits these methods work only if fit = \"assessment\", and not fit = \"MP\", since the later doesn’t compute the variance-covariance matrix of the parameters, which is essential for simulating. This chapter is based on the following model: fmod &lt;- ~ s(age, k = 8) + s(year, k = 29) + te(age, year, k = c(6, 15)) fit0 &lt;- sca(ple4, ple4.indices, fmodel=fmod) submodels(fit0) ## fmodel: ~s(age, k = 8) + s(year, k = 29) + te(age, year, k = c(6, 15)) ## srmodel: ~factor(year) ## n1model: ~s(age, k = 3) ## qmodel: ## BTS-Isis-early: ~s(age, k = 6) ## BTS-Combined (ISIS and TRIDENS): ~s(age, k = 6) ## SNS: ~s(age, k = 5) ## BTS-Combined (all): ~s(age, k = 6) ## IBTS_Q3: ~s(age, k = 6) ## IBTS_Q1: ~s(age, k = 5) ## vmodel: ## catch: ~s(age, k = 3) ## BTS-Isis-early: ~1 ## BTS-Combined (ISIS and TRIDENS): ~1 ## SNS: ~1 ## BTS-Combined (all): ~1 ## IBTS_Q3: ~1 ## IBTS_Q1: ~1 9.1 The simulate and predict methods 9.1.1 predict() The predict method computes the quantities of interest using the estimated coefficients and the design matrix of the model, defined via the formulas in the submodels. The method uses a fitted model object, created by a call to sca, and returns a list with one element for each submodel, where each element is a FLQuants object. fit.pred &lt;- predict(fit0) lapply(fit.pred, names) ## $stkmodel ## [1] &quot;harvest&quot; &quot;rec&quot; &quot;ny1&quot; ## ## $qmodel ## [1] &quot;BTS-Isis-early&quot; &quot;BTS-Combined (ISIS and TRIDENS)&quot; ## [3] &quot;SNS&quot; &quot;BTS-Combined (all)&quot; ## [5] &quot;IBTS_Q3&quot; &quot;IBTS_Q1&quot; ## ## $vmodel ## [1] &quot;catch&quot; &quot;BTS-Isis-early&quot; ## [3] &quot;BTS-Combined (ISIS and TRIDENS)&quot; &quot;SNS&quot; ## [5] &quot;BTS-Combined (all)&quot; &quot;IBTS_Q3&quot; ## [7] &quot;IBTS_Q1&quot; The stkmodel element reports harvest, rec and ny1. The qmodel reports one FLQuant for each index, and the vmodel element returns one FLQuant for catch (in fact catch.n) and one for each index. This allows easy access to the parameterised parts of the model, for example the initial population structure, ny1, can be accessed via fit.pred$stkmodel$ny1. ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 1957 ## 1 475697 ## 2 338143 ## 3 275944 ## 4 219853 ## 5 166943 ## 6 118746 ## 7 78637 ## 8 48780 ## 9 28840 ## 10 16647 ## ## units: 1000 If the fitted object has iterations, as after using the simulate method, predict will be applied to each iter, generating distributions of the above mentioned quantities. (#fig:sim_ny1)Simulations from the model prediction of initial age structure 9.1.2 simulate() As the name implies simulate is used to generate simulations of the fit. It operates over objects of class a4aFitSA using the method mvrnorm() provided by the R package (MASS?). The method generates random draws from a multivariate normal distribution with mean given by the coefficients of the model, and variance matrix given by the estimated covariance matrix of the coefficients (in practice this is a submatrix of the inverse of the hessian matrix). The method approximates the joint distribution of the model parameters as a multivariate normal in the log space, which is inline with the assumption made by ADMB when fitting the model. This approach is called ‘parametric bootstrap’, and it’s a common method for generating uncertainty in the parameters of a model. simulate() operates at the submodel level, e.g. simulate(fit0@pars@qmodel, nsim=250), when called over a a4aFitSA object the method simply runs simulate() over each of the submodels. In this case it returns an object of the same class with model parameters replaced by nsim simulated parameters and updated slots stock.n, catch.n and harvest. Figure 9.1 depicts the distribution of a parameter, the observation error of the first survey index. hist( exp(coef(sim.fit)$vmodel[[2]]), main = &quot;250 draws of a model parameter&quot;, nclass = 10, xlab = &quot;Survey index observation error&quot; ) Figure 9.1: Histogram of 250 draws from the approximate distribution of the estimate of survey observation error. In some simulations studies one needs to make sure the random draws are the same, which in R is obtained by explicitly setting the random seed the pseudo-random generator will use to initiate the process of randomization with the method set.seed(). The same is achieved in this case as the example below shows. set.seed(1234) sim.fit1 &lt;- simulate(fit0, nsim = 250) set.seed(1234) sim.fit2 &lt;- simulate(fit0, nsim = 250) all.equal(sim.fit1, sim.fit2) ## [1] TRUE If the whole stock is of interest, for example, to inspect model predictions of \\(SSB\\), the user should make use of the + with a fitted object including iterations. In such case the stock.n, catch.n and harvest slots of the stock object will be updated and the usual metrics can be computed and extracted, e.g. ssb(stk.pred). Figure 9.2: Simulations from the model prediction of initial age structure fits &lt;- simulate(fit0, 250) flqs &lt;- FLQuants(sim=iterMedians(stock.n(fits)), det=stock.n(fit0)) xyplot(data~year|factor(age), groups=qname, data=flqs, type=&quot;l&quot;, scales=list(y=list(relation=&quot;free&quot;, draw=FALSE)), auto.key=list(points=FALSE, lines=TRUE, columns=2), par.settings=list(superpose.line=list(col=c(&quot;gray35&quot;, &quot;black&quot;)), strip.background=list(col=&quot;gray90&quot;)), ylab=&quot;&quot;, layout=c(5,2)) Figure 9.3: Median simulations VS fit stks &lt;- ple4 + fits plot(stks) Figure 9.4: Stock summary of the simulated and fitted data 9.2 Prediction uncertainty 9.3 confidence interval coverage 9.4 Propagate uncertainty into stock assessment In a multistage stock assessment process as described in this book, it’s important to be able to propagate uncertainty across the different stages. This section describes methods to propagate uncertainty across stages and compares their outcomes in terms of stock assessment outputs. The idea is to add uncertainty as one moves from one stage to the next. If a stock has uncertainty on it’s growth parameters, or natural mortality, or any other quantity estimated or set during the input data preparation, the model fit uncertainty will be added to it by generating iterations in the input data which are then used to fit the stock assessment model. The suggested workflow is: Add uncertainty in growth or M parameters. Draw from the parameters distribution. Compute metrics for stock assessment. If there’s uncertainty in growth parameters use slicing to created iterations of metrics by age, e.g. catch at age and index at age. If there’s uncertainty in M parameters draw from the distribution and generate iterations of the M matrix. If both draw from growth and M parameters, potentially having into account correlation between those parameters, and generate iterations of age based metrics and M. Fit the stock assessment model to each iteration Simulate from each fit Aggregate results in single FLStock object. In this section we give an example of how uncertainty in natural mortality, set up using the m() method and the class a4aM (see chapter XX), is propagated through the stock assessment. We’ll use the stock of Red Mullet in the Mediterranean GSA 1 (see Introduction for details) and 3 methods to add estimation uncertainty (step 5 above): Take one draw of the fit Take n draws of the fit and summarize with the median Take n draws of the fit and combine all These outcomes will be compared with a fit across M iterations without any sampling from the fit. Using a4a methods we’ll model natural mortality using a negative exponential model by age, Jensen’s estimator for the level and no time trend. We include multivariate normal uncertainty using the mvrnorm() method and create 250 iterations. nits &lt;- 250 shape &lt;- FLModelSim(model=~exp(-age-0.5)) level &lt;- FLModelSim(model=~k^0.66*t^0.57, params = FLPar(k=0.4, t=10), vcov=matrix(c(0.002, 0.01,0.01, 1), ncol=2)) #trend &lt;- FLModelSim(model=~b, params=FLPar(b=0.5), vcov=matrix(0.02)) m4 &lt;- a4aM(shape=shape, level=level) m4 &lt;- mvrnorm(nits, m4) range(m4)[] &lt;- range(stk00)[] range(m4)[c(&quot;minmbar&quot;,&quot;maxmbar&quot;)]&lt;-c(1,1) flq &lt;- m(m4)[] quant(flq) &lt;- &quot;age&quot; stk0 &lt;- propagate(stk00, nits) m(stk0) &lt;- flq The M matrix for this stock is shown in Figure9.5). Figure 9.5: Natural mortality generated from M model’s parameter uncertainty We fit the same model to the new stock object which has uncertainty in the natural mortality and add estimation uncertainty following the methods described above. # create objects to store the results stk01 &lt;- stk0 stk02 &lt;- stk0 stk03 &lt;- propagate(stk00, nits*nits) # run without estimation unceratainty stk04 &lt;- stk00 + sca(stk0, idx00) for(i in 1:nits){ stk &lt;- iter(stk0, i) fit &lt;- sca(stk, idx00) # Method 1 iter(stk01, i) &lt;- stk + simulate(fit, 1) # Method 2 iter(stk02, i) &lt;- qapply(stk + simulate(fit, nits), iterMedians) # Method 3 iter(stk03, (nits*(i-1)+1):(nits*i)) &lt;- stk + simulate(fit, nits) } plot(FLStocks(&quot;M&quot;=stk04, &quot;M + 1 estimation sample&quot;=stk01, &quot;M + estimation median&quot;=stk02, &quot;M + n estimation samples&quot;=stk03)) Figure 9.6: Stock summary. Stock metrics computed over fits including uncertainty in M and estimation uncertainty "],["the-statistical-catch-at-age-stock-assessment-framework-with-markov-chain-monte-carlo-mcmc.html", "10 The statistical catch-at-age stock assessment framework with Markov Chain Monte Carlo (MCMC) 10.1 The MCMC method for sca 10.2 Diagnostics with CODA 10.3 ADMB’s arguments to tune the MCMC algorithm", " 10 The statistical catch-at-age stock assessment framework with Markov Chain Monte Carlo (MCMC) The previous methods were demonstrated using maximum likelihood estimation (MLE). However, ADMB also supports Markov Chain Monte Carlo (MCMC) methods, which provide significant advantages, particularly when working with complex models that involve many parameters. The key difference is that while MLE finds a single best estimate of parameters by maximizing the likelihood function, MCMC offers a broader perspective by generating an entire distribution of possible values. This approach is more informative because it does not just give the most likely estimate but also helps us understand the uncertainty surrounding it. With MCMC, researchers can incorporate prior knowledge and obtain results that are often more realistic and reliable (A. Gelman et al. 2013). This is especially useful when dealing with complicated models where traditional likelihood-based methods struggle, as MCMC allows for efficient exploration of possible solutions without requiring an exact mathematical formulation (Gilks, Richardson, and Spiegelhalter 1995; Robert and Casella 2005). One of the biggest advantages of MCMC is its flexibility when working with models that have irregular behavior, such as those with multiple peaks or abrupt changes in likelihood. Standard MLE methods assume that the likelihood function behaves smoothly, like a well-shaped bowl, but this is rarely true in real-world applications. In fisheries, ecology, and other applied sciences, models often have parameters that interact in complex ways, creating likelihood surfaces with ridges and multiple solutions. In these cases, MLE can easily get stuck in a local peak, failing to find the best possible estimate or underestimating the real uncertainty in the system (Neal 1993). Since MCMC uses a probabilistic sampling approach instead of strict optimization, it moves freely across the entire space of possible values, making it more robust and adaptable to challenging problems (Robert and Casella 2005). Traditional MLE-based uncertainty estimation relies on the Hessian matrix, which essentially measures how quickly the likelihood function changes as parameters vary. This method assumes that the shape of the likelihood function is roughly the same everywhere—meaning that a quadratic (bowl-like) approximation is valid (Pawitan 2001). However, this assumption is often unrealistic, especially in models with many parameters or correlations between them, as is common in fisheries stock assessment models. Furthermore, MLE uncertainty estimates require a large sample size for them to be accurate, which is not always available in real-world applications (Wasserman 2004; Vaart 1998). If these assumptions do not hold, MLE can give misleading confidence intervals, making decision-making riskier. Additionally, MLE assumes that the model is correctly specified—meaning that it accurately represents the real system being studied. If the model is misspecified or overly simplified, the Hessian-based uncertainty estimates may be highly unreliable, requiring alternative approaches like robust standard errors or resampling methods (White 1982). In fields like fisheries science, where models often involve multiple correlated parameters, MCMC provides a much more flexible and realistic way to estimate uncertainty. Unlike MLE, which assumes uncertainty follows a simple symmetrical pattern, MCMC can handle more complex distributions, giving a better representation of real-world variability. This is especially important when estimating key fisheries management indicators, such as spawning stock biomass (\\(SSB\\)) or fishing mortality (\\(F\\)), which influence critical policy decisions. Because MCMC does not impose strict mathematical assumptions about the shape of uncertainty, it produces estimates that are more reflective of real-world conditions, ultimately leading to more informed and reliable management strategies. ADMB’s approach to Markov Chain Monte Carlo (MCMC) enhances Bayesian analysis by efficiently exploring parameter uncertainty in complex models. Unlike standard MCMC tools, ADMB leverages automatic differentiation to improve sampling efficiency and speed (Fournier et al. 2012). It supports various sampling algorithms, including Metropolis-Hastings and Hamiltonian Monte Carlo, which help navigate high-dimensional parameter spaces and complex likelihood structures more effectively. This makes ADMB particularly useful in applied sciences like fisheries and ecology, where uncertainty estimation is crucial for decision-making. Additionally, ADMB provides built-in diagnostics to assess MCMC convergence and reliability, ensuring that posterior distributions are well-explored and results are robust (A. Gelman et al. 2013). To evaluate the quality of MCMC sampling, ADMB offers several key diagnostics. Autocorrelation analysis detects dependencies between successive samples, while the effective sample size (ESS) measures the number of independent samples in the chain. The Gelman-Rubin diagnostic (\\(\\hat{R}\\)) helps assess whether multiple chains have converged to the same distribution, with values close to 1 indicating good convergence. Trace plots visually inspect parameter behavior over iterations, revealing trends or poor mixing. Additionally, ADMB monitors the acceptance rate to ensure efficient sampling and provides posterior density estimates to check if the distribution has been properly explored. These tools help users refine their MCMC runs, adjusting sampling length or proposal distributions to improve performance and ensure reliable uncertainty estimates. The manual “A Guide for Bayesian Analysis in AD Model Builder” by Cole C. Monnahan, Melissa L. Muradian and Peter T. Kuriyam describe and explain a larger group of arguments that can be set when running MCMC with ADMB, which the a4a uses. 10.1 The MCMC method for sca This section shows how the sca methods interface with ADMB to use the MCMC fits. For this section we’ll use the hake assessment in mediterranean areas (gsa) 1, 5, 6 and 7. We’ll start buy fitting the MLE model and afterwards call the MCMC methods. The outcomes of the MCMC fit need to be inspected to make sure the chain converged and the results are robust. A set of diagnostics are available to do this work. [TO CHECK] For many Bayesian software platforms, the MCMC algorithms are started at user-specified or arbitrary places. ADMB has the advantage that it can robustly estimate the posterior mode and the covariance at that point. This information is very valuable in initializing the MCMC chain. Specifically, an MCMC chain starts from the posterior mode and uses the estimated covariance matrix in its proposed jumps (see the algorithm sections below). As such, ADMB chains typically do not need a long period to reach areas of high density. However, we caution the user to always check the MCMC output as other issues may lead to a chain that needs a longer burn-in. (Cole C. Monnahan et al. 2019) # load libraries and data library(FLa4a) library(ggplotFL) data(hke1567) data(hke1567.idx) nsim &lt;- 250 # MLE estimate fmod &lt;- ~s(age, k = 4) + s(year, k = 8) + s(year, k = 8, by = as.numeric(age == 0)) + s(year, k = 8, by = as.numeric(age == 4)) qmod &lt;- list(~I(1/(1 + exp(-age)))) fit &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod) fit &lt;- simulate(fit, nsim) To run the MCMC method, one needs to configure a set of arguments, which is done by creating a SCAMCMC object. Table 10.1 describes the arguments available to run the MCMC method, extracted from Monnahan (Cole C. Monnahan et al. 2019). For more details on the MCMC configuration in ADMB visit the ADMB website. Table 10.1: ADMB MCMC arguments Argument Description mcmc N Run N MCMC iterations mcsave N Save every N th MCMC iteration mcscale N Rescale step size for first N iterations mcmult N Rescale the covariance matrix mcrb N Reduce high parameter correlations mcprobe X Use a fat-tailed proposal distribution mcdiag Use a diagonal covariance matrix mcnoscale Do not scale the algorithm during mcu Use a uniform distribution as proposal distribution hybrid Use the hybrid method hynstep N Mean number of steps for the leapfrog method hyeps X The stepsize for the leapfrog method [X numeric and &gt; 0] # mcmc mc &lt;- SCAMCMC() # check the default pars mc ## An object of class &quot;SCAMCMC&quot; ## Slot &quot;mcmc&quot;: ## [1] 10000 ## ## Slot &quot;mcsave&quot;: ## [1] 100 ## ## Slot &quot;mcscale&quot;: ## [1] NaN ## ## Slot &quot;mcmult&quot;: ## [1] NaN ## ## Slot &quot;mcrb&quot;: ## [1] NaN ## ## Slot &quot;mcprobe&quot;: ## [1] NaN ## ## Slot &quot;mcseed&quot;: ## [1] NaN ## ## Slot &quot;mcdiag&quot;: ## [1] FALSE ## ## Slot &quot;mcnoscale&quot;: ## [1] FALSE ## ## Slot &quot;mcu&quot;: ## [1] FALSE ## ## Slot &quot;hybrid&quot;: ## [1] FALSE ## ## Slot &quot;hynstep&quot;: ## [1] NaN ## ## Slot &quot;hyeps&quot;: ## [1] NaN Defaults for now are ok, so lets fit the model. Note that the argument fit must be set to MCMC and the argument mcmc takes the SCAMCMC object. # fit the model fitmc00 &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) # check acceptance rate fitSumm(fitmc00) ## iters ## 1 ## nopar 52 ## nlogl NA ## maxgrad NA ## nobs 176 ## gcv NA ## convergence NA ## accrate NA As usual fitSumm store relevant information about the model fit. In the case of MCMC fit the information stored is the number of model paramters (nopar), the number of observations (nobs) and the acceptance rate (accrate). plot(FLStocks(mle=hke1567 + fit, mc=hke1567 + fitmc00)) Figure 10.1: Stock assessment summaries of maximum likelihood (mle) and monte carlo (mc) fits. 10.2 Diagnostics with CODA In essence, the diagnostics are used to give the analyst confidence that the posterior distribution of the parameters is unbiased, as much as possible with symetric non correlated distributions of each parameter, over which one can make inference. There’s a large body of literature about MCMC convergence. In this section we’ll focus on the out-of-the-box methods for metropolis hastings algorithm available to the stock assessment scientist: trace plots, autocorrelation and cross correlation analysis, geweke diagnostic, Gelman and Rubin’s convergence diagnostic, acceptance rate, cumulative means, distribution density and acceptance rate. These tools should be used together to evaluate proper mixing and convergence. ADMB has an hybrid algorithm based on Hamiltonian dynamic which will not be addressed here. The reader is invited to consult Cole C. Monnahan et al. (2019) for more information. We use the package CODA to run the diagnostics on MCMC fits. One needs to convert the sca output into a mcmc CODA object over which several diagnostics can be ran. The mcmc object is a matrix with the parameters (row = iters, cols= pars). library(coda) For demonstration purposes we’ll create a chain with 1000 samples (mcmc=10000) and save every 10 iters (mcsave=10), which will create a highly correlated and unstable chain, and update the initial MCMC fit to also have 1000 samples (mcmc=100000, mcsave=100). The latter will have lower correlation due to the higher thinning. # update initial fit, control random seed mc &lt;- SCAMCMC(mcmc=100000, mcsave=100, mcseed=10) fitmc01 &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) fitmc01.mc &lt;- FLa4a::as.mcmc(fitmc01) # highly correlated fit, control random seed mc &lt;- SCAMCMC(mcmc=10000, mcsave=10, mcseed=10) fitmc02 &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) fitmc02.mc &lt;- FLa4a::as.mcmc(fitmc02) 10.2.1 Traceplots Trace plots show the sampled values of a parameter over iterations. A plot that looks like a random, stable “cloud” of points with no trends or drifts, with rapid fluctuactions, is a signal of convergence, meaning the chain mixes well and is stationary. If the trace plot shows a strong trend or periodicity, drifts, or long autocorrelated stretches, it means the chain hasn’t converged. Figure 10.2 cleary depicts this difference between the two runs. traceplot(mcmc.list(mc01=fitmc01.mc[,1], mc02=fitmc02.mc[,2]), lwd=2, col=c(2,4), lty=1) Figure 10.2: MCMC chain’s trace for the first parameter. High correlation chain in blue, low correlation chain in red. Ploting the chains for the parameter clearly shows autocorrelation for the first parameter in the blue chain. It also shows an initial phase where the chain seems to be stuck in a single value. This initial phase, when the parameter seems to be stuck in a fixed position, is called the “burn-in” phase. These iterations can be dropped with the burnin method (Figure 10.3), although it doesn’t sort out the autocorrelation or the parameter density. traceplot(FLa4a::as.mcmc(burnin(fitmc02, 250))[,1], lwd=2, col=4, lty=1) Figure 10.3: MCMC chain with high autocorrelation after removing the initial 250 samples (burnin period). 10.2.2 Autocorrelation and crosscorrelation analysis Autocorrelation analysis is useful to assess stationarity, a stationary chain should have low autocorrelation, meaning that each sample is approximately independent. On the opposite, high autocorrelation indicates slow mixing and possible non-stationarity. furthermore, in a good mixed chain autocorrelation drops quickly to near zero, while a poor mixing will display high autocorrelation, meaning successive samples are too correlated, reducing efficiency. The autocorrelation plot produced by the acf function, will show correlation along the chain for each parameter at different lags. Figure 10.4 shows there’s a strong autocorrelation for the first parameter in the blue chain (right panel), which we’d like to avoid. acfplot(fitmc01.mc[,1], lwd=3, main=&quot;Low correlation chain&quot;) acfplot(fitmc02.mc[,1], lwd=3, main=&quot;High correlation chain&quot;) Figure 10.4: Autocorrelation plot of the first parameter in the MCMC chain Crosscorrelation inspects the pairwise correlation of all parameters, which is a useful tool to assess the efficiency of the sampling process and the independence of the generated samples. If cross-correlations are high, it often means that transitions between states are sluggish, leading to an increased autocorrelation time and requiring a larger number of samples to achieve effective independent samples. Conversely, low cross-correlation implies that parameters are explored more independently, leading to faster convergence and better mixing. crosscorr.plot(fitmc01.mc, main=&quot;Low correlation chain&quot;) crosscorr.plot(fitmc02.mc, main=&quot;High correlation chain&quot;) Figure 10.5: Crosscorrelation plots 10.2.3 Geweke diagnostic The geweke diagnostic computes the Geweke-Brooks Z-score (Geweke 1992), which indicates if the first and following parts of a sample from a Markov chain are drawn from the same distribution as the last part of the chain, usualy the last 50% of the samples. It’s useful to decide if the first few iterations should be discarded and provides information about the stability of the chain. Figure 10.6 shows the geweke plot for the MCMC run without thining and Figure 10.6 when the thining was set at 200 samples. geweke.plot(fitmc01.mc[,1], main=&quot;Low correlation chain&quot;) geweke.plot(fitmc02.mc[,1], main=&quot;High correlation chain&quot;) Figure 10.6: Geweke plot of the first parameter in the MCMC chains The panel on the left shows a much more regular chain, where the different blocks of data show similar distributions. The panel on the right clearly shows the z-score statistic out of the confidence intervals until 400 samples are discarded, which points to the need to drop a set of initial samples. The geweke diagnostic is also a good way to look at mixing by comparing the mean and variance of the first part of the chain to the last part. Good mixing will show no significant difference between early and late samples. Poor mixing will show large differences, indicating the chain has not explored the posterior fully. 10.2.4 Cumulative means Inspecting the cumulative mean along the chain is another good way to check for the stability of the chain. When the mixing is good the mean stabilizes quickly, and vice-versa if not. cm01 &lt;- fitmc01.mc[,1] cm01 &lt;- cumsum(cm01) / seq_along(cm01) cm02 &lt;- fitmc02.mc[,1] cm02 &lt;- cumsum(cm02) / seq_along(cm02) plot(cm01, type=&quot;l&quot;, xlab=&quot;samples&quot;, ylab=&quot;mean&quot;, main=&quot;Low correlation chain&quot;) plot(cm02, type=&quot;l&quot;, xlab=&quot;samples&quot;, ylab=&quot;mean&quot;, main=&quot;High correlation chain&quot;) Figure 10.7: Cumulative mean plots of the first parameter in the MCMC chains 10.2.5 Distribution density An important element of MCMC is to produce symetric posterior distributions, for one it’s a sign that the chain explored the space of the parameter, for other it makes inference about the parameters a lot more robust. If the distributions are skewed or multimodal, estimating the expected value and variance becomes a lot more complicated. As such having symetric distributions is preferred and should be checked before computing statistics of interest. Figure 10.8 shows the density plots for both runs, where it shows the symetric distribution of the uncorrelated chain (left panel) and the bimodal distribution of the correlated chain. densplot(fitmc01.mc[,1], main=&quot;Low correlation chain&quot;) densplot(fitmc02.mc[,1], main=&quot;High correlation chain&quot;) Figure 10.8: Density plots of the first parameter in the MCMC chains 10.2.6 Gelman-Rubin statistic The Gelman-Rubin statistic (\\(\\hat{R}\\)) (Andrew Gelman and Rubin 1992) can be used to check if multiple chains have reached a stable state and are properly exploring the target distribution. It compares how much variation exists within each chain to the variation between different chains. If all chains are sampling from the same distribution, these variations should be similar, and \\(\\hat{R}\\) will be close to 1, otherwise, if it’s greater than 1.1 it suggests that the chains have not yet converged. To compute \\(\\hat{R}\\), multiple chains are run with different starting points. The algorithm measures how spread out the samples are within each chain and compares it to how much the chains differ from each other. If the chains have not mixed well, they will appear too different from each other, and \\(\\hat{R}\\) will be large. If the chains have mixed properly, they will have a similar spread, and the statistic will be close to 1. To run another chain one makes use of the mcseed argument to make sure the 2 chains start from different places. The Gelman-Rubin statistics is computed by the gelman.diag method and depicted by the gelman.plot function. Ist’s easy to see the difference between the two fits. While the low corrrelation fit shows values close to 1 for most parameters, the high correlation fit shows a number of large values. # low correlation mc &lt;- SCAMCMC(mcmc=100000, mcsave=100, mcseed=30) fitmc01b &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) fitmc01b.mc &lt;- FLa4a::as.mcmc(fitmc01b) # highly correlated fit mc &lt;- SCAMCMC(mcmc=10000, mcsave=10, mcseed=30) fitmc02b &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) fitmc02b.mc &lt;- FLa4a::as.mcmc(fitmc02b) # create lists for comparison mclst01 &lt;- mcmc.list(a=fitmc01.mc, b=fitmc01b.mc) mclst02 &lt;- mcmc.list(a=fitmc02.mc, b=fitmc02b.mc) gelman.diag(mclst01) ## Potential scale reduction factors: ## ## Point est. Upper C.I. ## fMod:(Intercept) 1.001 1.001 ## fMod:s(age).1 1.024 1.112 ## fMod:s(age).2 1.002 1.005 ## fMod:s(age).3 1.021 1.099 ## fMod:s(year).1 1.001 1.011 ## fMod:s(year).2 1.004 1.020 ## fMod:s(year).3 1.011 1.051 ## fMod:s(year).4 1.008 1.040 ## fMod:s(year).5 1.002 1.013 ## fMod:s(year).6 1.004 1.024 ## fMod:s(year).7 1.001 1.007 ## fMod:s(year):by1..1 0.999 1.000 ## fMod:s(year):by1..2 1.003 1.011 ## fMod:s(year):by1..3 1.001 1.005 ## fMod:s(year):by1..4 1.003 1.013 ## fMod:s(year):by1..5 1.005 1.006 ## fMod:s(year):by1..6 1.018 1.088 ## fMod:s(year):by1..7 0.999 0.999 ## fMod:s(year):by1..8 0.999 1.000 ## fMod:s(year):by2..1 1.008 1.039 ## fMod:s(year):by2..2 0.999 1.001 ## fMod:s(year):by2..3 1.008 1.040 ## fMod:s(year):by2..4 1.000 1.004 ## fMod:s(year):by2..5 1.003 1.018 ## fMod:s(year):by2..6 1.003 1.003 ## fMod:s(year):by2..7 1.007 1.037 ## fMod:s(year):by2..8 1.000 1.001 ## n1Mod:(Intercept) 1.002 1.013 ## n1Mod:s(age).1 1.000 1.003 ## n1Mod:s(age).2 1.001 1.006 ## rMod:(Intercept) 1.000 1.000 ## rMod:factor(year)1 0.999 0.999 ## rMod:factor(year)2 1.000 1.001 ## rMod:factor(year)3 1.002 1.002 ## rMod:factor(year)4 1.002 1.005 ## rMod:factor(year)5 1.000 1.000 ## rMod:factor(year)6 1.005 1.008 ## rMod:factor(year)7 0.999 0.999 ## rMod:factor(year)8 1.010 1.045 ## rMod:factor(year)9 1.001 1.003 ## rMod:factor(year)10 1.000 1.004 ## rMod:factor(year)11 1.001 1.008 ## rMod:factor(year)12 1.011 1.053 ## rMod:factor(year)13 1.003 1.003 ## rMod:factor(year)14 1.004 1.019 ## rMod:factor(year)15 1.008 1.034 ## qMod:IND:(Intercept) 1.003 1.017 ## qMod:IND:I(1/(1 + exp(-age))) 1.004 1.017 ## vMod:catch:(Intercept) 0.999 0.999 ## vMod:catch:s(age).1 1.002 1.013 ## vMod:catch:s(age).2 1.001 1.010 ## vMod:IND:(Intercept) 1.000 1.000 ## ## Multivariate psrf ## ## 1.11 gelman.diag(mclst02) ## Potential scale reduction factors: ## ## Point est. Upper C.I. ## fMod:(Intercept) 1.067 1.25 ## fMod:s(age).1 1.014 1.07 ## fMod:s(age).2 1.047 1.15 ## fMod:s(age).3 1.009 1.05 ## fMod:s(year).1 1.041 1.06 ## fMod:s(year).2 1.118 1.43 ## fMod:s(year).3 1.001 1.00 ## fMod:s(year).4 1.111 1.40 ## fMod:s(year).5 1.012 1.03 ## fMod:s(year).6 1.121 1.44 ## fMod:s(year).7 1.067 1.23 ## fMod:s(year):by1..1 1.022 1.02 ## fMod:s(year):by1..2 1.070 1.25 ## fMod:s(year):by1..3 1.016 1.04 ## fMod:s(year):by1..4 1.055 1.23 ## fMod:s(year):by1..5 1.080 1.21 ## fMod:s(year):by1..6 1.038 1.17 ## fMod:s(year):by1..7 1.048 1.10 ## fMod:s(year):by1..8 1.076 1.30 ## fMod:s(year):by2..1 1.122 1.44 ## fMod:s(year):by2..2 1.044 1.06 ## fMod:s(year):by2..3 1.077 1.26 ## fMod:s(year):by2..4 1.027 1.07 ## fMod:s(year):by2..5 1.093 1.35 ## fMod:s(year):by2..6 1.034 1.13 ## fMod:s(year):by2..7 1.105 1.38 ## fMod:s(year):by2..8 1.044 1.07 ## n1Mod:(Intercept) 1.007 1.01 ## n1Mod:s(age).1 1.007 1.01 ## n1Mod:s(age).2 1.005 1.03 ## rMod:(Intercept) 1.010 1.03 ## rMod:factor(year)1 1.019 1.03 ## rMod:factor(year)2 1.027 1.12 ## rMod:factor(year)3 1.025 1.10 ## rMod:factor(year)4 1.047 1.18 ## rMod:factor(year)5 1.086 1.27 ## rMod:factor(year)6 1.000 1.00 ## rMod:factor(year)7 1.027 1.07 ## rMod:factor(year)8 1.049 1.20 ## rMod:factor(year)9 1.088 1.30 ## rMod:factor(year)10 1.042 1.18 ## rMod:factor(year)11 1.011 1.04 ## rMod:factor(year)12 1.002 1.01 ## rMod:factor(year)13 1.004 1.02 ## rMod:factor(year)14 1.002 1.01 ## rMod:factor(year)15 1.002 1.00 ## qMod:IND:(Intercept) 1.004 1.02 ## qMod:IND:I(1/(1 + exp(-age))) 0.999 1.00 ## vMod:catch:(Intercept) 0.999 1.00 ## vMod:catch:s(age).1 1.007 1.01 ## vMod:catch:s(age).2 1.056 1.18 ## vMod:IND:(Intercept) 1.002 1.01 ## ## Multivariate psrf ## ## 1.98 mclst01 &lt;- mcmc.list(a=fitmc01.mc[,1], b=fitmc01b.mc[,1]) mclst02 &lt;- mcmc.list(a=fitmc02.mc[,1], b=fitmc02b.mc[,1]) gelman.plot(mclst01, main=&quot;Low correlation chain&quot;) gelman.plot(mclst02, main=&quot;High correlation chain&quot;) Figure 10.9: Gelman-Rubin’s diagnostic plots for the first parameter. 10.2.7 Acceptance rate The acceptance rate in Markov Chain Monte Carlo (MCMC) methods plays a crucial role in balancing exploration and efficiency when sampling from a posterior distribution. It represents the proportion of proposed states that are accepted in the Markov chain and directly influences mixing, convergence, and the quality of inference. A low acceptance rate (e.g., &lt;20%) means that most proposed moves are rejected, leading to slow exploration of the posterior distribution. This can result in poor mixing and high autocorrelation between samples (A. Gelman et al. 2013). A high acceptance rate (e.g., &gt;80%) suggests that the proposals are too conservative, leading to small moves and highly correlated samples. Cole C. Monnahan et al. (2019) suggests that the optimal acceptance rate varies by model size, among other things, but is roughly 40%, although models with more parameters should have a lower optimal acceptance rate. Roberts, Gelman, &amp; Gilks, 1997 complementary suggest that for Random Walk Metropolis-Hastings (RWMH) in high-dimensional spaces an optimal acceptance rate is about 23%. The acceptance rate is reported out of the MCMC fit and can be accessed with the method fitSumm. Inspecting the acceptance rate for the models we’re using shows a higher acceptance rate for the high correlation model, although both are above the recommended optimal for high dimentional models, like the models used in stock assessment. cbind(fitSumm(fitmc01), fitSumm(fitmc02)) ## 1 1 ## nopar 52 52 ## nlogl NA NA ## maxgrad NA NA ## nobs 176 176 ## gcv NA NA ## convergence NA NA ## accrate NA NA 10.3 ADMB’s arguments to tune the MCMC algorithm This section is based on Cole C. Monnahan et al. (2019) and describes a set of arguments and methods which the stock assessment analyst can use to tune the MCMC algorythm and be more confident on its convergence and follow up inference. 10.3.1 Thinning rate For the Metropolis-Hastings algorithm, the most important tuning option available to the user is the saving rate (the inverse of the thinning rate). This is the rate at which parameters are saved, such that thinning is effectively discarding draws. This tuning option is critical since this algorithm generates autocorrelated parameters by design. The user controls the thinning rate by the argument mcsave. If N = 1 every single draw is saved (none are thinned out), which generates high autocorrelation, suggesting the need to thin more (save fewer). This is the case of the fitmc02 fit. In fitmc01 mcsave was increased to 100, by increasing the total samples by 100 and saving every 100th. This helps reduce the autocorrelation and produces independent draws from the posterior of interest. 10.3.2 mcscale and mcnoscale ADMB accomplishes this by “scaling” the covariance matrix up or down, depending on the current acceptance rate, during the first part of the chain. Scaling the covariance matrix down produces proposed sets closer to the current set, and vice versa for scaling up. By default, it scales during the first 500 iterations before thining, but the user can specify this with mcscale or turn off scaling with mcnoscale. ADMB rescales the covariance matrix every 200 iterations until the acceptance rate is between 0.15 and 0.4, or the scaling period is exceeded. Draws from this tuning phase should be discarded as part of the burn-in. The code below illustrates the effect in the acceptance rate of not scaling the hessian, the acceptance rate drops significantly, which means poor mixing and higher autocorrelation. # no scale mc &lt;- SCAMCMC(mcmc=100000, mcsave=100, mcseed=10, mcnoscale=TRUE) fitmc01ns &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) fitmc01ns.mc &lt;- FLa4a::as.mcmc(fitmc01ns) data.frame(hessian_scale=c(fitSumm(fitmc01)), hessian_noscale=c(fitSumm(fitmc01ns))) ## hessian_scale hessian_noscale ## 1 52 52 ## 2 NA NA ## 3 NA NA ## 4 176 176 ## 5 NA NA ## 6 NA NA ## 7 NA NA In the next analysis we show the aiutocorrelation statistics for lags of 0, 1, 5, 10 and 50. When the hessian is not scaled the autocorreltion of lag 1 increased from 0.33 to 0.43. data.frame(hessian_scale=c(autocorr.diag(fitmc01.mc[,1])), hessian_noscale=c(autocorr.diag(fitmc01ns.mc[,1]))) ## hessian_scale hessian_noscale ## 1 1.000000000 1.0000000000 ## 2 0.331021210 0.4283436263 ## 3 0.020797197 0.0006163336 ## 4 0.038728809 0.0067920000 ## 5 0.005681063 -0.0193736621 10.3.3 mcprobe For some models, there may be concern of being “stuck” in a local minimum and simply never proposing a value far enough away to escape it and find other regions of high density. Obviously this problem would present issues for maximum likelihood inference as well. ADMB has a built-in algorithm which modifies the default proposal distribution so it occasionally proposes very distant parameters (i.e. “probes”)5. The mcprobe X argument initiates this option. The modified proposal distribution is a mixture distribution of normal and Cauchy distributions. The argument X controls how the two distributions are mixed, with larger values being more Cauchy (fatter tails, larger jumps). The range of valid inputs is 0.00001 to 0.499, and if no value is supplied a default of 0.05 is used6 For some models, there may be concern of being “stuck” in a local minimum and simply never proposing a value far enough away to escape it and find other regions of high density. Obviously this problem would present issues for maximum likelihood inference as well. ADMB has a built-in algorithm which modifies the default proposal distribution so it occasionally proposes very distant parameters (i.e. “probes”). The mcprobe argument initiates this option. The modified proposal distribution is a mixture distribution of normal and Cauchy distributions. The argument X controls how the two distributions are mixed, with larger values being more Cauchy (fatter tails, larger jumps). The range of valid inputs is 0.00001 to 0.499, and if no value is supplied a default of 0.05 is used. # more Cauchy mc &lt;- SCAMCMC(mcmc=1000, mcsave=1, mcseed=10, mcprobe=0.45) fitmc02p &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) fitmc02p.mc &lt;- FLa4a::as.mcmc(fitmc02p) data.frame(probe_0.05=c(fitSumm(fitmc02)), probe_0.45=c(fitSumm(fitmc02p))) ## probe_0.05 probe_0.45 ## 1 52 52 ## 2 NA NA ## 3 NA NA ## 4 176 176 ## 5 NA NA ## 6 NA NA ## 7 NA NA In the next analysis we show the aiutocorrelation statistics for lags of 0, 1, 5, 10 and 50. When the hessian is not scaled the autocorreltion of lag 1 increased from 0.33 to 0.37. data.frame(probe_0.05=c(autocorr.diag(fitmc02.mc[,1])), probe_0.45=c(autocorr.diag(fitmc02p.mc[,1]))) ## probe_0.05 probe_0.45 ## 1 1.00000000 1.0000000 ## 2 0.87352614 0.9771081 ## 3 0.54400301 0.8964793 ## 4 0.33676814 0.7867470 ## 5 0.05451792 0.3936894 10.3.4 mcrb The -mcrb N option (which stands for “rescaled bounded”) alters the covariance matrix used to propose new parameter sets in the Metropolis-Hastings algorithm. Its intended use is to create a more efficient MCMC sampler so the analyses run faster. This option reduces the estimated correlation between parameters. The value of N must be integer and between 1 and 9, inclusive, with lower values leading to a bigger reduction in correlation. The option will be most effective under circumstances where the correlation between parameters at the posterior mode is higher than other regions of the parameter space. In this case, the algorithm may make efficient proposals near the posterior mode, but inefficient proposals in other parts of the parameter space. By reducing the correlation using mcrb the proposal function may be more efficient on average across the entire parameter space and require less thinning (and hence run faster) If poor performance is suspected to be caused by correlations that are too high, the mcrb option provides a quick, convenient to try a reduced correlation matrix in the algorithm # reduce correlation mc &lt;- SCAMCMC(mcmc=10000, mcsave=10, mcseed=10, mcrb=1) fitmc02r &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) fitmc02r.mc &lt;- FLa4a::as.mcmc(fitmc02r) data.frame(rslbound_no=c(fitSumm(fitmc02)), rslbound_high=c(fitSumm(fitmc02r))) ## rslbound_no rslbound_high ## 1 52 52 ## 2 NA NA ## 3 NA NA ## 4 176 176 ## 5 NA NA ## 6 NA NA ## 7 NA NA In the next analysis we show the autocorrelation statistics for lags of 0, 1, 5, 10 and 50. When the hessian is not scaled the autocorreltion of lag 1 increased from 0.33 to 0.37. data.frame(rslbound_no=c(autocorr.diag(fitmc02.mc[,1])), rslbound_high=c(autocorr.diag(fitmc02r.mc[,1]))) ## rslbound_no rslbound_high ## 1 1.00000000 1.0000000 ## 2 0.87352614 0.9907956 ## 3 0.54400301 0.9586125 ## 4 0.33676814 0.9169621 ## 5 0.05451792 0.6132896 References "],["projections-and-harvest-control-rules.html", "11 Projections and harvest control rules 11.1 Simple workflow 11.2 Initial condition assumptions 11.3 Scenarios 11.4 Relative scenarios 11.5 Limits 11.6 Harvest Control Rule", " 11 Projections and harvest control rules Massad et al. (2018) provide a useful distinction between forecasting and projection in scientific prediction, stating that prediction in general science can be categorized into two main components: forecasting and projection. According to their definition, “a forecast is an attempt to predict what will happen, whereas a projection describes what would happen, given certain hypotheses” (Massad et al., 2018). This distinction is also reflected in the Organisation for Economic Co-operation and Development (OECD 2009), which notes that “forecasting” and “prediction” are often used interchangeably in assessing the magnitude that a given variable will assume in the future. In the context of economic and environmental modeling, the OECD distinguishes between medium-term projections, which typically extend five to six years into the future, and other predictive exercises that rely on different methodologies. The term “projection” is generally used in two interrelated senses: (1) as a future value of a time series computed based on specific assumptions about environmental changes, and (2) in probability theory, as the conditional expectation of a variable. Since regression models provide conditional expectations of dependent variables based on predictor variables, this probabilistic use of projection aligns with forecasting and predictive modeling (OECD, 2009). Applying this conceptual framework to fisheries science, the process of advising on future fishing opportunities can be viewed as a combination of forecasting and projection. When using a harvest control rule (HCR) approved by decision-makers to predict catch levels that can be extracted from a stock, the analysis falls into the realm of forecasting, as it predicts what will happen under known system conditions. Conversely, when assessing potential future outcomes based on predefined scenarios—such as climate change effects or policy interventions—the approach aligns more closely with projections. In fisheries science, the distinction between forecasting and prediction is not always explicitly made, with the discipline generally emphasizing broader predictive approaches. It is common practice to run scenarios when advising fisheries managers, testing different assumptions about future ecological and management conditions. Strictly speaking, projections are not part of the stock assessment process itself. Stock assessment concludes when analysts compare estimates of biomass and fishing mortality with reference points, allowing for determinations of whether a stock is overfished or subject to overfishing (Hilborn and Walters 2013). Projections typically follow stock assessments, incorporating estimates or assumptions about population dynamics—growth, reproduction, and natural mortality—to predict future catches, biomass, and abundance under specific conditions and with quantified uncertainty. By integrating both forecasting and projection methodologies, fisheries science can offer robust, scenario-based decision-making tools that aid sustainable resource management (Punt, Smith, and Cui 2001). For this section we’ll be using the package FLasher [] from the FLR family of packages. library(FLa4a) library(FLasher) library(FLBRP) #library(ggplotFL) data(ple4) data(ple4.indices) 11.1 Simple workflow The basic workflow to project with FLasher is to extend the FLStock object to store the predictions using the method fwdWindow, set the targets for the projection with method fwdControl and project the fishery with the method fwd with a FLStock and a FLSR. We’ll start by fitting a model including, a stock recruitment model which will be used to forecast recruitment. We’ll also set the number of iterations we’ll be working with and the time period we want to project. # fit model fmod &lt;- ~ s(age, k = 8) + s(year, k = 25) + te(age, year, k = c(6, 15)) fit00 &lt;- sca(ple4, ple4.indices, srmodel=~geomean(), fmodel=fmod) stk00 &lt;- ple4 + fit00 # create stock recruitment model object sr00 &lt;- as(fit00, &quot;FLSR&quot;) # set projection # number of iterations nsim &lt;- 250 # most recent year in the data maxy &lt;- range(ple4)[&quot;maxyear&quot;] # number of years to project projy &lt;- 5 # last year for projections endpy &lt;- maxy + projy # initial year for projections inipy &lt;- maxy + 1 # extend stock object to store projection&#39;s results stk00 &lt;- fwdWindow(stk00, end = endpy) # set the control for the projections, in this case a fixed f of 0.3 trg00 &lt;- fwdControl(year = inipy:endpy, quant = &quot;f&quot;, value = 0.3) # project stk01 &lt;- fwd(stk00, control=trg00, sr=sr00) Figure 11.1: Projection of stock for 5 years with fixed fishing mortality and recruitment A natural addition to this forecast is to add uncertainty. We’ll do that by generating uncertainty in population numbers, catch numbers and fishing mortality, using simulate, and add stock recruitment uncertainty using the residuals of the fit. stk00 &lt;- ple4 + simulate(fit00, nsim) stk00 &lt;- fwdWindow(stk00, end = endpy) res00 &lt;- residuals(sr00) rec00 &lt;- window(rec(stk00), 2018, 2022) rec00 &lt;- rlnorm(rec00, mean(res00), sqrt(var(res00))) stk02 &lt;- fwd(stk00, control=trg00, sr=sr00, residuals=rec00) Figure 11.2: Stochastic projection of stock for 5 years with fixed fishing mortality and recruitment An alternative to the above workflow is to fit the stock recruitment model after the stock assessment model, using the output of the assessment as input to the stock recruitment fit. In which case stock recruitment estimation uncertainty can be added by fitting the stock recruitment model over stock assessment uncertainty, so that there will be stock-recruitment fit to each iteration generated from the stock assessment model. fit00 &lt;- sca(ple4, ple4.indices) stk00 &lt;- ple4 + simulate(fit00, nsim) sr00 &lt;- as.FLSR(stk00, model=&quot;geomean&quot;) sr00 &lt;- fmle(sr00, control = list(trace = 0)) stk00 &lt;- fwdWindow(stk00, end = endpy) res00 &lt;- residuals(sr00) rec00 &lt;- window(rec(stk00), inipy, endpy) rec00 &lt;- rlnorm(rec00, c(yearMeans(res00)), sqrt(c(yearVars(res00)))) stk03 &lt;- fwd(stk00, control=trg00, sr=sr00, residuals=rec00) Figure 11.3: Stochastic projection of stock for 5 years with fixed fishing mortality and recruitment These two methods don’t give very different result when the stock recruitment model is not having a large impact in the other parameters. However the second method is much slower due to all the fits needed to have the empirical distribution of the stock recruitment model parameters. Figure 11.4: Stochastic projection of stock for 5 years with fixed fishing mortality and recruitment. 01: projection without uncertainty, stock recruitment model fit within the stock assessment model; 02: projection with uncertainty, stock recruitment model fit within the stock assessment model; 03: projection with uncertainty, stock recruitment model fit after the stock assessment model 11.2 Initial condition assumptions When projecting the stock forward one needs to make a number of assumptions about initial conditions, the starting point from where projections will be made. The method fwdWindow has a set of options that allows the analyst to decide about those assumptions: Table 11.1: Initial conditions Argument Default value Description wt 3 Number of years to average over to get the future mean weights at age mat 3 Number of years to average over to get the future proportion mature at age m 3 Number of years to average over to get the future natural mortality at age spwn 3 Number of years to average over to get the future fraction of mortality before spawning discards.ratio 3 Number of years to average over to get the future mean proportion of discards at age catch.sel 3 Number of years to average over to get the future selection patern (fishing mortality at age which will be scaled based on canges in \\(\\bar{F}\\)) One can also define if those assumptions will be based on the mean value over the time period set, or randomly sampled from historical values, through setting the argument fun to mean or sample, respectively. For the next examples we’ll use the approach of fitting the stock recruitment within the assessment together with other parameters. We’ll set to 20 the number of years to compute mean weights at age, to 10 the number of years to average across and estimate the selection pattern in terms of fishing mortality at age. Finally, we’ll use a 10 year period to compute the average discard ratio. fmod &lt;- ~ s(age, k = 8) + s(year, k = 25) + te(age, year, k = c(6, 15)) fit00 &lt;- sca(ple4, ple4.indices, srmodel=~geomean(), fmodel=fmod) sr00 &lt;- as(fit00, &quot;FLSR&quot;) stk00 &lt;- ple4 + fit00 stk00 &lt;- fwdWindow(stk00, end = endpy, years = list(wt = 20, catch.sel = 10, discards.ratio = 10), fun = list(wt = &quot;sample&quot;)) trg00 &lt;- fwdControl(year = inipy:endpy, quant = &quot;f&quot;, value = 0.3) stk04 &lt;- fwd(stk00, control=trg00, sr=sr00) Figure 11.5: Stochastic projections of stock for 5 years with fixed fishing mortality and recruitment. Two scenarios with different assumptions about initial conditions 11.3 Scenarios There’s a wide range of scenarios that can be of interest to project in order to give advice to policy makers, or to better understand the fitted stock assessment model. For example, projecting the stock in the absence of fishing for a few generations, gives good insights about the dynamics of the population being modelled. fmod &lt;- ~ s(age, k = 8) + s(year, k = 25) + te(age, year, k = c(6, 15)) fit00 &lt;- sca(ple4, ple4.indices, srmodel=~geomean(), fmodel=fmod) sr00 &lt;- as(fit00, &quot;FLSR&quot;) stk00 &lt;- ple4 + simulate(fit00, nsim) # set projection projy &lt;- 25 endpy &lt;- maxy + projy inipy &lt;- maxy + 1 stk00 &lt;- fwdWindow(stk00, end = endpy) trg00 &lt;- fwdControl(year = inipy:endpy, quant = &quot;f&quot;, value = 0) # recruitment uncertainty res00 &lt;- residuals(sr00) rec00 &lt;- window(rec(stk00), inipy, endpy) rec00 &lt;- rlnorm(rec00, mean(res00), sqrt(var(res00))) # project stk05 &lt;- fwd(stk00, control=trg00, sr=sr00, residuals=rec00) Figure 11.6: Stochastic projection of stock for 25 years in the absence of fishing These scenarios are defined by the target quantities one’s trying to achieve. Table 11.2 FLasher there are the following target quantities: Table 11.2: Target quantities and their description Target Description srp ssb_end biomass_end ssb_spawn biomass_spawn ssb_flash biomass_flash inmb_end indb catch landings discards f fbar revenue effort When projecting the stock under the conditions defined by the scenario one can mix several quantities. For example it may be interesting to project an initial situation of growing the stock followed by a higher exploitation to evaluate how catches would behave. trg00 &lt;- fwdControl(year = inipy:endpy, quant = c(rep(&quot;ssb_end&quot;, 15), rep(&quot;f&quot;, 10)), value = c(rep(2000000, 15), rep(0.3, 10))) stk06 &lt;- fwd(stk00, control=trg00, sr=sr00, residuals=rec00) Figure 11.7: Stochastic projection of stock for 25 years with fixed SSB for 15 years followed by fixed fishing mortality for 10 years and constant recruitment 11.4 Relative scenarios Another scenario that is very useful when advising decision makers is to have objectives which are relative to previous preformances. For example one could increase spwanwing stock biomass by 10% each year. This is done buy using the argument relYear and setting value in relative terms, \\(1.1\\). fit00 &lt;- sca(ple4, ple4.indices, srmodel=~geomean()) sr00 &lt;- as(fit00, &quot;FLSR&quot;) stk00 &lt;- ple4 + simulate(fit00, nsim) # set projection projy &lt;- 5 endpy &lt;- maxy + projy inipy &lt;- maxy + 1 stk00 &lt;- fwdWindow(stk00, end = endpy) trg00 &lt;- fwdControl(year = inipy:endpy, quant = &quot;ssb_end&quot;, value = 1.1, relYear = inipy:endpy-1) # recruitment uncertainty res00 &lt;- residuals(sr00) rec00 &lt;- window(rec(stk00), inipy, endpy) rec00 &lt;- rlnorm(rec00, mean(res00), sqrt(var(res00))) # project stk07 &lt;- fwd(stk00, control=trg00, sr=sr00, residuals=rec00) Similar scenarios can be set for all quantities and any years to use as reference. The next example sets a scenario where \\(SSB\\) levels are set in relation to the most recent estimate out of the assessmeent. trg00 &lt;- fwdControl(year = inipy:endpy, quant = &quot;ssb_end&quot;, value = 1.1, relYear = maxy) stk08 &lt;- fwd(stk00, control=trg00, sr=sr00, residuals=rec00) Figure 11.8: Stochastic projection of stock for 25 years with fixed SSB for 15 years followed by fixed fishing mortality for 10 years and constant recruitment. scn01 = 10% SSB growth relative to previous year; scn02 = 10% higher SSB relative to most recent estimate 11.5 Limits An important element when projecting the stock forward is to keep the performance of the fishery within some boundaries. A common one requested by the industry is to keep catches within some stability. fwd can include those constraints using the min and max arguments. The next example sets the minimum future catches to half of mean historical catches. minc &lt;- 0.2*mean(catch(stk00), na.rm=TRUE) trg00 &lt;- fwdControl(year = inipy:endpy, quant = rep(c(&quot;ssb_end&quot;, &quot;catch&quot;), projy), value = rep(c(1500000, NA), projy), min=rep(c(NA, minc), projy)) stk09 &lt;- fwd(stk00, control=trg00, sr=sr00, residuals=rec00) Figure 11.9: Stochastic projection of stock for 25 years with SSB target of 1500000t and catch limit of 50% historical catches 11.6 Harvest Control Rule Harvest Control Rules (HCR) can be complex and of many shapes (REF). We’ll keep our examples simple to demonstrate the mechanism of coding HCR. HCR are decision algorythms that can be used to codify the decision making process, allowing for longer term stability of management decisions in fisheries. The HCR we’re going to explore is based on a target and a limit. The target is applied to the management objective and represents the intent of management. The limit is applied to the process we want to use as trigger for protective actions. For example, the objective of the management system is to extract the highest catches possible for a very long time (aka equilibrium). However, due to natural variability and scientific uncertainty, it can happen that the stock’s biomass decreases below what’s expected, in which case decision makers want to make sure the stock stays healthy and productive. This situation can be translated into a target of fishing mortality at the level that extracts the Maximum Sustainable Yield, and a biomass limit of e.g. half the biomass that would produce the referred catches, everything being in a stable equilibrium. If SSB falls below the limit then fishing mortality is set at 80% of the target. Such HCR could be written as \\(if \\quad SSB_y &gt; 0.5xB_{MSY} \\quad then \\quad F_{y+1} = F_{MSY} \\quad or \\quad else \\quad F_{y+1} = 0.8xF_{MSY}\\) and coded like # fit model fit00 &lt;- sca(ple4, ple4.indices, srmodel=~geomean()) stk00 &lt;- ple4 + fit00 # create stock recruitment model object sr00 &lt;- as(fit00, &quot;FLSR&quot;) # estimate reference points brp00 &lt;- FLBRP(stk00, sr=sr00) brp00 &lt;- brp(brp00) ftrg &lt;- refpts(brp00)[&#39;msy&#39;,&#39;harvest&#39;] blim &lt;- 0.5*refpts(brp00)[&#39;msy&#39;,&#39;biomass&#39;] # set projection # most recent year in the data maxy &lt;- range(ple4)[&quot;maxyear&quot;] # number of years to project projy &lt;- 2 # last year for projections endpy &lt;- maxy + projy # initial year for projections inipy &lt;- maxy + 1 # extend stock object to store projection&#39;s results stk00 &lt;- fwdWindow(stk00, end = endpy) # set the controls for the projections trg00 &lt;- fwdControl(year = inipy:endpy, quant = &quot;f&quot;, value = ftrg) trg01 &lt;- fwdControl(year = inipy:endpy, quant = &quot;f&quot;, value = 0.8*ftrg) # project if(ssb(stk00)[,ac(maxy)] &gt; blim){ stk10 &lt;- fwd(stk00, control=trg00, sr=sr00) } else { stk10 &lt;- fwd(stk00, control=trg01, sr=sr00) } Figure 11.10: Projection of stock for 2 years following a HCR with a target of FMSY and limit of 50% SSBMSY References "],["reference-points.html", "12 Reference Points 12.1 Yield per recruit reference points 12.2 Stock recruitment relationship based reference points 12.3 Economics reference points 12.4 Computing user specific reference points", " 12 Reference Points One of the primary objectives of stock assessment is the estimation of reference points. These serve as benchmarks for evaluating the outputs of assessment models and determining the status of a fish stock. Reference points are critical for effective fisheries management, guiding decisions on sustainable exploitation. The most common classification of stock status is bidimensional, comparing exploitation levels and biomass sizes against target reference points. This framework allows for the assessment of whether a stock is overfished or experiencing overfishing: Exploitation Levels: Typically represented by fishing mortality (F), overfishing occurs when F exceeds the target reference point. Conversely, if F is below the reference, the stock is considered to be sustainably fished. Biomass Size: Commonly measured by spawning stock biomass (SSB), stocks are deemed overfished if the SSB falls below the reference point. These assessments often utilize tools like the Kobe plot, which visually represents stock status in relation to these metrics. In addition to target reference points, limit reference points (LRPs) are commonly included in stock assessments. These represent thresholds that should not be crossed, as they signal a high risk of stock collapse or significant uncertainty in population dynamics. Effective management aims to maintain fishing pressure and biomass levels within safe biological limits, ensuring long-term sustainability and reducing the risk of adverse outcomes. Advancements in stock assessment science continue to refine these reference points. For example, Maximum Sustainable Yield (MSY) and its proxies, such as B\\(_{MSY}\\) (biomass at MSY) and F\\(_{MSY}\\) (fishing mortality at MSY), remain widely used. These reference points are related with the stock’s productivity, which in itself is a complex interaction between recruitment, growth and mortality processes. Recruitment is the process of input to the population, it defines the number of fish that will enter the population and are vulnerable to fishing. It encompasses the process of spawning, which depends on the reproductive potential of the individuals, and the survivability of the laervae up to entering the fishery, which mostly depends on environmental conditions. Individual growth defines the time needed for an individual to gain weight, grow in length and eventualy mature and spawn. Mortality is commonly split between mortality caused by fishing and mortality caused by natural events. Natural mortality merges together all factors by which an individual may die and are not related to fishing, for example predation from other species. These processes, recruitment, individual growth and natural mortality depend on a mix of interactions between environmental conditions and species’ biology. Fishing mortality on the other hand is mostly dependent on the human factor, it’s related with the choice to fish and the way to fish. It’s the outcome of the effort the fleet deploys, the selectivity of the gear used and the availability of individuals. For example, the productivity of the stock will be different if the fleet fishes in an area with lots of young fish using a small mesh size, from a fleet fishing in an area where young fish are not common and using a large mesh size. For this section we’ll be using the package FLBRP [] from the FLR family of packages. library(FLBRP) library(FLa4a) data(ple4) data(ple4.indices) fit0 &lt;- sca(ple4, ple4.indices) stk0 &lt;- ple4 + fit0 To proceed with the computation of reference points we must start by creating an FLBRP object and afterwards run the fitting process with brp(). The FLBRP class has information on selection pattern, mass at age, and biological parameters. The information is stored in the object’s slots which can be accessed with the usual commands, respectively catch.sel(), discards.sel(), stock.wt(), catch.wt(), discards.wt(), m() and mat(). These quantities are computed by averaging the 3 most recent years of the relevant stock object slots. In the case of the selection pattern it is computed by scaling F-at-age to a maximum of 1. By default FLBRP creates a harvest slot with 100 computations of fishing mortality at age scaled from \\(\\bar{F}=0\\) up to \\(F_{crash}\\) or \\(\\bar{F}=4\\), if the former isn’t possible to compute, which is later used to compute the reference points. A number of parameters can be set by the user to create the FLBRP object: Argument Default value Description fbar seq(0, 4, length.out = 101) nyears 3 biol.nyears nyears fbar.nyears nyears sel.nyears fbar.nyears na.rm TRUE mean “arithmetic” 12.1 Yield per recruit reference points In the case where no stock recruitment relationship exists, or was fitted, brp() will return yield per recruit reference points. By default it computes biomasses in the absence of fishing, also know as virgin biomass, \\(F_{MAX}\\), \\(F_{0.1}\\) and 40% Spawning per recruit reference points. brp0 &lt;- FLBRP(stk0) brp0 &lt;- brp(brp0) summary(brp0) ## An object of class &quot;FLBRP&quot; ## ## Name: ## Description: ## Quant: age ## Dims: age year unit season area iter ## 10 101 1 1 1 1 ## ## Range: min max pgroup minfbar maxfbar ## 1 10 10 2 6 ## ## ## Model: rec ~ a ## params ## iter a ## 1 1 ## ## refpts: calculated The selection pattern and other quantities can be depicted by calling plot() on the specific FLBRP object’s slot. To extract a table with all reference points one uses the method refpts(). Note in this case \\(F_{msy}\\) is the same as \\(F_{max}\\), since the assumed stock recruitment is mean recruitment. refpts(brp0) ## An object of class &quot;FLPar&quot; ## quant ## refpt harvest yield rec ssb biomass revenue cost ## virgin 0.00e+00 0.00e+00 1.00e+00 3.42e+00 3.53e+00 NA NA ## msy 2.10e-01 7.08e-02 1.00e+00 9.44e-01 1.03e+00 NA NA ## crash 1.47e+01 6.02e-06 1.00e+00 4.38e-06 2.87e-02 NA NA ## f0.1 1.58e-01 6.85e-02 1.00e+00 1.28e+00 1.38e+00 NA NA ## fmax 2.10e-01 7.08e-02 1.00e+00 9.44e-01 1.03e+00 NA NA ## spr.30 1.96e-01 7.06e-02 1.00e+00 1.03e+00 1.12e+00 NA NA ## mey NA NA NA NA NA NA NA ## quant ## refpt profit ## virgin NA ## msy NA ## crash NA ## f0.1 NA ## fmax NA ## spr.30 NA ## mey NA ## units: NA refpts(brp0)[c(&#39;msy&#39;, (&#39;fmax&#39;)), ] ## An object of class &quot;FLPar&quot; ## quant ## refpt harvest yield rec ssb biomass revenue cost profit ## msy 0.2099 0.0708 1.0000 0.9436 1.0349 NA NA NA ## fmax 0.2099 0.0708 1.0000 0.9436 1.0349 NA NA NA ## units: NA The depiction of the reference points with the method plot() shows recruitment as constant over all levels of biomass and set to \\(1\\). 12.2 Stock recruitment relationship based reference points An important way to improve reference points is to include stock recruitment dynamics. Yield per recruit, as in previous section, ignores this dynamics and assumes recruitment will be the same no matter SSB’s size, which is obviously wrong although in many cases due to unknown or very uncertain dynamics it’s the best one can do. The stock recruitment model must be fitted before computing reference points and the FLSR object has to be passed to the FLBRP call to create the object that brp() method will use. There’s two ways of fitting stock recruitment models: (i) after fitting the stock assessment model by using its outputs, SSB and recruitment, as data to fit the model; (ii) inside the stock assessment model together with all other quantities. There’s pros and cons on both approaches, we’re not going to dwell on those now though. 12.2.1 Stock recruitment after fitting the stock assessment model In the following example we’ll use a Beverton and Holt stock recruitment reltionship. There are several other relationships that can be used, see Table XX (ver ?bevholt()) Model Formula Function Description Beverton and Holt rec ~ a * ssb/(b + ssb) bevholt() [REF] Ricker [REF] rec ~ a * ssb * exp(-b * ssb) ricker() Segmented regression [REF] rec ~ ifelse(ssb &lt;= b, a * ssb, a * b) segreg Cushing [REF] rec ~ a * ssb^b cushing Shepherd [REF] shepherd Geomean [REF] geomean sr0 &lt;- as.FLSR(stk0, model=bevholt) sr0 &lt;- fmle(sr0, control = list(trace = 0)) plot(sr0) We now need to provide the FLSR object, sr0, to the FLBRP call and refit the reference points. brp0 &lt;- FLBRP(stk0, sr=sr0) model(brp0) ## rec ~ a * ssb/(b + ssb) ## &lt;environment: 0x5e2bd86f16a8&gt; params(brp0) ## An object of class &quot;FLPar&quot; ## params ## a b ## 1038832 9829 ## units: NA brp0 &lt;- brp(brp0) The new reference points can now be extracted using the refpts method with the FLBRP object as the main argument, and depict the relationships with plot(). Note this time by setting the flag obs to TRUE the plot will include the estimates of \\(SSB\\) and \\(R\\). refpts(brp0) ## An object of class &quot;FLPar&quot; ## quant ## refpt harvest yield rec ssb biomass revenue cost ## virgin 0.00e+00 0.00e+00 1.04e+06 3.54e+06 3.65e+06 NA NA ## msy 2.07e-01 7.28e+04 1.03e+06 9.87e+05 1.08e+06 NA NA ## crash 2.25e+00 1.11e-06 3.87e-04 3.66e-06 1.83e-05 NA NA ## f0.1 1.58e-01 7.06e+04 1.03e+06 1.32e+06 1.42e+06 NA NA ## fmax 2.10e-01 7.28e+04 1.03e+06 9.70e+05 1.06e+06 NA NA ## spr.30 1.96e-01 7.27e+04 1.03e+06 1.06e+06 1.15e+06 NA NA ## mey NA NA NA NA NA NA NA ## quant ## refpt profit ## virgin NA ## msy NA ## crash NA ## f0.1 NA ## fmax NA ## spr.30 NA ## mey NA ## units: NA Note \\(MSY\\) based reference points are no longer the same as \\(F_{MAX}\\), and recruitment is no longer constant over all \\(SSB\\) levels. 12.2.2 Stock recruitment during fitting the stock assessment model fit1 &lt;- sca(ple4, ple4.indices, srmodel = ~ bevholt(CV = 0.5)) a4aflsr &lt;- as(stkmodel(fit1), &quot;FLSR&quot;) # or a4aflsr &lt;- as(fit1, &quot;FLSR&quot;) plot(a4aflsr, obs = TRUE) a4abrp &lt;- FLBRP(stk0, a4aflsr) 12.3 Economics reference points We can add economic data to the FLBRP object to calculate economic based reference points, like maximum economic yield (MEY). We need to provide information about price, variable costs and fixed costs. The first in value at age per weight of fish, the others in value per unit of fishing mortality. # price price(brp0) &lt;- c(rep(1,3),rep(1.5,2),rep(2,5)) price(brp0)@units &lt;- &quot;1000 euro per ton&quot; # variable costs per F vcost(brp0) &lt;- 100000 vcost(brp0)@units &lt;- &quot;1000 euro per F&quot; # fixed costs per F fcost(brp0) &lt;- 50000 fcost(brp0)@units &lt;- &quot;1000 euro per F&quot; # reference points brp0 &lt;- brp(brp0) refpts(brp0) ## An object of class &quot;FLPar&quot; ## quant ## refpt harvest yield rec ssb biomass revenue cost ## virgin 0.00e+00 0.00e+00 1.04e+06 3.54e+06 3.65e+06 0.00e+00 5.00e+04 ## msy 2.07e-01 7.28e+04 1.03e+06 9.87e+05 1.08e+06 1.21e+05 7.07e+04 ## crash 2.25e+00 1.11e-06 3.87e-04 3.66e-06 1.83e-05 1.15e-06 2.75e+05 ## f0.1 1.58e-01 7.06e+04 1.03e+06 1.32e+06 1.42e+06 1.20e+05 6.58e+04 ## fmax 2.10e-01 7.28e+04 1.03e+06 9.70e+05 1.06e+06 1.21e+05 7.10e+04 ## spr.30 1.96e-01 7.27e+04 1.03e+06 1.06e+06 1.15e+06 1.22e+05 6.96e+04 ## mey 2.19e-01 7.27e+04 1.03e+06 9.22e+05 1.02e+06 1.21e+05 7.19e+04 ## quant ## refpt profit ## virgin -5.00e+04 ## msy 5.07e+04 ## crash -2.75e+05 ## f0.1 5.38e+04 ## fmax 5.03e+04 ## spr.30 5.21e+04 ## mey 4.89e+04 ## units: NA The reference points table is now complete with values for revenue, costs and profit, as well as estimtes for \\(MEY\\) based reference points. The point where profits are maximized, instead of the point where catch is maximized as in the case of MSY. plot(brp0) 12.4 Computing user specific reference points There is an option to calculate user defined reference points given a target F: custom_refs &lt;- FLPar(Ftrgt1 = 0.33, Ftrgt2 = 0.44) brp1 &lt;- brp0 + custom_refs refpts(brp1) ## An object of class &quot;FLPar&quot; ## quant ## refpt harvest yield rec ssb biomass revenue cost ## virgin 0.00e+00 0.00e+00 1.04e+06 3.54e+06 3.65e+06 0.00e+00 5.00e+04 ## msy 2.07e-01 7.28e+04 1.03e+06 9.87e+05 1.08e+06 1.21e+05 7.07e+04 ## crash 2.25e+00 1.11e-06 3.87e-04 3.66e-06 1.83e-05 1.15e-06 2.75e+05 ## f0.1 1.58e-01 7.06e+04 1.03e+06 1.32e+06 1.42e+06 1.20e+05 6.58e+04 ## fmax 2.10e-01 7.28e+04 1.03e+06 9.70e+05 1.06e+06 1.21e+05 7.10e+04 ## spr.30 1.96e-01 7.27e+04 1.03e+06 1.06e+06 1.15e+06 1.22e+05 6.96e+04 ## mey 2.19e-01 7.27e+04 1.03e+06 9.22e+05 1.02e+06 1.21e+05 7.19e+04 ## Ftrgt1 3.30e-01 6.53e+04 1.02e+06 4.95e+05 5.80e+05 1.05e+05 8.30e+04 ## Ftrgt2 4.40e-01 5.37e+04 1.00e+06 2.81e+05 3.58e+05 8.27e+04 9.40e+04 ## quant ## refpt profit ## virgin -5.00e+04 ## msy 5.07e+04 ## crash -2.75e+05 ## f0.1 5.38e+04 ## fmax 5.03e+04 ## spr.30 5.21e+04 ## mey 4.89e+04 ## Ftrgt1 2.16e+04 ## Ftrgt2 -1.13e+04 ## units: NA Or create an empty FLPar with specified reference points and recalculate everything: #brp2 &lt;- FLPar(NA,dimnames=list(refpt=c(&quot;virgin&quot;,&quot;f0.1&quot;,&quot;fmax&quot;,&quot;spr.30&quot;,&quot;spr.35&quot;,&quot;spr.45&quot;), quantity=c(&quot;harvest&quot;,&quot;yield&quot;,&quot;rec&quot;,&quot;ssb&quot;,&quot;biomass&quot;,&quot;revenue&quot;,&quot;cost&quot;,&quot;profit&quot;), iter=1)) brp2 &lt;- brp1 brp2@refpts &lt;- FLPar(NA, dimnames=list(refpt = c(&quot;virgin&quot;, &quot;f0.1&quot;, &quot;fmax&quot;, &quot;spr.30&quot;, &quot;spr.35&quot;,&quot;spr.45&quot;), quantity=c(&quot;harvest&quot;, &quot;yield&quot;, &quot;rec&quot;, &quot;ssb&quot;, &quot;biomass&quot;, &quot;revenue&quot;, &quot;cost&quot;, &quot;profit&quot;), iter=1)) brp2 &lt;- brp(brp2) refpts(brp2) ## An object of class &quot;FLPar&quot; ## quantity ## refpt harvest yield rec ssb biomass revenue cost ## virgin 0.00e+00 0.00e+00 1.04e+06 3.54e+06 3.65e+06 0.00e+00 5.00e+04 ## f0.1 1.58e-01 7.06e+04 1.03e+06 1.32e+06 1.42e+06 1.20e+05 6.58e+04 ## fmax 2.10e-01 7.28e+04 1.03e+06 9.70e+05 1.06e+06 1.21e+05 7.10e+04 ## spr.30 1.96e-01 7.27e+04 1.03e+06 1.06e+06 1.15e+06 1.22e+05 6.96e+04 ## spr.35 1.69e-01 7.16e+04 1.03e+06 1.23e+06 1.33e+06 1.21e+05 6.69e+04 ## spr.45 1.27e-01 6.64e+04 1.03e+06 1.59e+06 1.69e+06 1.13e+05 6.27e+04 ## quantity ## refpt profit ## virgin -5.00e+04 ## f0.1 5.38e+04 ## fmax 5.03e+04 ## spr.30 5.21e+04 ## spr.35 5.38e+04 ## spr.45 5.07e+04 ## units: NA One specific case is to compute \\(F_{MSY}\\) ranges according to Hilborn (2010) and Rindorf et al. (2016) ideas. For this case there’s already the method msyRanges, which takes as argument a fitted FLBRP object and delivers a FLPar object, similar to refpts. rp.rngs &lt;- msyRange(brp0, range=0.05) rp.rngs ## An object of class &quot;FLPar&quot; ## quantity ## refpt harvest yield rec ssb biomass revenue cost profit ## msy 2.07e-01 7.28e+04 1.03e+06 9.87e+05 1.08e+06 1.21e+05 7.07e+04 5.07e+04 ## min 1.45e-01 6.92e+04 1.03e+06 1.43e+06 1.53e+06 1.18e+05 6.45e+04 5.31e+04 ## max 2.87e-01 6.92e+04 1.02e+06 6.26e+05 7.14e+05 1.12e+05 7.87e+04 3.36e+04 ## units: NA Another simple way, although it onl;y works for \\(SPR\\) based reference points, is to include other spr.## points in the refpts table. References "],["annex---stock-assessment-workflow.html", "13 Annex - stock assessment workflow 13.1 The “mean” model 13.2 The age effects 13.3 Year effect on fishing mortality 13.4 Year effect on catchability 13.5 The initial year population abundance model, aka N1 13.6 The stock recruitment submodel 13.7 The variance submodel 13.8 Final comments", " 13 Annex - stock assessment workflow The following sections describes a potential workflow for fitting a a4a stock assessment model. The idea is to explore the age and year effects in isolation and adjust the model’s smoothness to model those effects. The procedure is heavily supported by residuals’ analysis. In a well specified model residuals should show a random pattern, without any trend or very high values (outliers). 13.1 The “mean” model To start the analysis we’ll fit a “mean” model, where all submodels will be set to an overall average, by using the \\(\\sim 1\\) formula. This will be our reference model to see how adding age and year effects will show up in the diagnostic tools, in particular in the residuals. library(FLa4a) data(hke1567) data(hke1567.idx) fit01 &lt;- sca(hke1567, hke1567.idx, fmod=~1, qmod=list(~1), srmod=~1, vmod=list(~1, ~1), n1mod=~1) res01 &lt;- residuals(fit01, hke1567, hke1567.idx) The common residuals plot clearly shows a time trend for each age (Figure 13.2) for both datasets. Furthermore, inspecting how catch ate age residuals are positioned across ages, by comparing the level of residuals for each age, one can see the pattern of lower than 0 residuals in age 0, reversing the signal for ages 1 and 2, close to 0 in ages 3 and 4, and again below 0 in age 5. plot(res01) Figure 13.1: Mean fit residuals by year) This pattern becomes more apparent when plotting the residuals by age across years. plot(res01, auxline=&quot;l&quot;, by=&quot;age&quot;) Figure 13.2: Mean fit residuals by age 13.2 The age effects The following models will introduce age effects in the fishing mortality submodel and catchability submodel. In the fishing mortality submodel we’ll introduce a factor which means that there will be as many parameters as ages minus 1 and each parameters will be independent of each other. fit02 &lt;- sca(hke1567, hke1567.idx, fmod=~factor(age), qmod=list(~1), srmod=~1, vmod=list(~1, ~1), n1mod=~1) res02 &lt;- residuals(fit02, hke1567, hke1567.idx) The residuals plot now shows catch at age residuals less stagered, reflecting the modelling of the age effect. plot(res02) Figure 13.3: Fishing mortality model with age effect residuals by year The residuals plot by age shows the same outcome. plot(res02, auxline=&quot;l&quot;, by=&quot;age&quot;) Figure 13.4: Fishing mortality model with age effect residuals by age We’ll now proceed adding an age effect to the catchability model while removing the catch at age effect. fit03 &lt;- sca(hke1567, hke1567.idx, fmod=~1, qmod=list(~factor(age)), srmod=~1, vmod=list(~1, ~1), n1mod=~1) res03 &lt;- residuals(fit03, hke1567, hke1567.idx) plot(res03) Figure 13.5: Index catchability model with age effect residuals by year The residuals plot by age shows the same outcome. plot(res03, auxline=&quot;l&quot;, by=&quot;age&quot;) Figure 13.6: Index catchability model with age effect residuals by age Finally both effects are brought together. fit04 &lt;- sca(hke1567, hke1567.idx, fmod=~factor(age), qmod=list(~factor(age)), srmod=~1, vmod=list(~1, ~1), n1mod=~1) res04 &lt;- residuals(fit04, hke1567, hke1567.idx) plot(res04) Figure 13.7: Fishing mortality and index catchability models with age effect residuals by year The residuals plot by age shows the same outcome. plot(res04, auxline=&quot;l&quot;, by=&quot;age&quot;) Figure 13.8: Fishing mortality and index catchability models with age effect residuals by age 13.3 Year effect on fishing mortality This model will introduce an year effect in the fishing mortality submodel on top of the F age effect added before. Inspecting the last set of residuals (Figure 13.8) one can easily see the pattern across years with more positive residuals in the beggining of the time series and more negative in the most recent years. As for age we’re using a factor for years. The new model’s residuals won’t show such a pronounced effect anymore (Figures 13.9 and 13.10). fit05 &lt;- sca(hke1567, hke1567.idx, fmod=~factor(age) + factor(year), qmod=list(~1), srmod=~1, vmod=list(~1, ~1), n1mod=~1) res05 &lt;- residuals(fit05, hke1567, hke1567.idx) plot(res05) Figure 13.9: Fishing mortality model with year effect residuals by year The residuals plot by age shows the same outcome. plot(res05, auxline=&quot;l&quot;, by=&quot;age&quot;) Figure 13.10: Fishing mortality model with year effect residuals by age We can see now that the residuals show a lot less patterns than before. There’s still some issues, the survey catchability seems to have an year trend. However the model is not fully specified yet, stock recruitment is modelled as constant over time, the initial population abundance is also modelled as a constant as well as the variance models. 13.4 Year effect on catchability It’s uncommon to include year trends on the abundance index catchability model. Such decision needs to be considered carefully as the trend in the index, in the case of a well design scientific survey, should result from a change in abundance. Modelling that trend would attribute such change to the survey design and remove it from the abundance. If the survey index is based on a commercial CPUE it becomes more likely that changes in selectivity or fishing behaviour could show up in the index as changes in abundance. Although the common process of standardizing CPUEs should deal with technical issues. In the case of adding year effects to teh catchability submode the same formulas can be used, to include period breaks, trends, etc. 13.5 The initial year population abundance model, aka N1 This model sets the n-at-age in the first year of the time series, which is needed due to the lack of previous data to reconstruct those cohorts. It will affect the population numbers in the lower triangle of the initial population matrix and catches. The following model will introduce an age effect in the population abundance in the first year of the time series. fit06 &lt;- sca(hke1567, hke1567.idx, fmod=~factor(age) + factor(year), qmod=list(~factor(age)), srmod=~1, vmod=list(~1, ~1), n1mod=~factor(age)) res06 &lt;- residuals(fit06, hke1567, hke1567.idx) The best way to inspect the effect of this model is to zoom into the initial years of the time series. Figure 13.11 zooms into the previous model, which used an intercept only model for N1, while Figure 13.12. Figure 13.11: N1 fitted as an intercept only model: 2007 - 2010 residuals by age Figure 13.12: N1 fitted with an age effect model: 2007 - 2010 residuals by age Comparing the two plots it can be seen the effect of modeling abundance in the initial year. Residuals for both catch at age and catchability improved considerably. The following years also improve to different levels. 13.6 The stock recruitment submodel In this example we’ll simply add a model to allow recruitment to vary over time and we’ll see how to track potential improvements in the residuals. fit07 &lt;- sca(hke1567, hke1567.idx, fmod=~factor(age) + factor(year), qmod=list(~factor(age)), srmod=~factor(year), vmod=list(~1, ~1), n1mod=~factor(age)) res07 &lt;- residuals(fit07, hke1567, hke1567.idx) The residuals plot by year are very useful to see the effect of adding a varying stock recruitment model. The year trends present in previous models are not absent. Recruitment variability when left unmodelled was being picked up by trends in the survey catchability and catch at age. And due to the cohort dynamics underlying the catch at age model, where propagating into other ages’ estimates. plot(res07) Figure 13.13: Stock-recruitment model with year effect residuals by year 13.7 The variance submodel Finally, we’re testing the variance submodel, specifically the catch at age variance model. We won’t dig into the catchability variance model though. It’s common to accept that a scientific survey following a well designed sampling protocol will have equal variance across ages since no preferential areas should be sampled sampled. The variance model will use a smoother with k=3. The expectation is that the variance model will have a U-shape, since yourger and older ages are usually less caught and as such estomates of those ages will have larger variances than fully exploited ages. fit08 &lt;- sca(hke1567, hke1567.idx, fmod=~factor(age) + s(year, k=10), qmod=list(~factor(age)), srmod=~s(year, k=10), vmod=list(~s(age, k=3), ~1), n1mod=~factor(age)) We’ll use the pearson residuals for this analysis since those are standardized by the predicted variances of the model and not the residual variance itself, like the more common standardized residuals. Figure 13.15 shows an improve set of residuals when compared to Figure 13.14 which add an intercept only model for the variance model. res07 &lt;- residuals(fit07, hke1567, hke1567.idx, type=&quot;pearson&quot;) res08 &lt;- residuals(fit08, hke1567, hke1567.idx, type=&quot;pearson&quot;) plot(res07) Figure 13.14: Variance model with intercept only age effect pearson residuals plot(res08) Figure 13.15: Variance model with age effect pearson residuals To see what’s happening with the variance model one can use predict to plot the different models fitted. Figure 13.16: Variance models for catch at age To see the effect these models have on the estimated quantities one can look at the variance of the estimates: Figure 13.17: Estimates of population abundance with different variance models 13.8 Final comments The sequence presented here can be changed and applied in any order the user is interested or prefers to. The approach of not allowing year effects in surveys and variance model can be modified if the user prefers to do so. The (ab)use of factor is for demonstration purposes only. The user is incentivised to explore other model forms, in particular smoothers. "],["references.html", "14 References", " 14 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
