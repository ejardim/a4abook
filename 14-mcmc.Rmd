# The statistical catch-at-age stock assessment framework with Markov Chain Monte Carlo (MCMC) \label{sec:mcmc}

The previous methods were demonstrated using maximum likelihood estimation (MLE). However, ADMB also supports Markov Chain Monte Carlo (MCMC) methods, which provide significant advantages, particularly when working with complex models that involve many parameters. The key difference is that while MLE finds a single best estimate of parameters by maximizing the likelihood function, MCMC offers a broader perspective by generating an entire distribution of possible values. This approach is more informative because it does not just give the most likely estimate but also helps us understand the uncertainty surrounding it. With MCMC, researchers can incorporate prior knowledge and obtain results that are often more realistic and reliable (Gelman et al., 2013). This is especially useful when dealing with complicated models where traditional likelihood-based methods struggle, as MCMC allows for efficient exploration of possible solutions without requiring an exact mathematical formulation (Gilks, Richardson, & Spiegelhalter, 1996; Robert & Casella, 2004).

One of the biggest advantages of MCMC is its flexibility when working with models that have irregular behavior, such as those with multiple peaks or abrupt changes in likelihood. Standard MLE methods assume that the likelihood function behaves smoothly, like a well-shaped bowl, but this is rarely true in real-world applications. In fisheries, ecology, and other applied sciences, models often have parameters that interact in complex ways, creating likelihood surfaces with ridges and multiple solutions. In these cases, MLE can easily get stuck in a local peak, failing to find the best possible estimate or underestimating the real uncertainty in the system (Neal, 1993). Since MCMC uses a probabilistic sampling approach instead of strict optimization, it moves freely across the entire space of possible values, making it more robust and adaptable to challenging problems (Robert & Casella, 2013).

Traditional MLE-based uncertainty estimation relies on the Hessian matrix, which essentially measures how quickly the likelihood function changes as parameters vary. This method assumes that the shape of the likelihood function is roughly the same everywhere—meaning that a quadratic (bowl-like) approximation is valid (Pawitan, 2001). However, this assumption is often unrealistic, especially in models with many parameters or correlations between them, as is common in fisheries stock assessment models. Furthermore, MLE uncertainty estimates require a large sample size for them to be accurate, which is not always available in real-world applications (Wasserman, 2004; van der Vaart, 1998). If these assumptions do not hold, MLE can give misleading confidence intervals, making decision-making riskier. Additionally, MLE assumes that the model is correctly specified—meaning that it accurately represents the real system being studied. If the model is misspecified or overly simplified, the Hessian-based uncertainty estimates may be highly unreliable, requiring alternative approaches like robust standard errors or resampling methods (White, 1982).

In fields like fisheries science, where models often involve multiple correlated parameters, MCMC provides a much more flexible and realistic way to estimate uncertainty. Unlike MLE, which assumes uncertainty follows a simple symmetrical pattern, MCMC can handle more complex distributions, giving a better representation of real-world variability. This is especially important when estimating key fisheries management indicators, such as spawning stock biomass ($SSB$) or fishing mortality ($F$), which influence critical policy decisions. Because MCMC does not impose strict mathematical assumptions about the shape of uncertainty, it produces estimates that are more reflective of real-world conditions, ultimately leading to more informed and reliable management strategies.

ADMB’s approach to Markov Chain Monte Carlo (MCMC) enhances Bayesian analysis by efficiently exploring parameter uncertainty in complex models. Unlike standard MCMC tools, ADMB leverages automatic differentiation to improve sampling efficiency and speed (Fournier et al., 2012). It supports various sampling algorithms, including Metropolis-Hastings and Hamiltonian Monte Carlo, which help navigate high-dimensional parameter spaces and complex likelihood structures more effectively. This makes ADMB particularly useful in applied sciences like fisheries and ecology, where uncertainty estimation is crucial for decision-making. Additionally, ADMB provides built-in diagnostics to assess MCMC convergence and reliability, ensuring that posterior distributions are well-explored and results are robust (Gelman et al., 2013).

To evaluate the quality of MCMC sampling, ADMB offers several key diagnostics. Autocorrelation analysis detects dependencies between successive samples, while the effective sample size (ESS) measures the number of independent samples in the chain. The Gelman-Rubin diagnostic (\(\hat{R}\)) helps assess whether multiple chains have converged to the same distribution, with values close to 1 indicating good convergence. Trace plots visually inspect parameter behavior over iterations, revealing trends or poor mixing. Additionally, ADMB monitors the acceptance rate to ensure efficient sampling and provides posterior density estimates to check if the distribution has been properly explored. These tools help users refine their MCMC runs, adjusting sampling length or proposal distributions to improve performance and ensure reliable uncertainty estimates.

The manual "A Guide for Bayesian Analysis in AD Model Builder" by Cole C. Monnahan, Melissa L. Muradian and Peter T. Kuriyam describe and explain a larger group of arguments that can be set when running MCMC with ADMB, which the `a4a` uses.

### References:
- Brooks, S., Gelman, A., Jones, G., & Meng, X.-L. (2011). *Handbook of Markov Chain Monte Carlo*. CRC Press.
- Gamerman, D., & Lopes, H. F. (2006). *Markov Chain Monte Carlo: Stochastic Simulation for Bayesian Inference*. CRC Press.
- Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2014). *Bayesian Data Analysis* (3rd ed.). CRC Press.
- Neal, R. M. (1993). *Probabilistic inference using Markov Chain Monte Carlo methods*. Department of Computer Science, University of Toronto.
- Robert, C., & Casella, G. (2013). *Monte Carlo Statistical Methods* (2nd ed.). Springer.
- Casella, G., & Berger, R. L. (2002). *Statistical Inference* (2nd ed.). Duxbury.
- Lehmann, E. L., & Casella, G. (1998). *Theory of Point Estimation* (2nd ed.). Springer.
- Pawitan, Y. (2001). *In All Likelihood: Statistical Modelling and Inference Using Likelihood*. Oxford University Press.
- van der Vaart, A. W. (1998). *Asymptotic Statistics*. Cambridge University Press.
- Wasserman, L. (2004). *All of Statistics: A Concise Course in Statistical Inference*. Springer.
- White, H. (1982). "Maximum Likelihood Estimation of Misspecified Models." *Econometrica*, 50(1), 1-25.
- Fournier, D. A., Skaug, H. J., Ancheta, J., Ianelli, J., Magnusson, A., Maunder, M. N., Nielsen, A., & Sibert, J. (2012). AD Model Builder: Using automatic differentiation for statistical inference of highly parameterized complex nonlinear models. *Optimization Methods and Software, 27*(2), 233–249.
- Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). *Bayesian Data Analysis* (3rd ed.). CRC Press.

## The MCMC method for `sca`

This section shows how the `sca` methods interface with `ADMB` to use the MCMC fits. For this section we'll use the hake assessment in mediterranean areas (gsa) 1, 5, 6 and 7.

We'll start buy fitting the `MLE` model and afterwards call the `MCMC` methods. The outcomes of the `MCMC` fit need to be inspected to make sure the chain converged and the results are robust. A set of diagnostics are available to do this work.

[TO CHECK]
For many Bayesian software platforms, the MCMC algorithms are started at user-specified or arbitrary places. ADMB has the advantage that it can robustly estimate the posterior mode and the covariance at that point. This information is very valuable in initializing the MCMC chain. Specifically, an MCMC chain starts from the posterior mode and uses the estimated covariance matrix in its proposed jumps (see the algorithm sections below). As such, ADMB chains typically do not need a long period to reach areas of high density. However, we caution the user to always check the MCMC output as other issues may lead to a chain that needs a longer burn-in. (REF TO Monnahan)

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
opts_chunk$set(dev='png', dev.args=list(type="cairo"))
```

```{r}
# load libraries and data
library(FLa4a)
library(ggplotFL)
data(hke1567)
data(hke1567.idx)
nsim <- 250
# MLE estimate
fmod <- ~s(age, k = 4) +
    s(year, k = 8) +
    s(year, k = 8, by = as.numeric(age == 0)) +
    s(year, k = 8, by = as.numeric(age == 4))
qmod <- list(~I(1/(1 + exp(-age))))
fit <- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod)
fit <- simulate(fit, nsim)
```

To run the MCMC method, one needs to configure a set of arguments, which is done by creating a `SCAMCMC` object. Table \@ref(tab:mcargs) describes the arguments available to run the MCMC method, extracted from Monnahan [ref]. For more details on the MCMC configuration in `ADMB` visit the `ADMB` website.

|Argument|Description|
|:---|:----------|
|mcmc N |Run N MCMC iterations|
|mcsave N| Save every N th MCMC iteration|
|mcscale N| Rescale step size for first N iterations|
|mcmult N| Rescale the covariance matrix|
|mcrb N| Reduce high parameter correlations|
|mcprobe X| Use a fat-tailed proposal distribution|
|mcdiag| Use a diagonal covariance matrix|
|mcnoscale| Do not scale the algorithm during|
|mcu| Use a uniform distribution as proposal distribution|
|hybrid| Use the hybrid method|
|hynstep N| Mean number of steps for the leapfrog method|
|hyeps X| The stepsize for the leapfrog method [X numeric and > 0]|

Table: (\#tab:mcargs) `ADMB` MCMC arguments

```{r}
# mcmc
mc <- SCAMCMC()
# check the default pars
mc
```

Defaults for now are ok, so lets fit the model. Note that the argument `fit` must be set to `MCMC` and the argument `mcmc` takes the `SCAMCMC` object.

```{r}
# fit the model
fitmc00 <- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = "MCMC", mcmc=mc)
# check acceptance rate
fitSumm(fitmc00)
```

As usual `fitSumm` store relevant information about the model fit. In the case of MCMC fit the information stored is the number of model paramters (`nopar`), the number of observations (`nobs`) and the acceptance rate (`accrate`).

```{r}
plot(hke1567 + fitmc00)
```

## Diagnostics with CODA

In essence, the diagnostics are used to give the analyst confidence that the posterior distributions of the parameters is unbiased, with simetric non correlated distributions of each parameter, over which one can make inference.

There's a large body of literature about MCMC convergence. In this section we'll focus on convergence, stationarity and mixing, which are essential for ensuring that the chain provides reliable samples from the target posterior distribution.

**[, are: (i) stationarity, the chain should converge to its stationary distribution; (ii) mixing, the chain should mix well, meaning it should explore the sample space efficiently without being overly correlated; and (iii) convergence, the chain should converge to the posterior distribution. Mixing in MCMC refers to how well the chain explores the parameter space. Poor mixing means the chain is getting stuck in certain regions, leading to high autocorrelation and slow convergence.]**

We use the package `CODA` to run the diagnostics on MCMC fits. One needs to convert the `sca` output into a `mcmc` CODA object over which several diagostics can be ran. The mcmc object is a matrix with the parameters (row = iters, cols= pars).

```{r}
library(coda)
```

The out-of-the-box methods available to the stock assessment scientist are trace plots, autocorrelation analysis, geweke diagnostic, acceptance rate, distribution density and cumulative means.

A trace plot shows the sampled values of a parameter over iterations. A plot that looks like a random, stable "cloud" of points with no trends or drifts, with rapid fluctuactions, is a signal of convergence, meaning the chain mixes well and is stationary. If the trace plot shows a strong trend or periodicity, drifts, or long autocorrelated stretches, it means the chain hasn't converged.

For demonstration purposes we'll create a chain with 1000 samples (`mcmc=1000`) and save every iter (`mcsave=1`), which will create a highly correlated and unstable chain, and update the initial MCMC fit to also have 1000 samples (`mcmc=10000`, `mcsave=100`).

```{r}
# update initial fit
mc <- SCAMCMC(mcmc=10000, mcsave=100)
fitmc01 <- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = "MCMC", mcmc=mc)
fitmc01.mc <- FLa4a::as.mcmc(fitmc01)
# highly correlated fit
mc <- SCAMCMC(mcmc=1000, mcsave=1)
fitmc02 <- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = "MCMC", mcmc=mc)
fitmc02.mc <- FLa4a::as.mcmc(fitmc02)
```

Figures \@ref(fig:chain01) and \@ref(fig:chain02) cleary depicts this difference.

Ploting the chain for the parameter clearly shows autocorrelation for the first parameter but also the initial phase where the chain seems to be stuck in a single value (Figure \@ref(fig:chain01)).

```{r, chain01, "MCMC chain trace with high autocorrelation and burnin period (left panel). Parameter density showing a double mode distribution (right panel).", fig.show="hold", out.width="50%"}
plot(fitmc01.mc[,1])
plot(fitmc02.mc[,1])
```


The initial phase is called the "burn-in" phase, where there's no information about the parameter. These iterations can be dropped with the `burnin` method (Figure \@ref(fig:chain01b)), although it doesn't sort out the autocorrelation or the parameter density.

```{r, chain01b, "MCMC chain with high autocorrelation after removing the initial 250 samples (burnin period)."}
plot(FLa4a::as.mcmc(burnin(fitmc02, 250))[,1])
```

Autocorrelation analysis is useful to assess stationarity, a stationary chain should have low autocorrelation, meaning that each sample is approximately independent. On the opposite, high autocorrelation indicates slow mixing and possible non-stationarity. The autocorrelation plot produced by the `acf` function, will show correlation along the chain for each parameter at a large set of lags. Figure \@ref(fig:acf01) shows there's a strong autocorrelation for the first parameter, which we'd like to avoid.

```{r, acf01, "Autocorrelation plot of the first parameter in the MCMC chain"}
acf(fitmc02.mc[,1])
```

A common way to deal with autocorrelation is to increase the thining of the chain, the number of samples that are kept in each run. This decision requires the chain to run for longer since a number of samples will be discarded. In `sca` this is managed by the argument `save`. To keep 1000 samples, as before, with a thining of 200, which means we'll keep one sample every 200, one needs to draw $200x1000=200000$ draws.

```{r}
mc <- SCAMCMC(mcmc=200000, mcsave=200)
fitmc03 <- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = "MCMC", mcmc=mc)
fitmc03.mc <- FLa4a::as.mcmc(fitmc03)
```

```{r, chain02, "MCMC chain trace with a thining of 200 samples. No autocorrelation and burnin period apears to exist (left panel). Parameter density shows a well behaved simetric distribution (right panel)."}
plot(fitmc02.mc[,1])
```

Figure \@ref(fig:acf02) shows the new chain autocorrelation, which is mostly gone.

```{r, acf02, "Autocorrelation plot of the first parameter in the MCMC chain, with a thining of 200."}
acf(fitmc03.mc[,1])
```

The geweke diagnostic computes the Geweke-Brooks Z-score, which indicates if the first and following parts of a sample from a Markov chain are drawn from the same distribution. It's useful to decide if the first few iterations should be discarded and provide information about the stability of the chain. Figure \@ref(fig:gew01) shows the geweke plot for the MCMC run without thining and Figure \@ref(fig:gew01) when the thining was set at 200 samples.

```{r, gew01, "Geweke plot of the first parameter in the MCMC chain"}
geweke.plot(fitmc02.mc[,1])
```

The first plot slearly shows the z-score statistic out of the confdemce intervals until 400 samples are discarded, which points to the need to drop a set of initial samples. The second plot shows a much more regular chain, where the different blocks of data point to similar distributions, which points to a stationary chain and no need of burnin. With a thining of 200 it means the initial 199 samples were dropped, it's no surprise that the burnin of 250 samples is not necessary anymore.

```{r, gew02, "Geweke plot of the first parameter in the MCMC chain, with a thining of 200."}
geweke.plot(fitmc03.mc[,1])
```

### Mixing


Autocorrelation analysis is also useful for this diagnostics. In a good mixed chain autocorrelation drops quickly to near zero, while a poor mixing will display high autocorrelation, meaning successive samples are too correlated, reducing efficiency. Figures \@ref(fig:acf01) and \@ref(fig:acf02) cleary depicts this difference.

The geweke diagnostic is also a good way to look at mixing by comparing the mean and variance of the first part of the chain to the last part. Good mixing will show no significant difference between early and late samples.
Poor mixing will show large differences, indicating the chain has not explored the posterior fully.

Inspecting the cumulative mean along the chain is another good way to check for the stability of the chain. When the mixing is good the mean stabilizes quickly, and vice-versa if not.

```{r, cmean01, "Cumulative mean along the MCMC chain."}
obj <- fitmc02.mc[,1]
obj <- cumsum(obj) / seq_along(obj)
plot(obj, type="l", xlab="samples", ylab="mean")
```

```{r, cmean02, "Cumulative mean along the MCMC chain with thining of 200."}
obj <- fitmc03.mc[,1]
obj <- cumsum(obj) / seq_along(obj)
plot(obj, type="l", xlab="samples", ylab="mean")
```

To improve mixing one should use longer chains, larger thining and drop initial samples.

### Convergence

Convergence in **Markov Chain Monte Carlo (MCMC)** means that the chain has reached the **stationary (target) distribution**, ensuring that samples are representative of the posterior. Here are the key ways to check for convergence:

---

### **1. Trace Plots (Visual Inspection)**
   - Plot the parameter values over iterations.
   - **Converged:** The plot looks like a **random, stationary cloud** with no trends or drifts.
   - **Not Converged:** If the chain shows strong trends, drifts, or long autocorrelated stretches, it hasn't converged.

---

### **3. Autocorrelation Analysis**
   - Compute the **autocorrelation function (ACF)** for different lags.
   - **Converged:** Autocorrelation decreases rapidly (i.e., samples are independent).
   - **Not Converged:** High autocorrelation suggests the chain is stuck in certain regions.

---

### **5. Running Mean (Cumulative Mean Plots)**
   - Plot the **cumulative mean** of a parameter over iterations.
   - **Converged:** The mean stabilizes after the burn-in.
   - **Not Converged:** The mean keeps shifting, indicating ongoing drift.

---

### **6. Geweke Diagnostic**
   - Compares the **mean and variance** of the first part of the chain with the last part.
   - **Converged:** No significant difference.
   - **Not Converged:** Large differences suggest the chain has not settled into the posterior.

---

### **7. Heidelberger-Welch Test**
   - Automatically tests for convergence and suggests a burn-in period.
   - **Converged:** Test passes, and burn-in is short.
   - **Not Converged:** Test fails, meaning more samples or a longer burn-in are needed.

---

### **How to Improve Convergence If It's Poor**
- **Run the chain longer** to allow better exploration.
- **Use better tuning** (adjust step sizes in Metropolis-Hastings or Hamiltonian Monte Carlo).
- **Reparameterize the model** to improve efficiency.
- **Use multiple independent chains** to compare behavior.
- **Use a more advanced sampler** (e.g., **Hamiltonian Monte Carlo** instead of random-walk Metropolis).

Ensuring convergence is **critical** for making valid statistical inferences from MCMC! 🚀

## MCMC arguments

As mentioned above `ADMB` has several options for MCMC. Here we demonstrate one of them, `mcprobe` which sets a fat-tailed proposal distribution, as an example of how to use the `SCAMCMC` objects.


mcprobe

For some models, there may be concern of being “stuck” in a local minimum and simply never proposing a value far enough away to escape it and find other regions of high density. Obviously this problem would present issues for maximum likelihood inference as well. ADMB has a built-in algorithm which modifies the default proposal distribution so it occasionally proposes very distant parameters (i.e. “probes”)5. The mcprobe X argument initiates this option. The modified proposal distribution is a mixture distribution of normal and Cauchy distributions. The argument X controls how the two distributions are mixed, with larger values being more Cauchy (fatter tails, larger jumps). The range of valid inputs is 0.00001 to 0.499, and if no value is supplied a default of 0.05 is used

```{r}
mc <- SCAMCMC(mcprobe=0.1)
fitmc01 <- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = "MCMC", mcmc=SCAMCMC())
fitSumm(fitmc01)
fitmc01p <- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = "MCMC", mcmc=mc)
fitSumm(fitmc01p)
mc <- SCAMCMC(mcmc=1000, mcsave=1, mcprobe=0.0005)
fitmc02p <- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = "MCMC", mcmc=mc)
fitSumm(fitmc02p)
mc <- SCAMCMC(mcmc=200000, mcsave=200, mcprobe=0.0005)
fitmc03p <- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = "MCMC", mcmc=mc)
fitSumm(fitmc03p)


```

All fits together

```{r}
plot(FLStocks(ll=hke1567 + fit, mc=hke1567 + fitmc00, mc_alt=hke1567 + fitmc01))
```

