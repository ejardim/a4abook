# Uncertainty \label{sec:predsim}

Uncertainty is a fundamental aspect of scientific advice, serving as a reflection of the inherent limitations within the knowledge base used to construct evidence and support scientific opinions. It highlights the gaps, variability, and potential biases present in data, methods, and modeling assumptions that underlie scientific conclusions.

In fisheries science, a field that has evolved primarily to provide evidence-based advice for the sustainable exploitation of marine resources, acknowledging and addressing uncertainty is of major importance. Given the dynamic, complex, and partially observable nature of aquatic ecosystems, the need to systematically characterize and communicate uncertainty is paramount to ensuring robust and credible assessments [@privitera2019].

In this section we'll address two important elements of quantifying uncertainty in stock assessment results, prediction error and propagation of uncertainty across modelling stages.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(FLa4a)
library(ggplot2)
data(hke1567)
data(hke1567.idx)
```

The two workhorses for this topic are `predict()` and `simulate()` which are implemented to work with `sca` fits of type `"assessment"`. Note `fit = "MP"` doesn't compute the variance-covariance matrix of the parameters, which is essential for simulating.

This chapter is based on the following model:

```{r, predsim_fit0}
nsim <- 250
fmod <- ~s(age, k = 4) +
    s(year, k = 8) +
    s(year, k = 8, by = as.numeric(age == 0)) +
    s(year, k = 8, by = as.numeric(age == 4))
qmod <- list(~I(1/(1 + exp(-age))))
fit0 <- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod)
stk0 <- hke1567 + fit0
```

## The `simulate` and `predict` methods

### `predict()`

The predict method computes the quantities of interest using the estimated coefficients and the design matrix of the model, defined via the formulas in the submodels. The method uses a fitted model object, created by a call to `sca`, and returns a list with one element for each submodel, where each element is a `FLQuants` object.

```{r, predsim_fit_pred}
fit.pred <- predict(fit0)
lapply(fit.pred, names)
```

The `stkmodel` element reports `harvest`, `rec` and `ny1`. The `qmodel` reports one `FLQuant` for each index, and the `vmodel` element returns one `FLQuant` for catch (in fact `catch.n`) and one for each index. This allows easy access to the parameterised parts of the model, for example the initial population structure, `ny1`, can be accessed via `fit.pred$stkmodel$ny1`.

```{r, echo=FALSE}
fit.pred$stkmodel$ny1
```

If the fitted object has iterations, as after using the `simulate` method, which will be discussed in the next section, predict will be applied to each iter, generating distributions of the above mentioned quantities (Figure \@ref(fig:simny1)).

```{r, simny1, fig.cap='Simulations from the model prediction of initial age structure', fig.pos = 'H', fig.height = 4, echo=FALSE, message=FALSE, warning=FALSE}
fit.sim <- simulate(fit0, nsim = 250)
sim.pred <- predict(fit.sim)
fit_sim_ny1 <- sim.pred$stkmodel$ny1

# reduce to quantiles
fit_sim_ny1 <- quantile(fit_sim_ny1, prob = c(0.025, 0.50, 0.975))

# reshape
dat <- reshape(
  as.data.frame(fit_sim_ny1, drop = TRUE),
  timevar = "iter", idvar = c("age"), direction = "wide"
)

# plot
ggplot(data = dat, aes(x = age, y = `data.50%`)) +
  geom_ribbon(aes(ymin = `data.2.5%`, ymax = `data.97.5%`),
    fill = "red", alpha = .15
  ) +
  geom_point() +
  geom_line() +
  ylab("Estimated initial age structure (numbers)") +
  scale_x_continuous(breaks = pretty(dat$age))
```

### `simulate()`

As the name implies `simulate` is used to generate simulations of the fit. It operates over objects of class `a4aFitSA` using the method `mvrnorm()` provided by the R package MASS [@mass]. The method generates random draws from a multivariate normal distribution with mean given by the coefficients of the model, and variance matrix given by the estimated covariance matrix of the coefficients. The method approximates the joint distribution of the model parameters as a multivariate normal in the log space, which is inline with the assumption made by `ADMB` when fitting the model. This approach is called 'parametric bootstrap', and it's a common method for generating uncertainty in the parameters of a model.

`simulate()` operates at the submodel level, *e.g.* `simulate(fit0@pars@qmodel, nsim=250)`, when called over a `a4aFitSA` object the method simply runs `simulate()` over each of the submodels. In this case it returns an object of the same class with model parameters replaced by `nsim` simulated parameters and updated slots `stock.n`, `catch.n` and `harvest`. Figure \@ref(fig:predsimhist) depicts the distribution of a parameter, the observation error of the first survey index.

```{r, predsimhist, fig.cap="Histogram of 250 draws from the approximate distribution of the estimate of survey observation error.", echo=FALSE}
hist(
  exp(coef(fit.sim)$vmodel[[2]]),
  main = "250 draws of a model parameter",
  nclass = 10,
  xlab = "Survey index observation error"
)
```

In some simulation studies one needs to make sure the random draws are the same, which in `R` is obtained by explicitly setting the random seed with the method `set.seed()`. The same is achieved in this case as the example below shows.

```{r}
set.seed(1234)
fit.sim1 <- simulate(fit0, nsim = 250)
set.seed(1234)
fit.sim2 <- simulate(fit0, nsim = 250)
all.equal(fit.sim1, fit.sim2)
```

If the whole stock is of interest, for example, to inspect model predictions of $SSB$, the user should use the `+` method with a fitted object including iterations. In such case the `stock.n`, `catch.n` and `harvest` slots of the stock object will be updated and the usual metrics can be computed and extracted, *e.g.* `ssb(stk.pred)`. Figure \@ref(fig:sim2) depicts the stock summary plot after adding estimation uncertainty through `simulate`.

```{r, sim2, fig.cap="Stock summary of the simulated and fitted data"}
stk.pred <- hke1567 + fit.sim
plot(FLStocks(simulated=stk.pred, fitted=stk0))
```

<!--## Prediction error-->


<!--
To address this topic we'll borrow from state-space modelling to define prediction error as the combination of observation error and estimation error (REF). Although `sca()` is not a state-space model, some of the inherent concepts of statistical catch at age models are similar to state-space models. `sca()` estimates parameters to model underlying unobserved processes, *e.g.* recruitment, catchability or fishing mortality, based on indirect observations of those processes, number of fish caught by the fleet and changes in abundance at sea.

Another similar approach can be found in statistical learning model. @hastie2009 decomposes prediction error into irreducible error, square bias and variance. Where the first term refers to variance that cannot be avoided, in our case estimated by residual variance. The second term, bias is ignored as the true value isn't known and the fits are assumed to be unbiased. The last term is the variance of the prediction due to the training dataset samples, which we're approximating with parameter estimation variance.

<!--EJ NOTE: I think this paper supports the same idea [@harville85], although the maths are a bit (too...) much for me.-->

<!--In econometrics @greene explains the need to sum random noise and estimation error to build prediction intervals.

@book{greene2018econometric,
  title={Econometric Analysis},
  author={Greene, William H.},
  year={2018},
  edition={8th},
  publisher={Pearson Education},
  isbn={9780134461366}
}-->

<!--However, `sca`'s residuals are estimates of observation error (measurement noise) and model error (misspecification, omitted variables), which means it will be an overestimate of observation error.

Nevertheless, in this section we'll assume prediction error to be the sum of residual variance and estimation variance, and will describe how to compute both elements and bringing them together in a `FLStock` object.-->

```{r, eval=FALSE, echo=FALSE}
nsim <- 250
fmod <- ~s(age, k = 4) +
    s(year, k = 8) +
    s(year, k = 8, by = as.numeric(age == 0)) +
    s(year, k = 8, by = as.numeric(age == 4))
qmod <- list(~I(1/(1 + exp(-age))))
fit0 <- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod)
stk0 <- hke1567 + fit0

# compute deviances
res0 <- residuals(fit0, hke1567, hke1567.idx, type="deviances")
# build object with estimation uncertainty
stk.eu <- hke1567 + simulate(fit0, nsim)
# build object with residual uncertainty
stk.ru <- propagate(stk0, nsim) + res0
# build object with prediction uncertainty
stk.pu <- hke1567 + simulate(fit0, nsim) + res0
```

```{r, preduncert, fig.cap="Prediction (pu), residual (ru) and estimation (eu) uncertainty", echo=FALSE, eval=FALSE}
plot(FLStocks(pu=stk.pu, eu=stk.eu, ru=stk.ru)) + facet_grid(qname~stock, scales="free") + theme(legend.position = "top")
```

<!--## Confidence interval coverage-->


## Propagate uncertainty into stock assessment

```{r, message=FALSE, warning=FALSE}
stk00 <- readRDS("data/MUT1_stk.rds")
idx00 <- readRDS("data/MUT1_idx.rds")
```

In a multistage stock assessment process as described in this book, it's important to be able to propagate uncertainty across the different stages. This section describes methods to propagate uncertainty across stages and compares their outcomes in terms of stock assessment outputs.

The idea is to add uncertainty as one moves from one stage to the next. If a stock has uncertainty on it's growth parameters, or natural mortality, or any other quantity estimated or set during the input data preparation, the model fit uncertainty will be added to it by generating iterations in the input data which are then used to fit the stock assessment model. The suggested workflow is:

1. Add uncertainty in growth or M parameters.
2. Draw from the parameters distribution.
3. Compute metrics for stock assessment.
    1. If there's uncertainty in growth parameters use slicing to created iterations of metrics by age, e.g. catch at age and index at age.
    2. If there's uncertainty in M parameters draw from the distribution and generate iterations of the M matrix.
    3. If both draw from growth and M parameters, potentially having into account correlation between those parameters, and generate iterations of age based metrics and M.
4. Fit the stock assessment model to each iteration
5. Simulate from each fit
6. Aggregate results in single `FLStock` object.

In this section we give an example of how uncertainty in natural mortality, set up using the `m()` method and the class `a4aM`, is propagated through the stock assessment. We'll use the stock of Red Mullet in the Mediterranean GSA 1 (see Introduction for details) and 3 methods to add estimation uncertainty (step 5 above):

1. Take one draw of the fit
2. Take n draws of the fit and summarize with the median
3. Take n draws of the fit and combine all

These outcomes will be compared with a fit across M iterations without any sampling from the fit.

Using `a4a` methods we'll model natural mortality using a negative exponential model by age, Jensen's estimator for the level and no time trend. We include multivariate normal uncertainty using the `mvrnorm()` method and create 250 iterations.

```{r}
nits <- 25

shape <- FLModelSim(model=~exp(-age-0.5))
level <- FLModelSim(model=~k^0.66*t^0.57, params = FLPar(k=0.4, t=10),
                     vcov=matrix(c(0.001, 0.01,0.01, 1), ncol=2))
#trend <- FLModelSim(model=~b, params=FLPar(b=0.5), vcov=matrix(0.02))

m4 <- a4aM(shape=shape, level=level)
m4 <- mvrnorm(nits, m4)
range(m4)[] <- range(stk00)[]
range(m4)[c("minmbar","maxmbar")]<-c(1,1)
flq <- m(m4)[]
quant(flq) <- "age"
stk0 <- propagate(stk00, nits)
m(stk0) <- flq
```

The M matrix for this stock is shown in Figure\@ref(fig:m)).

```{r, m, fig.cap="Natural mortality generated from M model's parameter uncertainty", echo=FALSE, message=FALSE, warning=FALSE}
bwplot(data~factor(age), data=m(stk0))
```

We fit the same model to the new stock object which has uncertainty in the natural mortality and add estimation uncertainty following the methods described above.


```{r}
# create objects to store the results
stk01 <- stk0
stk02 <- stk0
stk03 <- propagate(stk00, nits*nits)

# run without estimation uncertainty
stk04 <- stk00 + sca(stk0, idx00)
# update M, the "+" method doesn't do it automatically
m(stk04) <- flq

for(i in 1:nits){
    stk <- iter(stk0, i)
    fit <- sca(stk, idx00)
    # Method 1
    iter(stk01, i) <- stk + simulate(fit, 1)
    # Method 2
    iter(stk02, i) <- qapply(stk + simulate(fit, nits), iterMedians)
    # Method 3
    iter(stk03, (nits*(i-1)+1):(nits*i)) <- stk + simulate(fit, nits)
}

```


```{r, mprop, fig.cap="Stock summary. Stock metrics computed over fits including uncertainty in M and estimation uncertainty", echo=FALSE}
plot(FLStocks("M"=stk04, "M + 1 estimation sample"=stk01, "M + estimation median"=stk02, "M + n estimation samples"=stk03)) + facet_grid(qname~stock, scales="free") + theme(legend.position = "top")
```


