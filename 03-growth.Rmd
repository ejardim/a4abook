# Modelling Individual Growth and Using Stochastic Slicing to Convert Length-based Data Into Age-based Data {#growth}

```{R, message=FALSE, warning=FALSE, echo=FALSE}

# libraries
library(devtools)
library(FLa4a)
library(XML)
library(reshape2)
library(ggplotFL)
# datasets
data(ple4)
data(ple4.indices)
data(ple4.index)
data(rfLen)
```

The `a4a` stock assessment framework is based on age dynamics. Therefore, length information must be processed before running the stock assessment model. The methods in this section provide the analyst flexibility to use a large range of information sources, _e.g._ literature or online databases, to collect information about the species growth model and uncertainty about the model parameters.

The framework allows the analyst to parametrize individual growth, set the assumptions about it and condition the stock assessment model on those decisions. It incentivize the uptake of estimation uncertainty, as well as exploring several parametrizations and/or growth models to deal with structural uncertainty.

Within the `a4a` framework this is handled using the `a4aGr` class and its methods. This class stores information about the growth model and it's parameters, including parameters' uncertainty and the distributions governing it. The class's main method is `l2a()` that converts length to ages based on a length based stock object and using the model defined in the `a4aGr` instance.

## a4aGr - The growth class

The conversion of length data to age is performed through the use of a growth model. The implementation is done through the `a4aGr` class .

```{r, show_a4aGr} 
showClass("a4aGr")
```

To construct an `a4aGr` object, the growth model and parameters must be provided. Here we show an example using the von Bertalanffy growth model. To create the `a4aGr` object it's necessary to pass the model equation ($length \sim time$), the inverse model equation ($time \sim length$) and the parameters. Any growth model can be used as long as it's possible to write the model (and the inverse) as an R formula.

```{r, a4aGr_vB_example} 
vbObj <- a4aGr(
	grMod=~linf*(1-exp(-k*(t-t0))),      
	grInvMod=~t0-1/k*log(1-len/linf),      
	params=FLPar(linf=58.5, k=0.086, t0=0.001, units=c("cm","year-1","year"))     
)
```

Check the model and its inverse:

```{r}
lc=20
predict(vbObj, len=lc)
predict(vbObj, t=predict(vbObj, len=lc))
```

The predict method allows the transformation between lengths and ages, and vice-versa, using the growth model.

```{r, predict_araGr_example} 
predict(vbObj, len=5:10+0.5)
predict(vbObj, t=5:10+0.5)
```

## Adding uncertainty to growth parameters with a multivariate normal distribution

Uncertainty in the growth model is introduced through the inclusion of parameter uncertainty. This is done by making use of the parameter variance-covariance matrix (the `vcov` slot of the `a4aGr` class) and setting a distribution for the parameters. The variance-covariance matrix could come from the parameter uncertainty from fitting the growth model parameters, or a meta analysis of correlation between parameters.

Here we set the variance-covariance matrix by scaling a correlation matrix, using a cv of 0.2. Based on 

$$\rho_{x,y}=\frac{\Sigma_{x,y}}{\sigma_x \sigma_y}$$

and 

$$CV_x=\frac{\sigma_x}{\mu_x}$$

```{r, set_vcov_example} 
# Make an empty cor matrix
cm <- diag(c(1,1,1))
# k and linf are negatively correlated while t0 is independent
cm[1,2] <- cm[2,1] <- -0.5
# scale cor to var using CV=0.2
cv <- 0.2
p <- c(linf=60, k=0.09, t0=-0.01)
vc <- matrix(1, ncol=3, nrow=3)
l <- vc
l[1,] <- l[,1] <- p[1]*cv
k <- vc
k[,2] <- k[2,] <- p[2]*cv
t <- vc
t[3,] <- t[,3] <- p[3]*cv
mm <- t*k*l
diag(mm) <- diag(mm)^2
mm <- mm*cm
# check that we have the intended correlation
all.equal(cm, cov2cor(mm))
```

Create the `a4aGr` object as before but now we also include the `vcov` argument for the variance-covariance matrix.

```{r, making_vcov_example} 
vbObj <- a4aGr(
  grMod=~linf*(1-exp(-k*(t-t0))),
  grInvMod=~t0-1/k*log(1-len/linf),
  params=FLPar(
    linf=p["linf"], k=p["k"], t0=p["t0"],
    units=c("cm","year-1","year")),
    vcov=mm
)
```

First we show a simple example where we assume that the parameters are represented using a multivariate normal distribution. Note that the object we have just created has a single iteration of each parameter.

```{r, simulate_vcov_example} 
vbObj@params
dim(vbObj@params)
```

We simulate 250 iterations from the `a4aGr` object by calling `mvrnorm()` using the variance-covariance matrix we created earlier. The object will now have 250 iterations of each parameter, randomly sampled from the multivariate normal distribution.

```{r}
vbNorm <- mvrnorm(250,vbObj)
vbNorm@params
dim(vbNorm@params)
```

We can now convert from length to ages data based on the 250 parameter iterations, which will produce 250 sets of age data. For example, the next code will convert a single length vector using each of the 250 parameter iterations.

```{r}
lvec <- 5:10+0.5
ages <- predict(vbNorm, len=lvec)
dim(ages)
```

The marginal distributions of the parameters can be seen in Figure \@ref(fig:plotnormparams).

```{r, plotnormparams, fig.cap="The marginal distributions of each of the parameters from using a multivariate normal distribution.", echo=FALSE}
par(mfrow=c(1,3))
hist(c(params(vbNorm)["linf",]), main="linf", prob=TRUE, xlab="")
hist(c(params(vbNorm)["k",]), main="k", prob=TRUE, xlab="")
hist(c(params(vbNorm)["t0",]), main="t0", prob=TRUE, xlab="")
```

Pairwise plots show the covariance between each pair of parameters and the shape of their correlation (Figure \@ref(fig:plotnormscatter)).

```{r, plotnormscatter, fig.cap="Scatter plot of the 10000 samples parameter from the multivariate normal distribution.", echo=FALSE}
splom(data.frame(t(params(vbNorm)@.Data)), par.settings=list(plot.symbol=list(pch=19, cex=0.1, col=1)))
```

Using the new generated age vectors one can depict the growth curves for the 250 iterations, which displays individual growth uncertainty (Figure \@ref(fig:plotmvgrowth)).

```{r, plotmvgrowth, fig.cap="Growth curves using parameters simulated from a multivariate normal distribution.", echo=FALSE}
#df0 <- melt(predict(vbNorm, t=0:50+0.5))
bwplot(value~factor(Var1), data=melt(predict(vbNorm, t=0:50+0.5)), par.settings=list(plot.symbol=list(cex=0.2, col="gray50"), box.umbrella=list(col="gray40"), box.rectangle=list(col="gray30")), ylab="length (cm)", xlab="age (years)", scales=list(x=list(rot=90)))
#boxplot(t(predict(vbNorm, t=0:50+0.5)))

#xyplot(value~factor(Var1), groups=iter, type="l", data=melt(predict(vbNorm, t=0:50+0.5)), ylab="length (cm)", xlab="age (years)", scales=list(x=list(rot=90)), col="gray50")

```

## Adding uncertainty to growth parameters with a multivariate triangle distribution
\label{sec:growth_triangle_cop}

One alternative to using a normal distribution is to use a [triangle distribution](http://en.wikipedia.org/wiki/Triangle\_distribution). We use the package `triangle` [@R-triangle]  where this distribution is parametrized using the minimum, maximum and median values. This can be very attractive if the analyst needs to scrape information from the web or literature, and use a meta-analysis to build the parameters' distribution. The triangle distribution has the advantage of setting hard tail limits, avoiding to generate extreme values. Here we show an example of setting a triangle distribution with values taken from Fishbase [@fishbase].

The following shows a method to extract data from fishbase. However, due to potential changes in the way one gets data from fishbase from within R, we've downloaded the data beforehand and load it for this example.

```{r, fbscrap, eval=FALSE}
# The web address for the growth parameters for redfish (Sebastes norvegicus)
addr <- "https://fishbase.se/PopDyn/PopGrowthList.php?ID=501"
# Scrape the data
tab <- try(readHTMLTable(addr))
```

```{r,  echo=FALSE}
# Load local copy if no web
load("data/tab.RData")
```

```{r, tri_example}
# Interrogate the data table and get vectors of the values
linf <- as.numeric(as.character(tab$dataTable[,2]))
k <- as.numeric(as.character(tab$dataTable[,4]))
t0 <- as.numeric(as.character(tab$dataTable[,5]))
# Set the min (a), max (b) and median (c) values for the parameter as a list of lists
# Note that t0 has no 'c' (median) value. This makes the distribution symmetrical
triPars <- list(
  linf=list(a=min(linf), b=max(linf), c=median(linf)),
  k=list(a=min(k), b=max(k), c=median(k)),
  t0=list(a=median(t0, na.rm=T)-IQR(t0, na.rm=T)/2, b=median(t0, na.rm=T)+IQR(t0, na.rm=T)/2))

# Draw 250 samples using mvrtriangle
vbTri <- mvrtriangle(250, vbObj, paramMargins=triPars)
```

Note that in this case we're not building a new object with all the parameters' information. We're using the argument `paramMargins` to pass the parameters' information to the method.

The marginals will reflect the uncertainty on the parameter values that were scraped from @fishbase but, as we don't really believe the parameters are multivariate normal, here we adopted a distribution based on a _t_ copula with triangle marginals. The marginal distributions can be seen in Figure \@ref(fig:plottriparams) and the shape of the correlation can be seen in Figure \@ref(fig:plottriscatter).

```{r, plottriparams, echo=FALSE, fig.cap="The marginal distributions of each of the parameters from using a multivariate triangle distribution."}
par(mfrow=c(1,3))
hist(c(params(vbTri)["linf",]), main="linf", prob=TRUE, xlab="")
hist(c(params(vbTri)["k",]), main="k", prob=TRUE, xlab="")
hist(c(params(vbTri)["t0",]), main="t0", prob=TRUE, xlab="")
```

```{r, plottriscatter, echo=FALSE, fig.cap="Scatter plot of the 10000 samples parameter from the multivariate triangle distribution."}
splom(data.frame(t(params(vbTri)@.Data)), par.settings=list(plot.symbol=list(pch=19, cex=0.1, col=1)))
```

We can still use `predict()` to see the growth model uncertainty (Figure \@ref(fig:plottrigrowth)). Comparing with Figure \@ref(fig:plotmvgrowth) one can see that using triangle distribution generates a lot less outliers, or values outside the central range of the growth curve.

```{r, plottrigrowth, echo=FALSE, fig.cap="Growth curves using parameters simulated from a multivariate triangle distribution."}
#df0 <- melt(predict(vbTri, t=0:50+0.5))
bwplot(value~factor(Var1), data=melt(predict(vbTri, t=0:50+0.5)), par.settings=list(plot.symbol=list(cex=0.2, col="gray50"), box.umbrella=list(col="gray40"), box.rectangle=list(col="gray30")), ylab="length (cm)", xlab="age (years)", scales=list(x=list(rot=90)))
#boxplot(t(predict(vbTri, t=0:20+0.5)))

#xyplot(value~factor(Var1), groups=iter, type="l", data=melt(predict(vbTri, t=0:50+0.5)), ylab="length (cm)", xlab="age (years)", scales=list(x=list(rot=90)), col="gray50")

```

Remember that the above examples use a variance-covariance matrix that we essentially made up. An alternative would be to scrape the entire growth parameters dataset from Fishbase and compute the shape of the variance-covariance matrix yourself.

## Adding uncertainty to growth parameters with statistical copulas

A more general approach to adding parameter uncertainty is to make use of statistical copulas [@sklar1959]. @copulahistory describes statistical “copula” as a multivariate cumulative distribution function with uniform margins on the unit interval. @sklar1959 highlighted the fact that any multivariate distribution can be expressed as a function of its margins and a copula. The idea is very attractive, one can simulate any multivariate distribution by setting a multivariate function in the unit interval which describes how the margins relate to each other, and scale up the univariate uniform margin with any continuos univariate distribution.

In our case this is possible with the `mvrcop()` function, borrowed from the package `copula` [@R-copula]. The example below keeps the same parameters and changes only the copula type and family but a lot more can be done. Check the package `copula` for more information.

```{r, copula_triangle_example} 
vbCop <- mvrcop(250, vbObj,
  copula="archmCopula",
  family="clayton",
  param=2,
  margins="triangle",
  paramMargins=triPars)
```

The shape of the correlation as well as the resulting growth curves are shown in Figures \@ref(fig:plotcoptriscatter) and \@ref(fig:plotcoptrigrowth).

```{r, plotcoptriscatter, echo=FALSE, fig.cap="Scatter plot of the 250 samples parameter from the using an archmCopula copula with triangle margins."}
splom(data.frame(t(params(vbCop)@.Data)), par.settings=list(plot.symbol=list(pch=19, cex=0.1, col=1)))
```

```{r, plotcoptrigrowth, fig.cap="Growth curves using parameters simulated from an archmCopula copula with triangle margins.", echo=FALSE}
bwplot(value~factor(Var1), data=melt(predict(vbCop, t=0:50+0.5)), par.settings=list(plot.symbol=list(cex=0.2, col="gray50"), box.umbrella=list(col="gray40"), box.rectangle=list(col="gray30")), ylab="length (cm)", xlab="age (years)", scales=list(x=list(rot=90)))

#xyplot(value~factor(Var1), groups=iter, type="l", data=melt(predict(vbCop, t=0:50+0.5)), ylab="length (cm)", xlab="age (years)", scales=list(x=list(rot=90)), col="gray50")

```

## Converting from length to age based data - the `l2a()` method

After introducing uncertainty in the growth model through the parameters it's time to transform the length-based dataset into an age-based dataset. The method that deals with this process is `l2a()`. The implementation of this method for the `FLQuant` class is the main workhorse. There are two other implementations, for the `FLStock` and `FLIndex` classes, which are mainly wrappers that call the `FLQuant` method several times.

When converting from length-based data to age-based data you need to be aware of how the aggregation of length classes is performed. For example, individuals in length classes 1-2, 2-3, and 3-4 cm may all be considered as being of age 1 (obviously depending on the growth model). How should the values in those length classes be combined?

If the values are abundances then the values should be summed. Summing other types of values, such as mean weight, does not make sense. Instead these values are averaged over the length classes (possibly weighted by the abundance). This is controlled using the `stat` argument which can be either `mean` or `sum` (the default). Fishing mortality is not computed to avoid making wrong assumptions about the meaning of F at length.

We demonstrate the method by converting a catch-at-length `FLQuant` to a catch-at-age `FLQuant`. First we make an `a4aGr` object with a multivariate triangle distribution using parameters extracted from an AI agent. We use 10 iterations as an example, and call `l2a()` by passing in the length-based `FLQuant` and the `a4aGr` object.

```{r, FLQ_l2a, message=FALSE, warning=FALSE}
triPars <- list(
  linf=list(a=55, b=60),
  k=list(a=0.05, b=0.06),
  t0=list(a=-3, b=-2))

# Draw 10 samples using mvrtriangle
vbTriSmall <- mvrtriangle(10, vbObj, paramMargins=triPars)
# slice catch numbers at lengths to ages by summing catches
cth.n <- l2a(catch.n(rfLen.stk), vbTriSmall)
# note there's a lot of 0 catches so we'll set the plus group at 21
cth.n <- setPlusGroup(cth.n, 21)
# there's also negative ages. The simulated data included individuals in lengths that won't show in the catches, like 1 cm. We'll trim those ages
cth.n <- cth.n[ac(0:21)]

# slice catch weights at lengths to ages by averaging catches
cth.wt <- l2a(catch.wt(rfLen.stk), vbTriSmall, stat="mean")
# same process to deal with negative ages
cth.wt <- cth.wt[ac(0:21)]
```

In the previous example, the `FLQuant` object that was sliced (`catch.n(rfLen.stk)`) had only one iteration. This iteration was sliced by each of the iterations in the growth model. It is possible for the `FLQuant` object to have the same number of iterations as the growth model, in which case each iteration of the `FLQuant` and the growth model are used together. It is also possible for the growth model to have only one iteration while the `FLQuant` object has many iterations. The same growth model is then used for each of the `FLQuant` iterations. As with all `FLR` objects, the general rule is _one or n_ iterations.

As well as converting one `FLQuant` at a time, we can convert entire `FLStock` and `FLIndex` objects. In these cases the individual `FLQuant` slots of those classes are converted from length-based to age-based. As mentioned above, the aggregation method depends on the type of values the slots contain. The abundance slots (`*.n`, such as `stock.n`) are summed. The `*.wt`, `m`, `mat`, `harvest.spwn` and `m.spwn` slots of an `FLStock` object are averaged. The `catch.wt` and `sel.pattern` slots of an `FLIndex` object are averaged, while the `index`, `index.var` and `catch.n` slots are summed. 

The method for `FLStock` classes takes an additional argument for the plusgroup.

```{r, FLS_FLI_l2a, message=FALSE}
aStk <- l2a(rfLen.stk, vbTriSmall, plusgroup=21)
aIdx <- l2a(rfTrawl.idx, vbTriSmall)
```

When converting with `l2a()` all lengths above Linf are converted to the maximum age, as there is no information in the growth model about how to deal with individuals larger than Linf. 

