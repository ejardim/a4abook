[["index.html", "Fish stock assessment with R The a4a Initiative Chapter 1 Before starting 1.1 License, documentation and development status 1.2 Installing and loading libraries 1.3 How to read this document 1.4 How to get help 1.5 Notation", " Fish stock assessment with R The a4a Initiative John Doe and friends 2024-12-18 Chapter 1 Before starting 1.1 License, documentation and development status The software is released under the EUPL 1.1. For more information on the a4a methodologies refer to Jardim, et.al, 2014, Millar, et.al, 2014 and Scott, et.al, 2016. Documentation can be found at http://flr-project.org/FLa4a. You are welcome to: Submit suggestions and bug-reports at: https://github.com/flr/FLa4a/issues Send a pull request on: https://github.com/flr/FLa4a/ Compose a friendly e-mail to the maintainer, see packageDescription('FLa4a') 1.2 Installing and loading libraries To run the FLa4a methods the reader will need to install the package and its dependencies and load them. Some datasets are distributed with the package and as such need to be loaded too. # from CRAN install.packages(c(&quot;copula&quot;,&quot;triangle&quot;, &quot;coda&quot;, &quot;grid&quot;, &quot;gridExtra&quot;, &quot;latticeExtra&quot;)) # from FLR install.packages(c(&quot;FLCore&quot;, &quot;FLa4a&quot;), repos=&quot;http://flr-project.org/R&quot;) # libraries library(devtools) library(FLa4a) library(XML) library(reshape2) library(ggplotFL) # datasets data(ple4) data(ple4.indices) data(ple4.index) data(rfLen) packageVersion(&quot;FLCore&quot;) ## [1] &#39;2.6.20.920&#39; packageVersion(&quot;FLa4a&quot;) ## [1] &#39;1.9.0&#39; 1.3 How to read this document The target audience for this document are readers with some experience in R and some background on stock assessment. The document explains the approach being developed by a4a for fish stock assessment and scientific advice. It presents a mixture of text and code, where the first explains the concepts behind the methods, while the last shows how these can be run with the software provided. Moreover, having the code allows the reader to copy/paste and replicate the analysis presented here. The sections and subsections are as independent as possible, so it can be used as a reference document for the FLa4a. 1.4 How to get help a4a is build around S4 classes. S4 classes and methods in R offer an object-oriented programming framework but in order to access the documentation requires specific terminology. In this section we will demonstrate how to get information on the main building blocks of a4a. For example, FLStock is one of our main components in order to run our stock assessment model. We can check the structure of an FLStock object as follows: showClass(&quot;FLStock&quot;) ## Class &quot;FLStock&quot; [package &quot;FLCore&quot;] ## ## Slots: ## ## Name: catch catch.n catch.wt discards discards.n ## Class: FLQuant FLQuant FLQuant FLQuant FLQuant ## ## Name: discards.wt landings landings.n landings.wt stock ## Class: FLQuant FLQuant FLQuant FLQuant FLQuant ## ## Name: stock.n stock.wt m mat harvest ## Class: FLQuant FLQuant FLQuant FLQuant FLQuant ## ## Name: harvest.spwn m.spwn name desc range ## Class: FLQuant FLQuant character character numeric ## ## Extends: ## Class &quot;FLS&quot;, directly ## Class &quot;FLComp&quot;, by class &quot;FLS&quot;, distance 2 ## ## Known Subclasses: ## Class &quot;FLStockR&quot;, directly, with explicit coerce The object oriented structure of a4a gives the opportunity to change the behavior of a function according to the object that is applied to. For example we can check the available methods of the function plot showMethods(&quot;plot&quot;) ## Function: plot (package base) ## x=&quot;a4aFitCatchDiagn&quot;, y=&quot;missing&quot; ## x=&quot;a4aFit&quot;, y=&quot;FLIndices&quot; ## x=&quot;a4aFit&quot;, y=&quot;FLStock&quot; ## x=&quot;a4aFitMCMCs&quot;, y=&quot;missing&quot; ## x=&quot;a4aFitResiduals&quot;, y=&quot;missing&quot; ## x=&quot;a4aFits&quot;, y=&quot;missing&quot; ## x=&quot;ANY&quot;, y=&quot;ANY&quot; ## x=&quot;color&quot;, y=&quot;ANY&quot; ## x=&quot;Copula&quot;, y=&quot;ANY&quot; ## x=&quot;FLBiol&quot;, y=&quot;missing&quot; ## x=&quot;FLBiols&quot;, y=&quot;missing&quot; ## x=&quot;FLCohort&quot;, y=&quot;missing&quot; ## x=&quot;FLIndexBiomass&quot;, y=&quot;missing&quot; ## x=&quot;FLIndex&quot;, y=&quot;missing&quot; ## x=&quot;FLIndices&quot;, y=&quot;missing&quot; ## x=&quot;FLPar&quot;, y=&quot;missing&quot; ## x=&quot;FLQuant&quot;, y=&quot;FLQuant&quot; ## x=&quot;FLQuant&quot;, y=&quot;missing&quot; ## x=&quot;FLQuantPoint&quot;, y=&quot;FLQuant&quot; ## x=&quot;FLQuantPoint&quot;, y=&quot;FLQuants&quot; ## x=&quot;FLQuantPoint&quot;, y=&quot;missing&quot; ## x=&quot;FLQuants&quot;, y=&quot;FLPar&quot; ## x=&quot;FLQuants&quot;, y=&quot;FLPars&quot; ## x=&quot;FLQuants&quot;, y=&quot;missing&quot; ## x=&quot;FLSR&quot;, y=&quot;missing&quot; ## x=&quot;FLSRs&quot;, y=&quot;ANY&quot; ## x=&quot;FLStock&quot;, y=&quot;FLPar&quot; ## x=&quot;FLStock&quot;, y=&quot;FLStock&quot; ## x=&quot;FLStock&quot;, y=&quot;FLStocks&quot; ## x=&quot;FLStock&quot;, y=&quot;missing&quot; ## x=&quot;FLStocks&quot;, y=&quot;FLPar&quot; ## x=&quot;FLStocks&quot;, y=&quot;missing&quot; ## x=&quot;mvdc&quot;, y=&quot;ANY&quot; ## x=&quot;profile.mle&quot;, y=&quot;missing&quot; by calling showMethods R prints all the possible uses of the plot function. We want to see what it does when it is called on an FLStock object with no other object. We observe that plot takes two arguments, x and y. So, in the signature of the getMethod function we are going to use, we need to define both x and y. getMethod(&#39;plot&#39;, signature = list(&quot;FLStock&quot;,&quot;missing&quot;)) ## Method Definition: ## ## function (x, y, ...) ## { ## .local &lt;- function (x, metrics = list(Rec = rec, SSB = ssb, ## Catch = catch, F = fbar), na.rm = TRUE, ...) ## { ## metrics &lt;- metrics(x, metrics = metrics) ## if (&quot;F&quot; %in% names(metrics)) ## units(metrics$F) &lt;- paste0(range(x, c(&quot;minfbar&quot;, ## &quot;maxfbar&quot;)), collapse = &quot;-&quot;) ## if (&quot;SSB&quot; %in% names(metrics)) { ## if (all(dimnames(metrics$SSB)$unit %in% c(&quot;F&quot;, &quot;M&quot;))) { ## metrics$SSB &lt;- metrics$SSB[, , &quot;F&quot;] + metrics$SSB[, ## , &quot;M&quot;] ## if (&quot;Rec&quot; %in% names(metrics)) ## metrics$Rec &lt;- unitSums(metrics$Rec) ## } ## } ## if (&quot;Rec&quot; %in% names(metrics)) { ## if (dim(metrics$Rec)[4] &gt; 1) { ## metrics$Rec[metrics$Rec == 0] &lt;- NA ## } ## } ## p &lt;- plot(metrics, na.rm = na.rm, ...) + ylim(c(0, NA)) ## if (&quot;SSB&quot; %in% names(metrics)) ## if (all(dimnames(metrics$SSB)$unit %in% c(&quot;F&quot;, &quot;M&quot;))) { ## return(p + theme(legend.position = &quot;bottom&quot;, ## legend.key = element_blank()) + labs(color = &quot;Sex&quot;) + ## scale_color_manual(name = &quot;&quot;, labels = c(&quot;Both&quot;, ## &quot;F&quot;, &quot;M&quot;), values = flpalette_colours(3))) ## } ## return(p) ## } ## .local(x, ...) ## } ## &lt;bytecode: 0x600e20f92c20&gt; ## &lt;environment: namespace:ggplotFL&gt; ## ## Signatures: ## x y ## target &quot;FLStock&quot; &quot;missing&quot; ## defined &quot;FLStock&quot; &quot;missing&quot; 1.5 Notation Along this chapter the notation presented in Table 1.1 will be used. Mathematical descriptions will be kept as simple as possible for readability. Table 1.1: Mathematical notation Type Symbol Description variables \\(C\\) catches \\(F\\) fishing mortality \\(M\\) natural mortality \\(R\\) recruitment \\(Q\\) vessel or fleet catchability \\(w\\) weights \\(l\\) likelihood \\(I\\) abundance index \\(S\\) spawning stock biomass \\(CV\\) coefficient of variation \\(D\\) residuals or deviances \\(N\\) normal distribution \\(\\beta\\) parameter \\(a\\) stock-recruitment parameter \\(b\\) stock-recruitment parameter \\(\\sigma^2\\) variance of catch \\(\\tau^2\\) variance of index \\(\\phi^2\\) variance of predicted recruitment \\(\\upsilon^2\\) variance of residuals subscripts \\(a\\) age \\(y\\) year \\(C\\) catch \\(I\\) abundance index \\(N\\) normal distribution \\(s\\) survey \\(SR\\) stock recruitment relationship superscripts and accents \\(\\hat{}\\) observation \\(\\tilde{}\\) prediction \\(c\\) catches \\(s\\) abundance index "],["introduction.html", "Chapter 2 Introduction", " Chapter 2 Introduction The a4a stock assessment framework is based in a non-linear catch-at-age model implemented in R, FLR and ADMB that can be applied rapidly to a wide range of situations with low setup requirements. The framework is built of submodels which define the different parts of a statistical catch at age model that require structural assumptions. In the a4a framework these are fishing mortality-at-age, abundance indicies catchability-at-age, recruitment, observation variances of catch-at-age and abundance indices-at-age, and abundance-at-age in the first year of the data series (see section @ref{sec:math} for details). Other important processes, like natural mortality, individual growth and reproduction, are treated as fixed, as it’s common in stock assessment methods. Nevertheless, the a4a framework provides methods to condition these processes prior to the model fit, and propagate their uncertainty into the assessment process. See chapters XX and section XX. The submodels formulation uses linear models, which opens the possibility of using the linear modelling tools available in R. For example, mgcv gam formulas or factorial design formulas using lm(). The ‘language’ of linear models has been developing within the statistical community for many years, and constitutes an elegant way of defining models without going through the complexity of mathematical representations. This approach makes it also easier to communicate among scientists: J. A. Nelder, 1965, notation for randomized block design Wilkinson and Rodgers, 1973, symbolic description for factorial designs Hastie and Tibshirani, 1990, introduced notation for smoothers Chambers and Hastie, 1991, further developed for use in S "],["introduction-to-splines.html", "Chapter 3 Introduction to Splines 3.1 Understanding Spline Basics 3.2 Generate some artificial data 3.3 Cubic Regression Splines 3.4 Thin plate spline 3.5 Comparison 3.6 The mgcv package inside a4a 3.7 On the number of knots \\(k\\)", " Chapter 3 Introduction to Splines Splines are a powerful tool for modeling complex, non-linear relationships between variables in a flexible and interpretable way. They break a function into smooth, continuous polynomial segments, each called a piece or basis function, joined at specific points called knots. This piecewise approach allows us to capture the non-linearity in the data without overfitting. The core concept of “basis functions” is that they transform the input variable (or vector) \\(X\\) into a set of new variables, which are then used as inputs in the model. This allows the model to remain linear in these transformed variables, even though it can capture complex, non-linear relationships in the original variable. 3.1 Understanding Spline Basics The spline function is built by combining polynomial pieces, ensuring smoothness at the junctions, which are called knots. In regression, splines allow us to fit complex shapes by introducing non-linear trends while maintaining control over the smoothness. A simplified version of how splines work would be as follows: Let \\(S\\) our spline function, that is defined in an interval \\([a,b]\\). We seek to construct \\(S\\) by combining \\(k-1\\) polynomials \\(P\\), where \\(k\\) is the number of knots. Let also \\(t_{i}, i = 0, ..., k-1\\) the positions of the knots in the interval \\([a,b]\\). \\(S\\) is going to be defined as: \\[ S(t) = P_{0}(t), \\quad t_{0} \\leq t &lt; t_{1} \\\\ \\vdots \\\\ S(t) = P_{k-1}(t), \\quad t_{k-1} \\leq t &lt; t_{k} \\] The above definition is a simplified version of how splines work and can help as an intuitive approach. In reality splines need to satisfy some extra conditions like continuity on the points of junction, and of the first and second derivative. Depending on the basis functions the conditions may differ. The next part will demonstrate graphically how to approximate an unknown function through splines. 3.2 Generate some artificial data x &lt;- seq(0, 10, length.out = 100) y &lt;- sin(x) + rnorm(100, 0, 0.3) data &lt;- data.frame(x, y) ggplot(data, aes(x, y)) + geom_point() Figure 3.1: Data with Non-linear Trend We will use the package mgcv which is the one that is being used in a4a. 3.3 Cubic Regression Splines We fit a simple smoother on x with \\(k = 5\\): # Fit a natural cubic spline with ns() cr_model &lt;- gam(y ~ s(x, bs = &quot;cr&quot;, k = 5), data = data) # Predict and plot data$cr_fit &lt;- predict(cr_model) ggplot(data, aes(x, y)) + geom_point(alpha = 0.4) + geom_line(aes(y = cr_fit), color = &quot;darkgreen&quot;, linewidth = 1) Figure 3.2: Cubic regression spline fit The natural cubic spline fit above is constrained at the boundaries, by putting two of the five knots there, resulting in a linear behavior at the ends of the data range. This approach is helpful for data that has an approximately linear trend at the boundaries but exhibits non-linearity in the center. The knots in the cubic regression splines are placed by quantile through the variable space, so in the case where the data are evenly spread across the variable space the knots will be placed evenly. Figure 3.3: Cubic regression spline fit with knot positioning The gam routine creates \\(k-1\\) polynomials plus an intercept based on the knots: Figure 3.4: basis functions for cubic regression splines The polynomials transform the initial variable \\(x\\) and create a model matrix \\(X\\) with \\(k\\) columns and \\(n\\) rows, where \\(n\\) is the number of data points. This new transformation is being then used to fit the model and estimate the \\(\\beta_{0}, ... ,\\beta_{k-1}\\) coefficients. The fitted model results from \\(X\\beta\\) ggplot(data = df_xm_basis_b, aes(x = x, y = value, group = variable, colour = variable)) + geom_line() + geom_line(aes(y = fitted), color = &quot;darkgreen&quot;, linewidth = 1)+ geom_point(aes(y = obs), color = &quot;darkgrey&quot;, alpha = 0.4)+ theme(legend.position=&quot;bottom&quot;)+ylab(&quot;y&quot;) 3.4 Thin plate spline Thin plate splines are particularly useful for smoothing in multiple dimensions. However, they also work well with univariate data, as they offer flexibility and control over smoothness they are the default choice of the mgcv package. # Fit a thin plate spline with gam() tps_model &lt;- gam(y ~ s(x, k = 5, bs = &quot;tp&quot;), data = data) # Predict and plot data$tps_fit &lt;- predict(tps_model) ggplot(data, aes(x, y)) + geom_point(alpha = 0.4) + geom_line(aes(y = tps_fit), color = &quot;blue&quot;, linewidth = 1) Figure 3.5: Thin plate splines fit This spline automatically adjusts its flexibility across the data, with a smoothing penalty that controls the degree of bending. Thin plate splines are ideal when you need a smooth fit without predefined knots. The knots in thin plate splines are not positioned across the variable space, instead, by default, when no knot number is specified, there are as many knots as data points which essentially define the number of basis splines. In the case where \\(k\\) is specified the thin plate splines use the first \\(k\\) “principal components” in a similar way as the PCA algorithm works. Figure 3.6: basis functions for thin plate splines As in the example before, the fitted model results from \\(X\\beta\\) ggplot(data = df_xm_basis_b, aes(x = x, y = value, group = variable, colour = variable)) + geom_line() + geom_line(aes(y = fitted), color = &quot;darkgreen&quot;, linewidth = 1)+ geom_point(aes(y = obs), color = &quot;darkgrey&quot;, alpha = 0.4)+ theme(legend.position=&quot;bottom&quot;)+ylab(&quot;y&quot;)+ scale_y_continuous(limits = c(-1.5,1.5)) ## Warning: Removed 144 rows containing missing values or values outside the scale ## range (`geom_line()`). ## Warning: Removed 15 rows containing missing values or values outside the scale ## range (`geom_point()`). 3.5 Comparison Plotting together thin plate spline and cubic spline fits with the same number of knots. ggplot(data, aes(x, y)) + geom_point(alpha = 0.4) + geom_line(aes(y = tps_fit), color = &quot;blue&quot;, linewidth = 1) + geom_line(aes(y = cr_fit), color = &quot;darkgreen&quot;, linewidth = 1)+ labs(color = &quot;Spline Type&quot;) + scale_color_manual(values = c(&quot;blue&quot;, &quot;darkgreen&quot;)) Figure 3.7: Thin plate splines and cubic regression splines fit 3.6 The mgcv package inside a4a The mgcv package provides various user input options to define the smoother functions used to construct the submodels. Under the a4a framework, the mgcv package is used to construct the model matrices of the submodels, which are then passed to the ADMB where the model fitting takes place. The default option for the basis functions of the splines is bs = tp (thin plate splines) and is considered the optimal option. The user can define other smoothing basis functions using the bs argument. The user can refer to the smooth.terms of the mgcv package for a full description. There are many equivalent basis functions for the splines, and some of them have little or no effect in the context of a4a, since they differ only in the penalty term, which is not used in a4a. Examples for different smoothing terms: fmod00 &lt;- ~s(age)+s(year, bs = &#39;tp&#39;, k = 10) # thin plate splines fmod01 &lt;- ~s(age)+s(year, bs = &#39;cr&#39;, k = 10) # regression cubic splines fmod02 &lt;- ~s(age)+s(year, bs = &#39;bs&#39;, k = 10) # b-splines fmod03 &lt;- ~s(age)+s(year, bs = &#39;ps&#39;, k = 10) # p-splines fmod04 &lt;- ~s(age)+s(year, bs = &#39;ad&#39;, k = 10) # Adaptive smoothers fit00 &lt;- sca(stk, idx, fmodel = fmod00) fit01 &lt;- sca(stk, idx, fmodel = fmod01) fit02 &lt;- sca(stk, idx, fmodel = fmod02) fit03 &lt;- sca(stk, idx, fmodel = fmod03) fit04 &lt;- sca(stk, idx, fmodel = fmod04) In this example we are using the thin plate regression splines, cubic splines, b-splines, p-splines and adaptive smoothers plot(FLStocks(ThinPlate = stk + fit00, CubicRegressionSplines = stk + fit01, B_splines = stk + fit02, P_splines = stk + fit03, Adaptive_smoothers = stk + fit04 ))+theme(legend.position = &#39;bottom&#39;) Functionality of mgcv package provides also the option to work with interactions. Although s(age, year) can be defined, it uses a common bivariate spline for the two variables which are very different in scale. It is preferable if interactions are assumed to use te(age, year). Again is up to the user to define the basis functions for the smoothers. fmod03 &lt;- ~te(age, year, k = c(3,10)) fmod04 &lt;- ~te(age, year, k = c(3,10), bs = &#39;cr&#39;) fmod05 &lt;- ~te(age, year, bs = &#39;bs&#39;) fit03 &lt;- sca(stk, idx, fmodel = fmod03) fit04 &lt;- sca(stk, idx, fmodel = fmod04) fit05 &lt;- sca(stk, idx, fmodel = fmod05) plot(FLStocks(TP_tensor = stk + fit03, CB_tensor = stk + fit04, BS_tensor = stk + fit05))+theme(legend.position = &#39;bottom&#39;) 3.7 On the number of knots \\(k\\) \\(k\\) is the dimension of the basis used to represent the smooth term \\(s\\). The default depends on the number of variables that the smooth is a function of. In practice k-1 (or k) sets the upper limit on the degrees of freedom associated with an s smooth (1 degree of freedom is usually lost to the intercept of the smooth). For the smooths the upper limit of the degrees of freedom is given by the product of the k values provided for each marginal smooth less one, for the constraint. The choice of the k is not critical and in general it must be large enough to allow to have enough degrees of freedom capturing the underlying process and small enough to prevent overfitting. A strong pattern in the residuals can be a sign of low \\(k\\). fmod00 &lt;- ~s(age)+s(year, k = 5) fmod01 &lt;- ~s(age)+s(year, k = 10) # cubic splines fmod02 &lt;- ~s(age)+s(year, k = 20) # b-splines fit00 &lt;- sca(stk, idx, fmodel = fmod00) fit01 &lt;- sca(stk, idx, fmodel = fmod01) fit02 &lt;- sca(stk, idx, fmodel = fmod02) plot(FLStocks(&#39;k=5&#39; = stk + fit00, &#39;k=10&#39; = stk + fit01, &#39;k=20&#39; = stk + fit02))+theme(legend.position = &#39;bottom&#39;) We can check the result of choosing different \\(k\\) values on BIC and GCV: fmodsk &lt;- list() for(i in 10:20) { fmodsk[[paste0(i)]] &lt;- as.formula(paste0(&quot;~s(age)+s(year, k=&quot;,i,&quot;)&quot;)) } myFits &lt;- FLa4a:::multisca(FLStocks(stk), list(idx), fmodel = fmodsk) plot(myFits) "],["modelling-individual-growth-and-using-stochastic-slicing-to-convert-length-based-data-into-age-based-data.html", "Chapter 4 Modelling Individual Growth and Using Stochastic Slicing to Convert Length-based Data Into Age-based Data 4.1 Background 4.2 a4aGr - The growth class 4.3 Adding uncertainty to growth parameters with a multivariate normal distribution} 4.4 Adding uncertainty to growth parameters with a multivariate triangle distribution} 4.5 Adding uncertainty to growth parameters with statistical copulas} 4.6 Converting from length to age based data - the l2a() method}", " Chapter 4 Modelling Individual Growth and Using Stochastic Slicing to Convert Length-based Data Into Age-based Data The document explains the approach being developed by a4a to integrate uncertainty in individual growth into stock assessment and advice. It presents a mixture of text and code, where the first explains the concepts behind the methods, while the last shows how these can be run with the software provided.} 4.1 Background The a4a stock assessment framework is based on age dynamics. Therefore, to use length information it must be processed before it can be used in an assessment. The rationale is that the processing should give the analyst the flexibility to use a range of sources of information, literature or online databases, to grab information about the species growth model and the uncertainty about the model parameters. Within the a4a framework this is handled using the a4aGr class. In this section we introduce the a4aGr class and look at the variety of ways that parameter uncertainty can be included. 4.2 a4aGr - The growth class The conversion of length data to age is performed through the use of a growth model. The implementation is done through the a4aGr class. showClass(&quot;a4aGr&quot;) ## Class &quot;a4aGr&quot; [package &quot;FLa4a&quot;] ## ## Slots: ## ## Name: grMod grInvMod params vcov distr name desc ## Class: formula formula FLPar array character character character ## ## Name: range ## Class: numeric ## ## Extends: &quot;FLComp&quot; To construct an a4aGr object, the growth model and parameters must be provided. Check the help file for more information. Here we show an example using the von Bertalanffy growth model. To create the a4aGr object it’s necessary to pass the model equation (\\(length \\sim time\\)), the inverse model equation (\\(time \\sim length\\)) and the parameters. Any growth model can be used as long as it’s possible to write the model (and the inverse) as an R formula. vbObj &lt;- a4aGr( grMod=~linf*(1-exp(-k*(t-t0))), grInvMod=~t0-1/k*log(1-len/linf), params=FLPar(linf=58.5, k=0.086, t0=0.001, units=c(&quot;cm&quot;,&quot;year-1&quot;,&quot;year&quot;)) ) # Check the model and its inverse lc=20 predict(vbObj, len=lc) ## iter ## 1 ## 1 4.86575 predict(vbObj, t=predict(vbObj, len=lc))==lc ## iter ## 1 ## 1 TRUE The predict method allows the transformation between age and lengths using the growth model. predict(vbObj, len=5:10+0.5) ## iter ## 1 ## 1 1.149080 ## 2 1.370570 ## 3 1.596362 ## 4 1.826625 ## 5 2.061540 ## 6 2.301299 predict(vbObj, t=5:10+0.5) ## iter ## 1 ## 1 22.04376 ## 2 25.04796 ## 3 27.80460 ## 4 30.33408 ## 5 32.65511 ## 6 34.78488 4.3 Adding uncertainty to growth parameters with a multivariate normal distribution} Uncertainty in the growth model is introduced through the inclusion of parameter uncertainty. This is done by making use of the parameter variance-covariance matrix (the vcov slot of the a4aGr class) and assuming a distribution. The numbers in the variance-covariance matrix could come from the parameter uncertainty from fitting the growth model parameters. Here we set the variance-covariance matrix by scaling a correlation matrix, using a cv of 0.2. Based on \\[\\rho_{x,y}=\\frac{\\Sigma_{x,y}}{\\sigma_x \\sigma_y}\\] and \\[CV_x=\\frac{\\sigma_x}{\\mu_x}\\] # Make an empty cor matrix cm &lt;- diag(c(1,1,1)) # k and linf are negatively correlated while t0 is independent cm[1,2] &lt;- cm[2,1] &lt;- -0.5 # scale cor to var using CV=0.2 cv &lt;- 0.2 p &lt;- c(linf=60, k=0.09, t0=-0.01) vc &lt;- matrix(1, ncol=3, nrow=3) l &lt;- vc l[1,] &lt;- l[,1] &lt;- p[1]*cv k &lt;- vc k[,2] &lt;- k[2,] &lt;- p[2]*cv t &lt;- vc t[3,] &lt;- t[,3] &lt;- p[3]*cv mm &lt;- t*k*l diag(mm) &lt;- diag(mm)^2 mm &lt;- mm*cm # check that we have the intended correlation all.equal(cm, cov2cor(mm)) ## [1] TRUE Create the a4aGr object as before but now we also include the vcov argument for the variance-covariance matrix. vbObj &lt;- a4aGr(grMod=~linf*(1-exp(-k*(t-t0))), grInvMod=~t0-1/k*log(1-len/linf), params=FLPar(linf=p[&quot;linf&quot;], k=p[&quot;k&quot;], t0=p[&quot;t0&quot;], units=c(&quot;cm&quot;,&quot;year-1&quot;,&quot;year&quot;)), vcov=mm) First we show a simple example where we assume that the parameters are represented using a multivariate normal distribution. % This covariance matrix can have iterations (i.e. each iteration can have a different covariance matrix). CHECK % If the parameters or the covariance matrix have iterations then the medians accross iterations are computed before simulating. Check help for `mvrnorm} for more information. # Note that the object we have just created has a single iteration of each parameter vbObj@params ## An object of class &quot;FLPar&quot; ## params ## linf k t0 ## 60.00 0.09 -0.01 ## units: cm year-1 year dim(vbObj@params) ## [1] 3 1 # We simulate 10000 iterations from the a4aGr object by calling mvrnorm() using the # variance-covariance matrix we created earlier. vbNorm &lt;- mvrnorm(10000,vbObj) # Now we have 10000 iterations of each parameter, randomly sampled from the # multivariate normal distribution vbNorm@params ## An object of class &quot;FLPar&quot; ## iters: 10000 ## ## params ## linf k t0 ## 59.916656(11.92748) 0.089882( 0.01807) -0.010012( 0.00199) ## units: cm year-1 year dim(vbNorm@params) ## [1] 3 10000 We can now convert from length to ages data based on the 10000 parameter iterations. This gives us 10000 sets of age data. For example, here we convert a single length vector using each of the 10000 parameter iterations: ages &lt;- predict(vbNorm, len=5:10+0.5) dim(ages) ## [1] 6 10000 # We show the first ten iterations only as an illustration ages[,1:10] ## iter ## 1 2 3 4 5 6 7 8 ## 1 0.6068397 1.488213 1.037752 1.071041 0.9280267 1.265254 1.120248 0.8069418 ## 2 0.7252006 1.777696 1.240002 1.283388 1.1090665 1.515249 1.337920 0.9642726 ## 3 0.8454734 2.073037 1.446052 1.501059 1.2934463 1.771633 1.559588 1.1246907 ## 4 0.9677209 2.374480 1.656049 1.724328 1.4812919 2.034741 1.785402 1.2883195 ## 5 1.0920089 2.682281 1.870146 1.953490 1.6727359 2.304935 2.015520 1.4552903 ## 6 1.2184068 2.996714 2.088508 2.188865 1.8679190 2.582607 2.250109 1.6257423 ## iter ## 9 10 ## 1 0.7367531 1.350818 ## 2 0.8779795 1.612006 ## 3 1.0210482 1.877729 ## 4 1.1660079 2.148148 ## 5 1.3129092 2.423431 ## 6 1.4618049 2.703757 The marginal distributions can be seen in Figure @ref(fig:plot_norm_params). (#fig:plot_norm_params)The marginal distributions of each of the parameters from using a multivariate normal distribution. The shape of the correlation can be seen in Figure @ref(fig:plot_norm_scatter). (#fig:plot_norm_scatter)Scatter plot of the 10000 samples parameter from the multivariate normal distribution. Growth curves for the 1000 iterations can be seen in Figure @ref(fig:plot_mv_growth). (#fig:plot_mv_growth)Growth curves using parameters simulated from a multivariate normal distribution. 4.4 Adding uncertainty to growth parameters with a multivariate triangle distribution} One alternative to using a normal distribution is to use a triangle distribution. We use the package triangle where this distribution is parametrized using the minimum, maximum and median values. This can be very attractive if the analyst needs to scrape information from the web or literature and perform some kind of meta-analysis. Here we show an example of setting a triangle distribution with values taken from Fishbase. # The web address for the growth parameters for redfish (Sebastes norvegicus) addr &lt;- &quot;https://fishbase.se/PopDyn/PopGrowthList.php?ID=501&quot; # Scrape the data tab &lt;- try(readHTMLTable(addr)) ## Error : XML content does not seem to be XML: &#39;&#39; # Load local copy if no web if(is(tab, &quot;try-error&quot;)) load(&quot;data/tab.RData&quot;) # Interrogate the data table and get vectors of the values linf &lt;- as.numeric(as.character(tab$dataTable[,2])) k &lt;- as.numeric(as.character(tab$dataTable[,4])) t0 &lt;- as.numeric(as.character(tab$dataTable[,5])) # Set the min (a), max (b) and median (c) values for the parameter as a list of lists # Note that t0 has no &#39;c&#39; (median) value. This makes the distribution symmetrical triPars &lt;- list(list(a=min(linf), b=max(linf), c=median(linf)), list(a=min(k), b=max(k), c=median(k)), list(a=median(t0, na.rm=T)-IQR(t0, na.rm=T)/2, b=median(t0, na.rm=T)+ IQR(t0, na.rm=T)/2)) # Simulate 10000 times using mvrtriangle vbTri &lt;- mvrtriangle(10000, vbObj, paramMargins=triPars) The marginals will reflect the uncertainty on the parameter values that were scraped from Fishbase but, as we don’t really believe the parameters are multivariate normal, here we adopted a distribution based on a t copula with triangle marginals. The marginal distributions can be seen in Figure @ref(fig:plot_tri_params) and the shape of the correlation can be seen in Figure @ref(fig:plot_tri_scatter). (#fig:plot_tri_params)The marginal distributions of each of the parameters from using a multivariate triangle distribution. (#fig:plot_tri_scatter)Scatter plot of the 10000 samples parameter from the multivariate triangle distribution. We can still use predict() to see the growth model uncertainty (Figure @ref(fig:plot_tri_growth)). (#fig:plot_tri_growth)Growth curves using parameters simulated from a multivariate triangle distribution. Remember that the above examples use a variance-covariance matrix that we essentially made up. An alternative would be to scrape the entire growth parameters dataset from Fishbase and compute the shape of the variance-covariance matrix yourself. 4.5 Adding uncertainty to growth parameters with statistical copulas} A more general approach to adding parameter uncertainty is to make use of statistical copulas and marginal distributions of choice. This is possible with the mvrcop() function borrowed from the package copula. The example below keeps the same parameters and changes only the copula type and family but a lot more can be done. Check the package copula for more information. % Needs more explanation vbCop &lt;- mvrcop(10000, vbObj, copula=&quot;archmCopula&quot;, family=&quot;clayton&quot;, param=2, margins=&quot;triangle&quot;, paramMargins=triPars) The shape of the correlation changes (Figure @ref(fig:plot_cop_tri_scatter)) as well as the resulting growth curves (Figure @ref(fig:plot_cop_tri_growth)). (#fig:plot_cop_tri_scatter)Scatter plot of the 10000 samples parameter from the using an archmCopula copula with triangle margins. (#fig:plot_cop_tri_growth)Growth curves from the using an archmCopula copula with triangle margins. 4.6 Converting from length to age based data - the l2a() method} After introducing uncertainty in the growth model through the parameters it’s time to transform the length-based dataset into an age-based dataset. The method that deals with this process is l2a(). The implementation of this method for the FLQuant class is the main workhorse. There are two other implementations, for the FLStock and FLIndex classes, which are mainly wrappers that call the FLQuant method several times. When converting from length-based data to age-based data you need to be aware of how the aggregation of length classes is performed. For example, individuals in length classes 1-2, 2-3, and 3-4 cm may all be considered as being of age 1 (obviously depending on the growth model). How should the values in those length classes be combined? If the values are abundances then the values should be summed. Summing other types of values, such as mean weight, does not make sense. Instead these values are averaged over the length classes (possibly weighted by the abundance). This is controlled using the stat argument which can be either mean or sum (the default). Fishing mortality is not computed to avoid making wrong assumptions about the meaning of F at length. We demonstrate the method by converting a catch-at-length FLQuant to a catch-at-age FLQuant. First we make an a4aGr object with a multivariate triangle distribution (using the parameters we set above). We use 10 iterations as an example. And call l2a() by passing in the length-based FLQuant and the a4aGr object. vbTriSmall &lt;- mvrtriangle(10, vbObj, paramMargins=triPars) cth.n &lt;- l2a(catch.n(rfLen.stk), vbTriSmall) ## Warning in log(1 - len/linf): NaNs produced ## Warning in log(1 - len/linf): NaNs produced ## Warning in log(1 - len/linf): NaNs produced ## Warning in log(1 - len/linf): NaNs produced ## Warning in log(1 - len/linf): NaNs produced ## Warning in log(1 - len/linf): NaNs produced ## Warning in log(1 - len/linf): NaNs produced ## Warning in log(1 - len/linf): NaNs produced ## Warning in log(1 - len/linf): NaNs produced dim(cth.n) ## [1] 60 26 1 4 1 10 In the previous example, the FLQuant object that was sliced (catch.n(rfLen.stk)) had only one iteration. This iteration was sliced by each of the iterations in the growth model. It is possible for the FLQuant object to have the same number of iterations as the growth model, in which case each iteration of the FLQuant and the growth model are used together. It is also possible for the growth model to have only one iteration while the FLQuant object has many iterations. The same growth model is then used for each of the FLQuant iterations. As with all FLR objects, the general rule is one or n iterations. As well as converting one FLQuant at a time, we can convert entire FLStock and FLIndex objects. In these cases the individual FLQuant slots of those classes are converted from length-based to age-based. As mentioned above, the aggregation method depends on the type of values the slots contain. The abundance slots (*.n, such as stock.n) are summed. The *.wt, m, mat, harvest.spwn and m.spwn slots of an FLStock object are averaged. The catch.wt and sel.pattern slots of an FLIndex object are averaged, while the index, index.var and catch.n slots are summed. The method for FLStock classes takes an additional argument for the plusgroup. aStk &lt;- l2a(rfLen.stk, vbTriSmall, plusgroup=14) ## Warning in .local(object, model, ...): Individual weights, M and maturity will be (weighted) averaged accross lengths, ## harvest is not computed and everything else will be summed. ## If this is not what you want, you&#39;ll have to deal with these slots by hand. ## Warning in .local(object, model, ...): Some ages are less than 0, indicating a mismatch between input data lengths ## and growth parameters (possibly t0) ## Warning in .local(object, model, ...): Trimming age range to a minimum of 0 ## [1] &quot;maxfbar has been changed to accomodate new plusgroup&quot; aIdx &lt;- l2a(rfTrawl.idx, vbTriSmall) ## Warning in l2a(rfTrawl.idx, vbTriSmall): Some ages are less than 0, indicating a mismatch between input data lengths ## and growth parameters (possibly t0) ## Warning in l2a(rfTrawl.idx, vbTriSmall): Trimming age range to a minimum of 0 When converting with l2a() all lengths above Linf are converted to the maximum age, as there is no information in the growth model about how to deal with individuals larger than Linf. "],["natural-mortality-modelling.html", "Chapter 5 Natural Mortality Modelling 5.1 Background 5.2 a4aM - The M class 5.3 Adding uncertainty to natural mortality parameters with a multivariate normal distribution 5.4 Adding uncertainty to natural mortality parameters with statistical copulas 5.5 Computing natural mortality time series - the “m” method", " Chapter 5 Natural Mortality Modelling The document explains the approach being developed by a4a to integrate uncertainty in natural mortality into stock assessment and advice. It presents a mixture of text and code, where the first explains the concepts behind the methods, while the last shows how these can be run with the software provided. 5.1 Background In the a4a natural mortality is dealt with as an external parameter to the stock assessment model. The rationale to modelling natural mortality is similar to that of growth: one should be able to grab information from a range of sources and feed it into the assessment. The mechanism used by a4a is to build an interface that makes it transparent, flexible and hopefully easy to explore different options. In relation to natural mortality it means that the analyst should be able to use distinct models like Gislasson’s, Charnov’s, Pauly’s, etc in a coherent framework making it possible to compare the outcomes of the assessment. Within the a4a framework, the general method for inserting natural mortality in the stock assessment is to: Create an object of class a4aM which holds the natural mortality model and parameters. Add uncertainty to the parameters in the a4aM object. Apply the m() method to the a4aM object to create an age or length based FLQuant object of the required dimensions. The resulting FLQuant object can then be directly inserted into an FLStock object to be used for the assessment. In this section we go through each of the steps in detail using a variety of different models. 5.2 a4aM - The M class Natural mortality is implemented in a class named a4aM. This class is made up of three objects of the class FLModelSim. Each object is a model that represents one effect: an age or length effect, a scaling (level) effect and a time trend, named shape, level and trend, respectively. The impact of the models is multiplicative, i.e. the overal natural mortality is given by shape x level x trend. Check the help files for more information. showClass(&quot;a4aM&quot;) ## Class &quot;a4aM&quot; [package &quot;FLa4a&quot;] ## ## Slots: ## ## Name: shape level trend name desc range ## Class: FLModelSim FLModelSim FLModelSim character character numeric ## ## Extends: &quot;FLComp&quot; The a4aM constructor requires that the models and parameters are provided. The default method will build each of these models as a constant value of 1. As a simple example, the usual “0.2” guessestimate could be set up by setting the level model to have a single parameter with a fixed value, while the other two models, shape and trend, have a default value of 1 (meaning that they have no effect). mod02 &lt;- FLModelSim(model=~a, params=FLPar(a=0.2)) m1 &lt;- a4aM(level=mod02) m1 ## a4aM object: ## shape: ~1 ## level: ~a ## trend: ~1 More interesting natural mortality shapes can be set up using biological knowledge. The following example uses an exponential decay over ages (implying that the resulting FLQuant} generated by the m() method will be age based). We also use Jensen’s second estimator (Kenchington, 2014) as a scaling level model, which is based on the von Bertalanffy \\(K\\) parameter, \\(M=1.5K\\). shape2 &lt;- FLModelSim(model=~exp(-age-0.5)) level2 &lt;- FLModelSim(model=~1.5*k, params=FLPar(k=0.4)) m2 &lt;- a4aM(shape=shape2, level=level2) m2 ## a4aM object: ## shape: ~exp(-age - 0.5) ## level: ~1.5 * k ## trend: ~1 Note that the shape model has age as a parameter of the model but is not set using the params argument. The shape model does not have to be age-based. For example, here we set up a shape model using Gislason’s second estimator (Kenshington, 2013): \\(M_l=K(\\frac{L_{\\inf}}{l})^{1.5}\\). We use the default level and trend models. % Current m() method is not ideal for length based methods as you cannot specify length range and half-widths to make compatible with FLStockLen shape_len &lt;- FLModelSim(model=~K*(linf/len)^1.5, params=FLPar(linf=60, K=0.4)) m_len &lt;- a4aM(shape=shape_len) Another option is to model how an external factor may impact the natural mortality. This can be added through the trend model. Suppose natural mortality can be modelled with a dependency on the NAO index, due to some mechanism that results in having lower mortality when NAO is negative and higher when it’s positive. In this example, the impact is represented by the NAO value on the quarter before spawning, which occurs in the second quarter. We use this to make a complicated natural mortality model with an age based shape model, a level model based on \\(K\\) and a trend model driven by NAO, where mortality increases by 50% if NAO is positive on the first quarter. # Get NAO nao &lt;- read.table(&quot;https://www.cpc.ncep.noaa.gov/products/precip/CWlink/pna/norm.nao.monthly.b5001.current.ascii.table&quot;, skip=1, fill=TRUE, na.strings=&quot;-99.90&quot;) dnms &lt;- list(quant=&quot;nao&quot;, year=1950:2024, unit=&quot;unique&quot;, season=1:12, area=&quot;unique&quot;) # Build an FLQuant from the NAO data nao.flq &lt;- FLQuant(unlist(nao[,-1]), dimnames=dnms, units=&quot;nao&quot;) # Build covar by calculating mean over the first 3 months nao &lt;- seasonMeans(trim(nao.flq, year=dimnames(stock.n(ple4))$year)) # Turn into Boolean nao &lt;- (nao&gt;0) # Constructor trend3 &lt;- FLModelSim(model=~1+b*nao, params=FLPar(b=0.5)) shape3 &lt;- FLModelSim(model=~exp(-age-0.5)) level3 &lt;- FLModelSim(model=~1.5*k, params=FLPar(k=0.4)) m3 &lt;- a4aM(shape=shape3, level=level3, trend=trend3) m3 ## a4aM object: ## shape: ~exp(-age - 0.5) ## level: ~1.5 * k ## trend: ~1 + b * nao 5.3 Adding uncertainty to natural mortality parameters with a multivariate normal distribution Uncertainty on natural mortality is added through uncertainty on the parameters. In this section we’ll’ show how to add multivariate normal uncertainty. We make use of the class FLModelSim method mvrnorm(), which is a wrapper for the method mvrnorm() distributed by the package MASS. We’ll create an a4aM object with an exponential shape, a level model based on \\(k\\) and temperature (Jensen’s third estimator), and a trend model driven by the NAO (as above). Afterwards a variance-covariance matrix for the level and trend models will be included. Finally, create an object with 100 iterations using the mvrnorm() method. Create the object: shape4 &lt;- FLModelSim(model=~exp(-age-0.5)) level4 &lt;- FLModelSim(model=~k^0.66*t^0.57, params=FLPar(k=0.4, t=10), vcov=array(c(0.002, 0.01,0.01, 1), dim=c(2,2))) trend4 &lt;- FLModelSim(model=~1+b*nao, params=FLPar(b=0.5), vcov=matrix(0.02)) m4 &lt;- a4aM(shape=shape4, level=level4, trend=trend4) # Call mvrnorm() m4 &lt;- mvrnorm(100, m4) m4 ## a4aM object: ## shape: ~exp(-age - 0.5) ## level: ~k^0.66 * t^0.57 ## trend: ~1 + b * nao Inspect the level model (for example): m4@level ## An object of class &quot;FLModelSim&quot; ## Slot &quot;model&quot;: ## ~k^0.66 * t^0.57 ## ## Slot &quot;params&quot;: ## An object of class &quot;FLPar&quot; ## iters: 100 ## ## params ## k t ## 0.40367(0.0362) 10.19582(1.0405) ## units: NA ## ## Slot &quot;vcov&quot;: ## [,1] [,2] ## [1,] 0.002 0.01 ## [2,] 0.010 1.00 ## ## Slot &quot;distr&quot;: ## [1] &quot;norm&quot; Note the variance in the parameters: params(trend(m4)) ## An object of class &quot;FLPar&quot; ## iters: 100 ## ## params ## b ## 0.50079(0.148) ## units: NA Note the shape model has no parameters and no uncertainty: params(shape(m4)) ## An object of class &quot;FLPar&quot; ## params ## ## NA ## units: NA In this particular case, the shape model will not be randomized because it doesn’t have a variance-covariance matrix. Also note that because there is only one parameter in the `trend} model, the randomization will use a univariate normal distribution. The same model could be achieved by using mnrnorm() on each model component: m4 &lt;- a4aM(shape=shape4, level=mvrnorm(100, level4), trend=mvrnorm(100, trend4)) %Note: How to include ageing error ??? 5.4 Adding uncertainty to natural mortality parameters with statistical copulas We can also use copulas to add parameter uncertainty to the natural mortality model, similar to the way we use them for the growth model in Section \\(\\ref{sec:growth_triangle_cop}\\). As stated above these processes make use of the methods implemented for the FLModelSim class. % EXPAND… In the following example we’ll use again Gislason’s second estimator, \\(M_l=K(\\frac{L_{\\inf}}{l})^{1.5}\\) and a triangle copula to model parameter uncertainty. The method mvrtriangle() is used to create 1000 iterations. linf &lt;- 60 k &lt;- 0.4 # vcov matrix (make up some values) mm &lt;- matrix(NA, ncol=2, nrow=2) # 10% cv diag(mm) &lt;- c((linf*0.1)^2, (k*0.1)^2) # 0.2 correlation mm[upper.tri(mm)] &lt;- mm[lower.tri(mm)] &lt;- c(0.05) # a good way to check is using cov2cor cov2cor(mm) ## [,1] [,2] ## [1,] 1.0000000 0.2083333 ## [2,] 0.2083333 1.0000000 # create object mgis2 &lt;- FLModelSim(model=~k*(linf/len)^1.5, params=FLPar(linf=linf, k=k), vcov=mm) # set the lower, upper and (optionally) centre of the parameters (without the centre, the triangle is symmetrical) pars &lt;- list(list(a=55,b=65), list(a=0.3, b=0.6, c=0.35)) mgis2 &lt;- mvrtriangle(1000, mgis2, paramMargins=pars) mgis2 ## An object of class &quot;FLModelSim&quot; ## Slot &quot;model&quot;: ## ~k * (linf/len)^1.5 ## ## Slot &quot;params&quot;: ## An object of class &quot;FLPar&quot; ## iters: 1000 ## ## params ## linf k ## 59.99490(2.0496) 0.40664(0.0738) ## units: NA ## ## Slot &quot;vcov&quot;: ## [,1] [,2] ## [1,] 36.00 0.0500 ## [2,] 0.05 0.0016 ## ## Slot &quot;distr&quot;: ## [1] &quot;un &lt;deprecated slot&gt; triangle&quot; The resulting parameter estimates and marginal distributions can be seen in Figure @ref(fig:plot_tri_gis_m) and @ref(fig:plot_tri_gis_m_hist). (#fig:plot_tri_gis_m)Parameter estimates for Gislason’s second natural mortality model from using a triangle distribution. (#fig:plot_tri_gis_m_hist)Marginal distributions of the parameters for Gislason’s second natural mortality model using a triangle distribution. We now have a new model that can be used for the shape model. You can use the constructor or the set method to add the new model. Note that we have a quite complex method now for M. A length based shape} model from Gislason's work, Jensen's third model based on temperatureleveland a timetrend` depending on NAO. All of the component models have uncertainty in their parameters. m5 &lt;- a4aM(shape=mgis2, level=level4, trend=trend4) # or m5 &lt;- m4 shape(m5) &lt;- mgis2 5.5 Computing natural mortality time series - the “m” method Now that we have set up the natural mortality a4aM model and added parameter uncertainty to each component, we are ready to generate the FLQuant of natural mortality. For that we need the m() method. The m() method is the workhorse method for computing natural mortality. The method returns an FLQuant that can be inserted in an FLStock for usage by the assessment method. %The method uses the range slot to work out the dimensions of the FLQuant object. % Future developments will also allow for easy insertion into FLStockLen objects. The size of the FLQuant object is determined by the min, max, minyear and maxyear elements of the range slot of the a4aM object. By default the values of these elements are set to 0. Giving an FLQuant with length 1 in the quant and year dimension. The range slot can be set by hand, or by using the rngquant() and rngyear() methods. The name of the first dimension of the output FLQuant (e.g. ‘age’ or ‘len’) is determined by the parameters of the shape model. If it is not clear what the name should be then the name is set to ‘quant’. Here we demonstrate m() using the simple a4aM object we created above that has constant natural mortality. Start with the simplest model: m1 ## a4aM object: ## shape: ~1 ## level: ~a ## trend: ~1 Check the range: range(m1) ## min max plusgroup minyear maxyear minmbar maxmbar ## 0 0 0 0 0 0 0 Simple - no ages or years: m(m1) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## quant 0 ## 0 0.2 ## ## units: NA Set the quant and year ranges: range(m1, c(&quot;min&quot;,&quot;max&quot;)) &lt;- c(0,7) # set the quant range range(m1, c(&quot;minyear&quot;,&quot;maxyear&quot;)) &lt;- c(2000, 2010) # set the year range range(m1) ## min max plusgroup minyear maxyear minmbar maxmbar ## 0 7 0 2000 2010 0 0 Create the object with the M estimates by age and year, note the name of the first dimension is ‘quant’. m(m1) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## quant 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 ## 0 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## 1 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## 2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## 3 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## 4 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## 5 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## 6 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## 7 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## ## units: NA The next example has an age-based shape. As the shape model has ‘age’ as a variable which is not included in the FLPar slot it is used as the name of the first dimension of the resulting FLQuant. Note that in this case the mbar values in the range become relevant, once that mbar is used to compute the mean level. This mean level will match the value given by the level model. The mbar range can be changed with the rngmbar() method. We illustrate this by making an FLQuant with age varying natural mortality. Check the model and set the ranges: m2 ## a4aM object: ## shape: ~exp(-age - 0.5) ## level: ~1.5 * k ## trend: ~1 range(m2, c(&quot;min&quot;,&quot;max&quot;)) &lt;- c(0,7) # set the quant range range(m2, c(&quot;minyear&quot;,&quot;maxyear&quot;)) &lt;- c(2000, 2003) # set the year range range(m2) ## min max plusgroup minyear maxyear minmbar maxmbar ## 0 7 0 2000 2003 0 0 m(m2) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 2000 2001 2002 2003 ## 0 0.60000000 0.60000000 0.60000000 0.60000000 ## 1 0.22072766 0.22072766 0.22072766 0.22072766 ## 2 0.08120117 0.08120117 0.08120117 0.08120117 ## 3 0.02987224 0.02987224 0.02987224 0.02987224 ## 4 0.01098938 0.01098938 0.01098938 0.01098938 ## 5 0.00404277 0.00404277 0.00404277 0.00404277 ## 6 0.00148725 0.00148725 0.00148725 0.00148725 ## 7 0.00054713 0.00054713 0.00054713 0.00054713 ## ## units: NA Note that the level value is: predict(level(m2)) ## iter ## 1 ## 1 0.6 Which is the same as: m(m2)[&quot;0&quot;] ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 2000 2001 2002 2003 ## 0 0.6 0.6 0.6 0.6 ## ## units: NA This is because the mbar range is currently set to “0” and “0” (see above) and the mean natural mortality value over this range is given by the level model. We can change the mbar range: range(m2, c(&quot;minmbar&quot;,&quot;maxmbar&quot;)) &lt;- c(0,5) range(m2) ## min max plusgroup minyear maxyear minmbar maxmbar ## 0 7 0 2000 2003 0 5 Which rescales the the natural mortality at age: m(m2) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 2000 2001 2002 2003 ## 0 2.2812888 2.2812888 2.2812888 2.2812888 ## 1 0.8392392 0.8392392 0.8392392 0.8392392 ## 2 0.3087389 0.3087389 0.3087389 0.3087389 ## 3 0.1135787 0.1135787 0.1135787 0.1135787 ## 4 0.0417833 0.0417833 0.0417833 0.0417833 ## 5 0.0153712 0.0153712 0.0153712 0.0153712 ## 6 0.0056547 0.0056547 0.0056547 0.0056547 ## 7 0.0020803 0.0020803 0.0020803 0.0020803 ## ## units: NA Check that the mortality over the mean range is the same as the level model: quantMeans(m(m2)[as.character(0:5)]) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 2000 2001 2002 2003 ## all 0.6 0.6 0.6 0.6 ## ## units: NA The next example uses a time trend for the trend model. We use the m3 model we made earlier. The trend model for this model has a covariate, ‘nao’. This needs to be passed to the m() method. The year range of the ‘nao’ covariate should match that of the range slot. Simple, pass in a single nao value (only one year): m(m3, nao=1) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 0 ## 0 0.9 ## ## units: NA Set ages: range(m3, c(&quot;min&quot;,&quot;max&quot;)) &lt;- c(0,7) m(m3, nao=0) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 0 ## 0 0.60000000 ## 1 0.22072766 ## 2 0.08120117 ## 3 0.02987224 ## 4 0.01098938 ## 5 0.00404277 ## 6 0.00148725 ## 7 0.00054713 ## ## units: NA With ages and years - passing in the NAO data as numeric (1,0,1,0) range(m3, c(&quot;minyear&quot;,&quot;maxyear&quot;)) &lt;- c(2000, 2003) m(m3, nao=as.numeric(nao[,as.character(2000:2003)])) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 2000 2001 2002 2003 ## 0 0.90000000 0.60000000 0.90000000 0.90000000 ## 1 0.33109150 0.22072766 0.33109150 0.33109150 ## 2 0.12180175 0.08120117 0.12180175 0.12180175 ## 3 0.04480836 0.02987224 0.04480836 0.04480836 ## 4 0.01648407 0.01098938 0.01648407 0.01648407 ## 5 0.00606415 0.00404277 0.00606415 0.00606415 ## 6 0.00223088 0.00148725 0.00223088 0.00223088 ## 7 0.00082069 0.00054713 0.00082069 0.00082069 ## ## units: NA The final example show how m() can be used to make an FLQuant with uncertainty (see Figure @ref(fig:uncertain_m)). We use the m4 object from earlier with uncertainty on the level and trend parameters. range(m4, c(&quot;min&quot;,&quot;max&quot;)) &lt;- c(0,7) range(m4, c(&quot;minyear&quot;,&quot;maxyear&quot;)) &lt;- c(2000, 2003) flq &lt;- m(m4, nao=as.numeric(nao[,as.character(2000:2003)])) flq ## An object of class &quot;FLQuant&quot; ## iters: 100 ## ## , , unit = unique, season = all, area = unique ## ## year ## age 2000 2001 2002 ## 0 3.1327180(0.425283) 2.0846727(0.204838) 3.1327180(0.425283) ## 1 1.1524625(0.156453) 0.7669082(0.075356) 1.1524625(0.156453) ## 2 0.4239673(0.057556) 0.2821298(0.027722) 0.4239673(0.057556) ## 3 0.1559688(0.021174) 0.1037897(0.010198) 0.1559688(0.021174) ## 4 0.0573777(0.007789) 0.0381821(0.003752) 0.0573777(0.007789) ## 5 0.0211081(0.002866) 0.0140464(0.001380) 0.0211081(0.002866) ## 6 0.0077652(0.001054) 0.0051674(0.000508) 0.0077652(0.001054) ## 7 0.0028567(0.000388) 0.0019010(0.000187) 0.0028567(0.000388) ## year ## age 2003 ## 0 3.1327180(0.425283) ## 1 1.1524625(0.156453) ## 2 0.4239673(0.057556) ## 3 0.1559688(0.021174) ## 4 0.0573777(0.007789) ## 5 0.0211081(0.002866) ## 6 0.0077652(0.001054) ## 7 0.0028567(0.000388) ## ## units: NA dim(flq) ## [1] 8 4 1 1 1 100 (#fig:uncertain_m)Natural mortality with age and year trend. "],["stock-assessment-framework.html", "Chapter 6 Stock assessment framework 6.1 Maths description 6.2 Classes description ", " Chapter 6 Stock assessment framework 6.1 Maths description 6.1.1 Observations The stock assessment model behind the framework is based on two types of observations: catches, \\(\\hat{C}\\), and abundance indices, \\(\\hat{I}\\). The model predicts catches at age \\(C_{ay}\\) and indices of abundance \\(I_{ays}\\) for each age \\(a\\), year \\(y\\) and survey \\(s\\) in the input dataset. It is assumed that the observed catches are normally distributed about the model predictions on the log scale with some observation variance \\(\\sigma^2_{ay}\\). that is: \\[\\log \\hat{C}_{ay} \\sim \\text{Normal} \\Big( \\log C_{ay}, \\sigma^2_{ay}\\Big)\\] likewise, the observed survey indices are assumed to be normally distributed about the model predictions on the log scale with some observation variance \\(\\tau^2_{ays}\\): \\[\\log \\hat{I}_{ays} \\sim \\text{Normal} \\Big( \\log I_{ays}, \\tau^2_{ays} \\Big)\\] This leads to the definition of the log-likelihood of the observed catches \\[\\ell_C = \\sum_{ay} w^{(c)}_{ay}\\ \\ell_N \\Big( \\log C_{ay}, \\sigma^2_{ay} ;\\ \\log \\hat{C}_{ay} \\Big)\\] and the log-likelihood of the observed survey indices \\[\\ell_I = \\sum_s \\sum_{ay} w^{(s)}_{ays}\\ \\ell_N \\Big( \\log I_{ays}, \\tau_{ays}^2 ;\\ \\log \\hat{I}_{ays} \\Big)\\] where \\(\\ell_N\\) is the normal log-likelihood function and \\(w^{(c)}_{ay}\\) and \\(w^{(s)}_{ays}\\) are optional weights for the catch and index observations, respectively. The total log-likelihood is then \\[\\ell = \\ell_C + \\ell_I\\] 6.1.2 The population dynamics model To predict catches and survey indices, the model uses the standard population dynamics model \\[N_{a+1,y+1} = N_{ay} \\exp \\left( - F_{ay} - M_{ay} \\right)\\] where \\(N_{ay}\\) is the number of individuals at age \\(a\\) in year \\(y\\), \\(F_{ay}\\) is the fishing mortality at age \\(a\\) in year \\(y\\), and \\(M_{ay}\\) is the natural mortality at age \\(a\\) in year \\(y\\). Any fish that survived beyond the oldest age \\(A\\) in the model are accumulated in the oldest age group and are assumed to be fished at a common rate \\(F_{A,y}\\). \\[ \\begin{align} N_{A,y+1} = &amp;N_{A-1,y} \\exp \\left( - F_{A-1,y} - M_{A-1,y} \\right) \\\\ &amp;+ N_{A,y} \\exp \\left( - F_{A,y} - M_{A,y} \\right) \\end{align} \\] The numbers \\(N_{a,y}\\) are initiated in the first year, \\(y=1\\) and at the youngest age, \\(a=1\\), and the matrix of numbers at age are filled in according to the population dynamics model stated above. Defining \\(R_y = N_{1,y}\\), the numbers at age can be written (ignoring the plus group) as: \\[N_{a,y} = \\left\\{ \\begin{matrix} R_{y-a+1} \\exp \\left( - \\sum^a_{i=1} F_{a-i,y-i} + M_{a-i,y-i} \\right) &amp; y \\geq a \\\\ N_{a-y+1,1} \\exp \\left( - \\sum^{a-y}_{i=1} F_{a-i,y-i} + M_{a-i,y-i} \\right) &amp; y \\lt a \\end{matrix} \\right.\\] Catches in numbers by age and year are defined in terms of the three quantities: natural mortality, fishing mortality and recruitment; using a modified form of the well known Baranov catch equation: \\[C_{ay} = \\frac{F_{ay}}{F_{ay}+M_{ay}}\\left(1 - e^{-(F_{ay}+M_{ay})}\\right) R_{y}e^{-\\sum (F_{ay} + M_{ay})} \\] Survey indices by age and year are defined in terms of the same three quantities with the addition of survey catchability: \\[I_{ays} = Q_{ays} R_{y}e^{-\\sum (F_{ay} + M_{ay})}\\] Observed catches and observed survey indices are assumed to be log-normally distributed, or equivalently, normally distributed on the log-scale, with specific observation variance: \\[ \\log \\hat{C}_{ay} \\sim \\text{Normal} \\Big( \\log C_{ay}, \\sigma^2_{ay}\\Big) \\] \\[ \\log \\hat{I}_{ays} \\sim \\text{Normal} \\Big( \\log I_{ays}, \\tau^2_{ays} \\Big) \\] The log-likelihood can now be defined as the sum of the log-likelihood of the observed catches: \\[ \\ell_C = \\sum_{ay} w^{(c)}_{ay}\\ \\ell_N \\Big( \\log C_{ay}, \\sigma^2_{ay} ;\\ \\log \\hat{C}_{ay} \\Big) \\] and the log-likelihood of the observed survey indices as: \\[ \\ell_I = \\sum_s \\sum_{ay} w^{(s)}_{ays}\\ \\ell_N \\Big( \\log I_{ays}, \\tau_{ays}^2 ;\\ \\log \\hat{I}_{ays} \\Big)\\] giving the total log-likelihood \\[\\ell = \\ell_C + \\ell_I\\] which is defined in terms of the strictly positive quantites, \\(M_{ay}\\), \\(F_{ay}\\), \\(Q_{ays}\\) and \\(R_{y}\\), and the observation variances \\(\\sigma_{ay}\\) and \\(\\tau_{ays}\\). As such, the log-likelihood is over-parameterised as there are many more parameters than observations. In order to reduce the number of parameters, \\(M_{ay}\\) is assumed known (as is common). %==================================================================== %THE FOLLOWING NEEDS REVISION, need to bring in N1 submod %==================================================================== The remaining parameters are written in terms of a linear combination of covariates \\(x_{ayk}\\), e.g. \\[\\log F_{ay} = \\sum_k \\beta_k x_{ayk}\\] where \\(k\\) is the number of parameters to be estimated and is sufficiently small. Using this tecnique the quantities \\(\\log F\\), \\(\\log Q\\), \\(\\log \\sigma\\) and \\(\\log \\tau\\) %\\(\\log \\text{initial\\,age\\,structure}\\) % this is not present in the above (in bold in the equations above) can be described by a reduced number of parameters. The following section has more discussion on the use of linear models in a4a. %==================================================================== The a4a statistical catch-at-age model can addionally allow for a functional relationship that links predicted recruitment \\(\\tilde{R}\\) based on spawning stock biomass and modelled recruitment \\(R\\), to be imposed as a fixed variance random effect. [NEEDS REVISION, sentence not clear] Options for the relationship are the hard coded models Ricker, Beverton Holt, smooth hockeystick or geometric mean. This is implemented by including a third component in the log-likelihood: \\[\\ell_{SR} = \\sum_y \\ell_N \\Big( \\log \\tilde{R}_y(a, b), \\phi_y^2 ;\\ \\log R_y \\Big)\\] giving the total log-likelihood \\[\\ell = \\ell_C + \\ell_I + \\ell_{SR}\\] Using the (time varying) Ricker model as an example, predicted recruitment is \\[\\tilde{R}_y(a_y,b_y) = a_y S_{y-1} e^{-b_y S_{y-1}}\\] where \\(S\\) is spawning stock biomass derived from the model parameters \\(F\\) and \\(R\\), and the fixed quantites \\(M\\) and mean weights by year and age. It is assumed that \\(R\\) is log-normally distributed, or equivalently, normally distributed on the log-scale about the (log) recruitment predicted by the SR model \\(\\tilde{R}\\), with known variance \\(\\phi^2\\), i.e. \\[\\log R_y \\sim \\text{Normal} \\Big( \\log \\tilde{R}_y, \\phi_y^2 \\Big)\\] which leads to the definition of \\(\\ell_{SR}\\) given above. In all cases \\(a\\) and \\(b\\) are strictly positive, and with the quantities \\(F\\), \\(R\\), etc. linear models are used to parameterise \\(\\log a\\) and/or \\(\\log b\\), where relevant. %==================================================================== %THE FOLLOWING NEEDS REVISION, this is not just the default I guess, % it’s always present since R predictions will be a mix of S/R and \\(\\gamma\\) %==================================================================== By default, recruitment \\(R\\) as apposed to the reruitment predicted from a stock recruitment model \\(\\tilde{R}\\), is specified as a linear model with a parameter for each year, i.e. \\[\\log R_y = \\gamma_y\\] This is to allow modelled recruitment \\(R_y\\) to be shrunk towards the stock recruitment model. However, if it is considered appropriate that recruitment can be determined exactly by a relationship with covariates, it is possible, to instead define \\(\\log R\\) in terms of a linear model in the same way as \\(\\log F\\), \\(\\log Q\\), \\(\\log \\sigma\\) and \\(\\log \\tau\\). %But this is pretty much the same as taking a geometric mean, with a model on log a, and making the variance very small. %==================================================================== % WE NEED TO ADD SOMETHING ABOUT HOW THE PLUSGROUP IS MODELLED %==================================================================== 6.2 Classes description Figure 6.1 shows the process to fit a statistical stock assessment catch at age method. Figure: The fit process The a4aFit class The a4aFitSA and a4aFitMCMC classes. Both classes have the same structure "],["submodel-structure.html", "Chapter 7 Submodel structure 7.1 Submodel building blocks and fundamental R formulas 7.2 The major effects available for modelling 7.3 The submodel class and methods", " Chapter 7 Submodel structure The a4a stock assessment framework allows the user to set up a large number of different models. The mechanics which provide this flexibility are designed around the concept of submodels. Each unknown variable that must be estimated is treated as a linear model, for which the user has to define the model structure using R formulas, including mgcv gam formulas. All submodels use the same specification process, the R formula interface, wich gives lot’s of flexibility to explore models and combination of submodels. There are 5 submodels in operation: a model for F-at-age (\\(F_{ay}\\)) a (list) of model(s) for abundance indices catchability-at-age (\\(Q_{ays}\\)), a model for recruitment (\\(R_y\\)) a list of models for the observation variance of catch-at-age and abundance indices (\\(\\{\\sigma^2_{ay}, \\tau^2_{ays}\\}\\)) a model for the initial age structure \\(N_{a,y=1}\\), When setting the structure of each submodel the user is in fact building the predictive model and its parameters. The optimization process, done through ADMB, estimates the parameters and their variance-covariance matrix, allowing further analysis to be carried out, like simulation, prediction, diagnostics, etc. All the statistical machinery will be at the user’s reach. 7.1 Submodel building blocks and fundamental R formulas The elements available to build submodels formulas are ‘age’ and ‘year’, which can be used to build models with different structures. In R’s linear modelling language, a constant model is coded as $ 1$, while a slope over time would simply be \\(\\sim year\\), a smoother over time \\(\\sim s(year, k=10)\\), a model with a coefficient for each year would be \\(\\sim factor(year)\\). Transformations of the variables are as usual, e.g. \\(\\sim sqrt(year)\\), etc; while combinations of all the above can be done although non-convergence will limit the possibilities. Using the \\(F\\) submodel as example the following specifies the models described in the previous paragraph: # models m1 &lt;- ~1 m2 &lt;- ~ year m3 &lt;- ~ s(year, k=10) m4 &lt;- ~ factor(year) m5 &lt;- ~ sqrt(year) # fits fit1 &lt;- sca(ple4, ple4.indices, fmodel=m1, fit=&quot;MP&quot;) fit2 &lt;- sca(ple4, ple4.indices, fmodel=m2, fit=&quot;MP&quot;) fit3 &lt;- sca(ple4, ple4.indices, fmodel=m3, fit=&quot;MP&quot;) fit4 &lt;- sca(ple4, ple4.indices, fmodel=m4, fit=&quot;MP&quot;) fit5 &lt;- sca(ple4, ple4.indices, fmodel=m5, fit=&quot;MP&quot;) # plot lst &lt;- FLStocks(constant=ple4+fit1, linear=ple4+fit2, smooth=ple4+fit3, factor=ple4+fit4, sqrt=ple4+fit5) lst &lt;- lapply(lst, fbar) lgnd &lt;- list(points=FALSE, lines=TRUE, space=&#39;right&#39;) xyplot(data~year, groups=qname, lst, auto.key=lgnd, type=&#39;l&#39;, ylab=&#39;fishing mortality&#39;) (#fig:fund_forms)Example of fundamental R formulas The models above and their combinations can be used to model both ‘age’ and ‘year’. The corresponding fits for age are: # models m1 &lt;- ~1 m2 &lt;- ~ age m3 &lt;- ~ s(age, k=3) m4 &lt;- ~ factor(age) m5 &lt;- ~ sqrt(age) # fits fit1 &lt;- sca(ple4, ple4.indices, fmodel=m1, fit=&quot;MP&quot;) fit2 &lt;- sca(ple4, ple4.indices, fmodel=m2, fit=&quot;MP&quot;) fit3 &lt;- sca(ple4, ple4.indices, fmodel=m3, fit=&quot;MP&quot;) fit4 &lt;- sca(ple4, ple4.indices, fmodel=m4, fit=&quot;MP&quot;) fit5 &lt;- sca(ple4, ple4.indices, fmodel=m5, fit=&quot;MP&quot;) # plot lst &lt;- FLStocks(constant=ple4+fit1, linear=ple4+fit2, smooth=ple4+fit3, factor=ple4+fit4, sqrt=ple4+fit5) lst &lt;- lapply(lst, function(x) harvest(x)[,&#39;2000&#39;]) xyplot(data~age, groups=qname, lst, auto.key=lgnd, type=&#39;l&#39;, ylab=&#39;fishing mortality in 2000&#39;) (#fig:fund_forms_age)Example of fundamental R formulas 7.2 The major effects available for modelling Although the building blocks for formulas are ‘age’ and ‘year’, in fact there are three effects that can be modelled for each submodel: ‘age’, ‘year’ and ‘cohort’. As examples note the following models for fishing mortality. # the age effect ageeffect &lt;- ~ factor(age) # the year effect yeareffect &lt;- ~ factor(year) # the cohort cohorteffect &lt;- ~ factor(year-age) # the fits fit1 &lt;- sca(ple4, ple4.indices, fmodel=yeareffect) fit2 &lt;- sca(ple4, ple4.indices, fmodel=ageeffect) fit3 &lt;- sca(ple4, ple4.indices, fmodel=cohorteffect) and the graphical representation of the three models in Figures 7.1 to 7.3. wireframe(harvest(fit1), main=&#39;year effect&#39;) Figure 7.1: Major effects: the year effect (~ factor(year)) wireframe(harvest(fit2), main=&#39;age effect&#39;) Figure 7.2: Major effects: the age effect (~ factor(age)) wireframe(harvest(fit3), main=&#39;cohort effect&#39;) Figure 7.3: Major effects: the cohort effect (~ factor(year-age)) 7.3 The submodel class and methods %==================================================================== % COLIN TO CHECK THE SECTION %==================================================================== Although the specification of each submodel is done through a R formula, internally the a4a sca fit creates an object (of class ‘submodel’) which stores more information and allows a wide number of methods to be applied. The most important cases are prediction and simulation methods. The submodel class is a S4 class with the following slots: showClass(&quot;submodel&quot;) ## Class &quot;submodel&quot; [package &quot;FLa4a&quot;] ## ## Slots: ## ## Name: formula coefficients vcov centering distr ## Class: formula FLPar array FLPar character ## ## Name: link linkinv name desc range ## Class: function function character character numeric ## ## Extends: &quot;FLComp&quot; Objects of class ‘submodel’ are created in the fitting process using the formulas set by the user (or defaults if missing) and information from the fit. For example begining with a simple a4a fit: # fit a model with indices &quot;IBTS_Q1&quot; and &quot;IBTS_Q3&quot; fit0 &lt;- sca(ple4, ple4.indices[c(&quot;IBTS_Q1&quot;, &quot;IBTS_Q3&quot;)], fmodel = ~ s(age, k = 5), qmodel = list( ~ s(age, k = 4), ~ s(age, k = 4)), srmodel = ~ 1, n1model = ~ s(age, k = 5), vmodel = list( ~ 1, ~ 1, ~ 1), verbose = FALSE) fit0 ## a4a model fit for: PLE ## ## Call: ## .local(stock = stock, indices = indices, fmodel = ..1, qmodel = ..2, ## srmodel = ..3, n1model = ..4, vmodel = ..5, verbose = FALSE) ## ## Time used: ## Pre-processing Running a4a Post-processing Total ## 0.23201323 0.48279190 0.03722453 0.75202966 ## ## Submodels: ## fmodel: ~s(age, k = 5) ## srmodel: ~1 ## n1model: ~s(age, k = 5) ## qmodel: ## IBTS_Q1: ~s(age, k = 4) ## IBTS_Q3: ~s(age, k = 4) ## vmodel: ## catch: ~1 ## IBTS_Q1: ~1 ## IBTS_Q3: ~1 Within the sca fit object there’s a collection of submodels for each component of the a4a stock assessment model (fishing mortality, survey catchability, stock recruitment relationship, initial population and the observation variance for catch numbers and survey indices). For readability the following example will use the fishing mortality submodel although the methods and models described can be applied to each of the 5 submodels. The submodel can be extracted using: fmod &lt;- fmodel(fit0) inside this object is a formula, the coeficients estimated by the model and their covariance matrix. coef(fmod) vcov(fmod) it is also possible to get the data behind the fit and the design matrix required to get the fitted values as.data.frame(fmod) getX(fmod) this makes it possble to check the fit manually like this (note that the data is centered in the model, which is addressed by adding the centering after the prediction is made, see code below): dat &lt;- as.data.frame(fmod) X &lt;- getX(fmod) b &lt;- coef(fmod) dat$fit &lt;- exp(c(X %*% b))# + dat$data.centering) plot(fit ~ age, type = &quot;l&quot;, data = dat[dat$year == 2016,], ylab = &quot;Estimated fishing mortality at age&quot;, ylim = c(0, max(dat$fit)), las = 1) or simulate from the fit by resampling based on the variance matrix of the coefficients, which is done by simulating many coefficients (here called bsim). # get the variance matrix of the coefficients and simulate Sigma &lt;- vcov(fmod)[,,1] bsim &lt;- mvrnorm(999, b, Sigma) fit_sim &lt;- exp(X %*% bsim) dat$ci_lower &lt;- apply(fit_sim, 1, quantile, p = 0.025) dat$ci_upper &lt;- apply(fit_sim, 1, quantile, p = 0.975) plot(fit ~ age, type = &quot;l&quot;, data = dat[dat$year == 2016,], ylab = &quot;Estimated fishing mortality at age&quot;, ylim = c(0, max(dat$ci_upper)), las = 1) lines(ci_lower ~ age, data = dat[dat$year == 2016,], lty = 2) lines(ci_upper ~ age, data = dat[dat$year == 2016,], lty = 2) The above code is to show how it is possible to use an FLa4a submodel to predict and simulate, and not intended to be used by the user. The recomended way to predict from a submodel and make simulations is to use the genFLQuant function (and for finer control, there is also a simulate function). The above example can be done using FLa4a functions as follows: fmod_fit &lt;- genFLQuant(fmod) dat &lt;- as.data.frame(fmod_fit) plot(data ~ age, type = &quot;l&quot;, data = dat[dat$year == 2016,], ylab = &quot;Estimated fishing mortality at age&quot;, ylim = c(0, max(dat$data)), las = 1) or using the ggplotFL package: fmod_fit &lt;- genFLQuant(fmod) ggplot(data = fmod_fit[,&quot;2016&quot;], aes(x = age, y = data)) + geom_point() + geom_line() + ylab(&quot;Estimated fishing mortality at age&quot;) + ylim(0, max(fmod_fit)) and the simulations and confidence intervals are easily computed: # simulate 999 fmod_fit_sim &lt;- genFLQuant(fmod, nsim = 999) # reduce to quantiles fmod_fit_sim &lt;- quantile(fmod_fit_sim[,&quot;2016&quot;], c(0.025, 0.50, 0.975)) plotting can be done by converting to a data.frame, reshaping and using standard ggplot2 functionality dat &lt;- reshape( as.data.frame(fmod_fit_sim, drop=TRUE), timevar = &quot;iter&quot;, idvar = &quot;age&quot;, direction = &quot;wide&quot; ) ggplot(data=dat, aes(x = age, y = `data.50%`)) + geom_ribbon(aes(ymin = `data.2.5%`, ymax = `data.97.5%`), fill=&quot;red&quot;, alpha = .15) + geom_point() + geom_line() + ylab(&quot;Estimated fishing mortality at age&quot;) + ylim(0, max(fmod_fit_sim)) "],["fitting.html", "Chapter 8 Fitting 8.1 Fishing mortality submodel (\\(F_{ay}\\)) 8.2 Abundance indices catchability submodel (\\(Q_{ays}\\)) 8.3 Stock-recruitment submodel (\\(R_y\\))} 8.4 Observation variance submodel (\\(\\{\\sigma^2_{ay}, \\tau^2_{ays}\\}\\)) 8.5 Initial year abundance submodel (\\(N_{a,y=1}\\))} 8.6 Data weigthing 8.7 Working with covariates 8.8 Assessing files 8.9 Missing observations in the catch matrix or index", " Chapter 8 Fitting The a4a stock assessment framework is implemented in R through the method sca(). The method call requires as a minimum a FLStock object and a FLIndices object, in which case the default submodels will be set by the method. Having described building blocks, basic formulations and effects available to build a submodel’s model, it’s important to look into specific formulations and relate them to commonly known representations. Note that although a large number of formulations are available for each submodel, the user must carefuly decide on the full stock assessment model being build and avoid over-paramerizing. Over-parametrization may lead to non-convergence, but may also end up not being very useful for prediction/forecasting, which is one of the main objectives of stock assessment. data(ple4) data(ple4.indices) fit &lt;- sca(ple4, ple4.indices) stk &lt;- ple4 + fit plot(stk) By calling the fitted object the default submodel formulas are printed in the console: fit ## a4a model fit for: PLE ## ## Call: ## .local(stock = stock, indices = indices) ## ## Time used: ## Pre-processing Running a4a Post-processing Total ## 0.8468976 13.9342637 0.1726148 14.9537761 ## ## Submodels: ## fmodel: ~te(age, year, k = c(6, 30), bs = &quot;tp&quot;) + s(age, k = 6) ## srmodel: ~factor(year) ## n1model: ~s(age, k = 3) ## qmodel: ## BTS-Isis-early: ~s(age, k = 6) ## BTS-Combined (ISIS and TRIDENS): ~s(age, k = 6) ## SNS: ~s(age, k = 5) ## BTS-Combined (all): ~s(age, k = 6) ## IBTS_Q3: ~s(age, k = 6) ## IBTS_Q1: ~s(age, k = 5) ## vmodel: ## catch: ~s(age, k = 3) ## BTS-Isis-early: ~1 ## BTS-Combined (ISIS and TRIDENS): ~1 ## SNS: ~1 ## BTS-Combined (all): ~1 ## IBTS_Q3: ~1 ## IBTS_Q1: ~1 To set specific submodels the user has to write the relevant R formula and include it in the call. The arguments for each submodel are self-explanatory: fishing mortality is ‘fmodel’, indices’ catchability is ‘qmodel’, stock-recruitment is ‘srmodel’, observation variance is ‘vmodel’ and for initial year’s abundance is ‘n1model’. The following model comes closer to the official stock assessment of North Sea plaice, as such we’ll name it \\(0\\) and keep it for future comparisons: For future referencing we’ll start with a base fit to be used for future comparisons, named fit 0. fmod0 &lt;- ~s(age, k=6)+s(year, k=10)+te(age, year, k=c(3,8)) qmod0 &lt;- list(~s(age, k = 4), ~s(age, k = 3), ~s(age, k = 3)+year, ~s(age, k = 3), ~s(age, k = 4), ~s(age, k = 6)) srmod0 &lt;- ~ s(year, k=20) vmod0 &lt;- list(~s(age, k=4), ~1, ~1, ~1, ~1, ~1, ~1, ~1) n1mod0 &lt;- ~ s(age, k=3) fit0 &lt;- sca(ple4, ple4.indices, fmodel=fmod0, qmodel=qmod0, srmodel=srmod0, n1model=n1mod0, vmodel=vmod0) stk0 &lt;- ple4 + fit0 plot(stk0) As before by calling the fitted object submodels’ formulas are printed in the console: fit ## a4a model fit for: PLE ## ## Call: ## .local(stock = stock, indices = indices) ## ## Time used: ## Pre-processing Running a4a Post-processing Total ## 0.8468976 13.9342637 0.1726148 14.9537761 ## ## Submodels: ## fmodel: ~te(age, year, k = c(6, 30), bs = &quot;tp&quot;) + s(age, k = 6) ## srmodel: ~factor(year) ## n1model: ~s(age, k = 3) ## qmodel: ## BTS-Isis-early: ~s(age, k = 6) ## BTS-Combined (ISIS and TRIDENS): ~s(age, k = 6) ## SNS: ~s(age, k = 5) ## BTS-Combined (all): ~s(age, k = 6) ## IBTS_Q3: ~s(age, k = 6) ## IBTS_Q1: ~s(age, k = 5) ## vmodel: ## catch: ~s(age, k = 3) ## BTS-Isis-early: ~1 ## BTS-Combined (ISIS and TRIDENS): ~1 ## SNS: ~1 ## BTS-Combined (all): ~1 ## IBTS_Q3: ~1 ## IBTS_Q1: ~1 The method sca has other arguments which may be set by the user: [covar:] a `FLQuant} with covariates; [wkdir:] a folder (character) where the files will be saved for posterior inspection by the user; [verbose:] be more verbose (logical); [fit:] type of fit (character), ‘MP’ runs the minimizer without trying to invert the hessian and as such doesn’t return the covariance matrix of the parameters, normally used inside loops where parameter variance may not be relevant; ‘assessment’ runs minimizer and inverts hessian, returns the covariance matrix of the estimated parameters and the convergence criteria set in ; ‘MCMC’ runs ’s MCMC fit [center:] shall observations be centered before fitting (logical); [mcmc:] ’s MCMC arguments (character vector), must be paired with `fit=“MCMC”}. There are a set of methods for a4a fit objects which help manipulating sca() results, namely: [+:] update the stock object with the fitted fishing mortalities, population abundance and catch in numbers at age; 8.1 Fishing mortality submodel (\\(F_{ay}\\)) We will now take a look at some examples for \\(F\\) models and the forms that we can get. A non-separable model, where we consider age and year to interact can be modeled using a smooth interaction term in the F model using a tensor product of cubic splines with the te method (Figure 8.1), again borrowed from mgcv. fmod &lt;- ~ te(age, year, k = c(4,20)) fit &lt;- sca(ple4, ple4.indices[1], fmod) wireframe(harvest(fit), zlab=&quot;F&quot;) Figure 8.1: Fishing mortality smoothed non-separable model In the last examples the fishing mortalities (Fs’) are linked across age and time. What if we want to free up a specific age class because in the residuals we see a consistent pattern. This can happen, for example, if the spatial distribution of juveniles is disconnected to the distribution of adults. The fishery focuses on the adult fish, and therefore the the F on young fish is a function of the distribution of the juveniles and could deserve a specific model. This can be achieved by adding a component for the year effect on age 1 (Figure 8.2). fmod &lt;- ~ te(age, year, k = c(4,20)) + s(year, k = 5, by = as.numeric(age==1)) fit &lt;- sca(ple4, ple4.indices[1], fmod) wireframe(harvest(fit), zlab=&quot;F&quot;) Figure 8.2: Fishing mortality age-year interaction model with extra age 1 smoother. 8.1.1 Separable model One of the most useful models for fishing mortality is one in which ‘age’ and ‘year’ effects are independent, that is, where the shape of the selection pattern does not change over time, but the overall level of fishing mortality do. Commonly called a ‘separable model’. A full separable model in a4a is written using the factor function which converts age and year effects into categorical values, forcing a different coefficient to be estimated for each level of both effects. This model has age x year number of parameters. fmod1 &lt;- ~ factor(age) + factor(year) fit1 &lt;- sca(ple4, ple4.indices, fmodel=fmod1, fit=&quot;MP&quot;) One can reduce the number of parameters and add dependency along both effects, although still keeping independence of each other, by using smoothers rather than factor. We’ll use a (unpenalised) thin plate spline provided by package mgcv method s(). We’re using the North Sea Plaice data, and since it has 10 ages we will use a simple rule of thumb that the spline should have fewer than \\(\\frac{10}{2} = 5\\) degrees of freedom, and so we opt for 4 degrees of freedom. We will also do the same for year and model the change in \\(F\\) through time as a smoother with 20 degrees of freedom. fmod2 &lt;- ~ s(age, k=4) + s(year, k=20) fit2 &lt;- sca(ple4, ple4.indices, fmodel=fmod2, fit=&quot;MP&quot;) An interesting extension of the separable model is the ‘double separable’ where a third factor or smoother is added for the cohort effect. fmod3 &lt;- ~ s(age, k=4) + s(year, k=20) + s(as.numeric(year-age), k=10) fit3 &lt;- sca(ple4, ple4.indices, fmodel=fmod3, fit=&quot;MP&quot;) ## Warning: *** ~s(age, k = 4) + s(year, k = 20) + s(as.numeric(year - age), k = 10) has 1 too many parameter(s)!! ## i will remove the redundant ones: ## s(as.numeric(year - age)).9 Figures 8.3 and 8.4 depicts the three models selectivities for each year. Each separable model has a single selectivity that changes it’s overall scale in each year, while the double separable introduces some variability over time by modeling the cohort factor. flqs &lt;- FLQuants(factor=harvest(fit1), smooth=harvest(fit2), double=harvest(fit3)) pset &lt;- list(strip.background=list(col=&quot;gray90&quot;)) xyplot(data~age|qname, groups=year, data=flqs, type=&quot;l&quot;, col=1, layout=c(3,1), ylab=&quot;fishing mortality&quot;, par.settings=pset) Figure 8.3: Selection pattern of separable models. Each line represents the selection pattern in a specific year. Independent age and year effects (factor), internally dependent age and year (smooth), double separable (double). wireframe(data~age+year|qname, data=as.data.frame(flqs), layout=c(3,1)) Figure 8.4: Fishing mortality of separable models. Independent age and year effects (factor), internally dependent age and year (smooth), double separable (double). 8.1.2 Constant selectivity for contiguous ages or years To set these models we’ll use the method replace() to define which ages or years will be modelled together with a single coefficient. The following example shows replace() in operation. The dependent variables used in the model will be changed and attributed the same age or year, as such during the fit observations of those age or year with will be seen as replicates. One can think of it as sharing the same mean value, which will be estimated by the model. age &lt;- 1:10 # last age same as previous replace(age, age&gt;9, 9) ## [1] 1 2 3 4 5 6 7 8 9 9 # all ages after age 6 replace(age, age&gt;6, 6) ## [1] 1 2 3 4 5 6 6 6 6 6 year &lt;- 1950:2010 replace(year, year&gt;2005, 2005) ## [1] 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 ## [16] 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 ## [31] 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 ## [46] 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2005 2005 2005 2005 ## [61] 2005 In the \\(F\\) submodel one can use this method to fix the estimation of \\(F\\) in the plus group to be the same as in the last non-aggregated age. fmod &lt;- ~ s(replace(age, age&gt;9, 9), k=4) + s(year, k=20) fit &lt;- sca(ple4, ple4.indices, fmod) wireframe(harvest(fit), zlab=&quot;F&quot;) Figure 8.5: F-at-age fixed above age 9 Or estimate the average \\(F\\) in the most recent years, instead of averaging after the assessment to compute the statu quo selection pattern. fmod &lt;- ~ s(age, k=4) + s(replace(year, year&gt;2013, 2013), k=20) fit &lt;- sca(ple4, ple4.indices, fmod) wireframe(data~age+year, data=harvest(fit)[,ac(2010:2017)], screen=c(z=-130, y=0, x=-60), zlab=&quot;F&quot;) Figure 8.6: F-at-age fixed for the most recent 5 years 8.1.3 Time blocks selectivity To define blocks of data sca() uses the method breakpts(), which creates a factor from a vector with levels defined by the second argument. year &lt;- 1950:2010 # two levels separated in 2000 breakpts(year, 2000) ## [1] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] ## [7] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] ## [13] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] ## [19] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] ## [25] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] ## [31] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] ## [37] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] ## [43] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] (1949,2000] ## [49] (1949,2000] (1949,2000] (1949,2000] (2000,2010] (2000,2010] (2000,2010] ## [55] (2000,2010] (2000,2010] (2000,2010] (2000,2010] (2000,2010] (2000,2010] ## [61] (2000,2010] ## Levels: (1949,2000] (2000,2010] # five periods with equal interval breakpts(year, seq(1949, 2010, length=6)) ## [1] (1949,1961.2] (1949,1961.2] (1949,1961.2] (1949,1961.2] ## [5] (1949,1961.2] (1949,1961.2] (1949,1961.2] (1949,1961.2] ## [9] (1949,1961.2] (1949,1961.2] (1949,1961.2] (1949,1961.2] ## [13] (1961.2,1973.4] (1961.2,1973.4] (1961.2,1973.4] (1961.2,1973.4] ## [17] (1961.2,1973.4] (1961.2,1973.4] (1961.2,1973.4] (1961.2,1973.4] ## [21] (1961.2,1973.4] (1961.2,1973.4] (1961.2,1973.4] (1961.2,1973.4] ## [25] (1973.4,1985.6] (1973.4,1985.6] (1973.4,1985.6] (1973.4,1985.6] ## [29] (1973.4,1985.6] (1973.4,1985.6] (1973.4,1985.6] (1973.4,1985.6] ## [33] (1973.4,1985.6] (1973.4,1985.6] (1973.4,1985.6] (1973.4,1985.6] ## [37] (1985.6,1997.8] (1985.6,1997.8] (1985.6,1997.8] (1985.6,1997.8] ## [41] (1985.6,1997.8] (1985.6,1997.8] (1985.6,1997.8] (1985.6,1997.8] ## [45] (1985.6,1997.8] (1985.6,1997.8] (1985.6,1997.8] (1985.6,1997.8] ## [49] (1997.8,2010] (1997.8,2010] (1997.8,2010] (1997.8,2010] ## [53] (1997.8,2010] (1997.8,2010] (1997.8,2010] (1997.8,2010] ## [57] (1997.8,2010] (1997.8,2010] (1997.8,2010] (1997.8,2010] ## [61] (1997.8,2010] ## 5 Levels: (1949,1961.2] (1961.2,1973.4] (1973.4,1985.6] ... (1997.8,2010] Note seq() computes ‘left-open’ intervals, which means that to include 1950 the sequence must start one year earlier. These methods can be used to create discrete time series, for which a different selection pattern is allowed in each block. This is called an interaction in statistical modelling parlance, and typically a * denotes an interaction term; for smoothers an interaction is achieved using the by argument. When this argument is a factor a replicate of the smooth is produced for each factor level. In the next case we’ll use the breakpts() to split the time series at 1990, although keeping the same shape in both periods, a thin plate spline with 3 knots (Figure 8.7). fmod &lt;- ~s(age, k = 3, by = breakpts(year, 1990)) fit &lt;- sca(ple4, ple4.indices, fmod) Figure 8.7: F-at-age in two periods using in both cases a thin plate spline with 3 knots 8.1.4 Time changing selectivity In many cases, it may be desirable to allow the selection pattern to evolve over time, from year to year. Again there are several ways to do this, one way is to estimate a mean selection pattern, while also allowing F to vary over time for each age. This is like a seperate smoother over year, with ‘age blocks’ so, looking back at previous examples, we have: fmodel &lt;- ~ s(year, k = 15, by = factor(age)) + s(age, k = 4) This is a type of interaction between age and year, but the only connection (or correlation) across ages is via the smoother on age, however there are still 15 degrees of freedom for each age, so the model 5 x 15 + 4 = 69 degrees of freedom. To include correlation across ages and years together then the tensor product (te() function) is used, this has the effect of restricting the flexibility of the model for F. In the following, there is a smoother in 2 dimensions (age and year) where there is 5 degrees of freedom in the age direction, and 15 in the year dimension, resulting in a total of 5 x 15 = 65 degrees of freedom fmodel &lt;- ~ te(age, year, k = c(5, 15)) Often the above formulations provide too much flexibility, and a more complicated, but simpler model is preferable: fmodel &lt;- ~ s(age, k = 4) + s(year, k = 15) + te(age, year, k = c(3, 5)) in the above model, the main effects for age and year still have similar flexibility to the full tensor model, however, the interaction (or the change in F at age over time) has been restricted, so that the full model now has 4 + 15 + 3 x 5 = 34 degrees of freedom. 8.1.5 Trawl fleets} 8.1.6 Nets and Liners fleets} 8.1.7 Multigear fleets} 8.1.8 Trawl surveys} 8.1.9 Closed form selection pattern} One can use a closed form for the selection pattern. The only requirement is to be able to write it as a formula, the example below uses a logistic form. fmod &lt;- ~ I(1/(1+exp(-age))) fit &lt;- sca(ple4, ple4.indices, fmod) wireframe(harvest(fit), zlab=&quot;F&quot;) Figure 8.8: F-at-age logistic 8.1.10 More models More complicated models can be built with these tools. For example, Figure 8.9 shows a model where the age effect is modelled as a smoother (the same thin plate spline) throughout years but independent from each other. fmod &lt;- ~ factor(age) + s(year, k=10, by = breakpts(age, c(2:8))) fit &lt;- sca(ple4, ple4.indices, fmod) wireframe(harvest(fit), zlab=&quot;F&quot;) Figure 8.9: F-at-age as thin plate spline with 3 knots for each age A quite complex model that implements a cohort effect can be set through the following formula. Figure 8.10 shows the resulting fishing mortality. Note that in this case we end up with a variable F pattern over time, but rather than using 4 * 10 = 40 parameters, it uses, 4 + 10 + 10 = 24. fmodel &lt;- ~ s(age, k = 4) + s(pmax(year - age, 1957), k = 10) + s(year, k = 10) fit &lt;- sca(ple4, ple4.indices, fmodel=fmodel) Figure 8.10: F-at-age with a cohort effect. 8.2 Abundance indices catchability submodel (\\(Q_{ays}\\)) The catchability submodel is set up the same way as the \\(F\\) submodel and the tools available are the same. The only difference is that the submodel is set up as a list of formulas, where each formula relates with one abundance index. There’s no limitation in the number of indices or type that can be used for a fit. It’s the analyst that has to decide based on her/his expertise and knowledge of the stock and fleet dynamics. 8.2.1 Catchability submodel for age based indices A first model is simply a dummy effect on age, which means that a coefficient will be estimated for each age. Note that this kind of model considers that levels of the factor are independent (Figure 8.11). qmod &lt;- list(~factor(age)) fit &lt;- sca(ple4, ple4.indices[1], qmodel=qmod) qhat &lt;- predict(fit)$qmodel[[1]] wireframe(qhat, zlab=&quot;q&quot;) Figure 8.11: Catchability age independent model If one considers catchability at a specific age to be dependent on catchability on the other ages, similar to a selectivity modelling approach, one option is to use a smoother at age, and let the data ‘speak’ regarding the shape (Figure 8.12). qmod &lt;- list(~ s(age, k=4)) fit &lt;- sca(ple4, ple4.indices[1], qmodel=qmod) qhat &lt;- predict(fit)$qmodel[[1]] wireframe(qhat, zlab=&quot;q&quot;) Figure 8.12: Catchability smoother age model Finally, one may want to investigate a trend in catchability with time, very common in indices built from CPUE data. In the example given here we’ll use a linear trend in time, set up by a simple linear model (Figure 8.13). qmod &lt;- list( ~ s(age, k=4) + year) fit &lt;- sca(ple4, ple4.indices[1], qmodel=qmod) qhat &lt;- predict(fit)$qmodel[[1]] wireframe(qhat, zlab=&quot;q&quot;) Figure 8.13: Catchability with a linear trend in year 8.2.2 Catchability submodel for age aggregated biomass indices} The previous section was focused on age disaggregated indices, but age aggregated indices (CPUE, biomass, DEPM, etc) may also be used to tune the total biomass of the population. In these cases a different class for the index must be used, the FLIndexBiomass, which uses a vector index with the age dimension called ‘all’. Note that in this case the qmodel should be set without age factors, although it can have a ‘year’ component and covariates if needed. An interesting feature with biomass indices is the age range they refer to can be specified. # simulating a biomass index (note the name of the first dimension element) using # the ple4 biomass and an arbritary catchability of 0.001 plus a lognormal error. dnms &lt;- list(age=&quot;all&quot;, year=range(ple4)[&quot;minyear&quot;]:range(ple4)[&quot;maxyear&quot;]) bioidx &lt;- FLIndexBiomass(FLQuant(NA, dimnames=dnms)) index(bioidx) &lt;- stock(ple4)*0.001 index(bioidx) &lt;- index(bioidx)*exp(rnorm(index(bioidx), sd=0.1)) range(bioidx)[c(&quot;startf&quot;,&quot;endf&quot;)] &lt;- c(0,0) # note the name of the first dimension element index(bioidx) ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 1957 1958 1959 1960 1961 1962 1963 1964 1965 ## all 360.53 449.15 472.07 463.22 528.81 586.14 627.37 498.65 544.16 ## year ## age 1966 1967 1968 1969 1970 1971 1972 1973 1974 ## all 617.47 526.80 438.93 555.89 447.57 523.39 502.85 342.87 407.90 ## year ## age 1975 1976 1977 1978 1979 1980 1981 1982 1983 ## all 567.95 541.02 482.38 522.50 430.16 513.22 475.38 498.89 529.43 ## year ## age 1984 1985 1986 1987 1988 1989 1990 1991 1992 ## all 544.50 576.77 819.83 772.10 802.54 642.95 510.40 449.05 397.38 ## year ## age 1993 1994 1995 1996 1997 1998 1999 2000 2001 ## all 347.99 279.20 352.72 317.43 348.78 423.67 414.72 330.78 311.17 ## year ## age 2002 2003 2004 2005 2006 2007 2008 2009 2010 ## all 353.84 404.47 478.99 395.61 478.67 421.10 559.80 613.24 702.84 ## year ## age 2011 2012 2013 2014 2015 2016 2017 ## all 620.04 757.23 904.08 1028.53 1048.28 960.68 958.36 ## ## units: t # fitting the model fit &lt;- sca(ple4, FLIndices(bioidx), qmodel=list(~1)) To estimate a constant selectivity over time one used the model \\(\\sim 1\\). As a matter of fact the estimate value, 0.001, is not very far from the simulated one, 0.001. An example where the biomass index refers only to age 2 to 4 (for example a CPUE that targets these particular ages). # creating the index dnms &lt;- list(age=&quot;all&quot;, year=range(ple4)[&quot;minyear&quot;]:range(ple4)[&quot;maxyear&quot;]) bioidx &lt;- FLIndexBiomass(FLQuant(NA, dimnames=dnms)) # but now use only ages 2:4 index(bioidx) &lt;- tsb(ple4[ac(2:4)])*0.001 index(bioidx) &lt;- index(bioidx)*exp(rnorm(index(bioidx), sd=0.1)) range(bioidx)[c(&quot;startf&quot;,&quot;endf&quot;)] &lt;- c(0,0) # to pass this information to the model one needs to specify an age range range(bioidx)[c(&quot;min&quot;,&quot;max&quot;)] &lt;- c(2,4) # fitting the model fit &lt;- sca(ple4, FLIndices(bioidx), qmodel=list(~1)) Once more the estimate value, 9.7^{-4}, is not very far from the simulated one, 0.001. 8.2.3 Catchability submodel for single age indices Similar to age aggregated indices one may have an index that relates only to one age, like a recruitment index. In this case the `FLIndex} object must have in the first dimension the age it referes to. The fit is then done relating the index with the proper age in numbers. Note that in this case the qmodel should be set without age factors, although it can have a ‘year’ component and covariates if needed. idx &lt;- ple4.indices[[1]][1] fit &lt;- sca(ple4, FLIndices(recidx=idx), qmodel=list(~1)) # the estimated catchability is predict(fit)$qmodel[[1]] ## An object of class &quot;FLQuant&quot; ## , , unit = unique, season = all, area = unique ## ## year ## age 1985 1986 1987 1988 1989 1990 ## 1 0.00030194 0.00030194 0.00030194 0.00030194 0.00030194 0.00030194 ## year ## age 1991 1992 1993 1994 1995 ## 1 0.00030194 0.00030194 0.00030194 0.00030194 0.00030194 ## ## units: 8.3 Stock-recruitment submodel (\\(R_y\\))} The S/R submodel is a special case, in the sense that it can be set up with the same linear tools as the \\(F\\) and \\(Q\\) models, but it can also use some hard coded models. The example shows how to set up a simple dummy model with factor(), a smooth model with s(), a Ricker model (ricker()), a Beverton and Holt model (bevholt()), a hockey stick model (hockey()), and a geometric mean model (geomean()). See Figure 8.14 for results. As mentioned before, the ‘structural’ models have a fixed variance, which must be set by defining the coefficient of variation. srmod &lt;- ~ factor(year) fit &lt;- sca(ple4, ple4.indices, srmodel=srmod) srmod &lt;- ~ s(year, k=10) fit1 &lt;- sca(ple4, ple4.indices, srmodel=srmod) srmod &lt;- ~ ricker(CV=0.05) fit2 &lt;- sca(ple4, ple4.indices, srmodel=srmod) srmod &lt;- ~ bevholt(CV=0.05) fit3 &lt;- sca(ple4, ple4.indices, srmodel=srmod) srmod &lt;- ~ hockey(CV=0.05) fit4 &lt;- sca(ple4, ple4.indices, srmodel=srmod) srmod &lt;- ~ geomean(CV=0.05) fit5 &lt;- sca(ple4, ple4.indices, srmodel=srmod) flqs &lt;- FLQuants(factor=stock.n(fit)[1], smother=stock.n(fit1)[1], ricker=stock.n(fit2)[1], bevholt=stock.n(fit3)[1], hockey=stock.n(fit4)[1], geomean=stock.n(fit5)[1]) xyplot(data~year, groups=qname, data=flqs, type=&quot;l&quot;, auto.key=list(points=FALSE, lines=TRUE, columns=3), ylab=&quot;No. recruits&quot;) Figure 8.14: Stock-recruitment models fits 8.4 Observation variance submodel (\\(\\{\\sigma^2_{ay}, \\tau^2_{ays}\\}\\)) The variance model allows the user to set up the shape of the observation variances \\(\\sigma^2_{ay}\\) and \\(\\tau^2_{ays}\\). This is an important subject related with fisheries data used for input to stock assessment models. The defaults assume a U-shape model for catch-at-age and constant variance for abundance indices. The first relies on the fact that it’s common to have more precision on the most represented ages and less precision on the less frequent ages which tend to be the younger and older individuals. These sizes are less caught by the fleets and as such do not appear as often at the auction markets samples. With regards to the abundance indices, one assumes a scientific survey to have a well designed sampling scheme and protocols which keep observation error at similar levels across ages. vmod &lt;- list(~s(age, k=3), ~1) fit1 &lt;- sca(ple4, ple4.indices[1], vmodel=vmod) vmod &lt;- list(~s(age, k=3), ~s(age, k=3)) fit2 &lt;- sca(ple4, ple4.indices[1], vmodel=vmod) Variance estimated for the constant model is 0.476 while for the U-shape model, fitted with a smoother, changes with ages (Figure 8.15). wireframe(predict(fit2)$vmodel[[2]], zlab=&quot;variance&quot;) Figure 8.15: Abundance index observation variance estimate Observation variance options have an impact in the final estimates of population abundance, which can be seen in Figure 8.16. flqs &lt;- FLQuants(smother=stock.n(fit1), factor=stock.n(fit2)) xyplot(data~year|age, groups=qname, data=flqs, type=&quot;l&quot;, scales=list(y=list(relation=&quot;free&quot;, draw=FALSE)), auto.key=list(points=FALSE, lines=TRUE, columns=2), par.settings=list(superpose.line=list(col=c(&quot;gray35&quot;, &quot;black&quot;)), strip.background=list(col=&quot;gray90&quot;)), ylab=&quot;&quot;) Figure 8.16: Population estimates using two different variance models 8.5 Initial year abundance submodel (\\(N_{a,y=1}\\))} The submodel for the stock number at age in the first year of the time series is set up with the usual modelling tools (Figure 8.17). Beare in mind that the year effect does not make sense here since it refers to a single year, the first in the time series of data available. This model has its influence limited to the initial lower triangle of the population matrix, which in assessments with long time series doesn’t make much difference. Nevertheless, when modelling stocks with short time series in relation to the number of ages present, it becomes more important and should be given proper attention. n1mod &lt;- ~s(age, k=3) fit1 &lt;- sca(ple4, ple4.indices, fmodel=fmod0, qmodel=qmod0, srmodel=srmod0, vmodel=vmod0, n1model=n1mod) n1mod &lt;- ~factor(age) fit2 &lt;- sca(ple4, ple4.indices, fmodel=fmod0, qmodel=qmod0, srmodel=srmod0, vmodel=vmod0, n1model=n1mod) flqs &lt;- FLQuants(smother=stock.n(fit1)[,1], factor=stock.n(fit2)[,1]) pset &lt;- list(superpose.line=list(col=c(&quot;gray50&quot;, &quot;black&quot;), lty=c(1,2))) xyplot(data~age, groups=qname, data=flqs, type=&quot;l&quot;, auto.key=lgnd, par.settings=pset, ylab=&quot;&quot;) Figure 8.17: Nay=1 models The impact in the overall perspective of the stock status is depicted in Figure 8.18. As time goes by the effect of this model vanishes and the fits become similar. flqs &lt;- FLQuants(smother=stock.n(fit1), factor=stock.n(fit2)) pset$strip.background &lt;- list(col=&quot;gray90&quot;) scl &lt;- list(y=list(relation=&quot;free&quot;, draw=FALSE)) xyplot(data~year|factor(age), groups=qname, data=flqs, type=&quot;l&quot;, scales=scl, auto.key=lgnd, par.settings=pset, ylab=&quot;&quot;) Figure 8.18: Population estimates using two different variance models 8.6 Data weigthing %==================================================================== % COLIN TO CHECK THE SECTION %==================================================================== By default the likelihood components are not weighted and the contribution of each to the maximum likelihood depends on their own likelihood score. However, the user may change these weights by penalizing data points, the \\(w_{ays}\\) in section ??. The likelihood score of each data point will be multiplied by the normalized weights (\\(\\sum w_{ays} = 1\\)). This is done by adding a variance matrix to the catch.n and index.n slots of the stock and index objects. The values should be given as coefficients of variation on the log scale, so that variance is \\(\\log{({CV}^2 + 1)}\\). Figures 8.19 and 8.20 show the results of the two fits in the population abundance and stock summary. stk &lt;- ple4 idx &lt;- ple4.indices[1] # cv of observed catches varslt &lt;- catch.n(stk) varslt[] &lt;- 0.4 catch.n(stk) &lt;- FLQuantDistr(catch.n(stk), varslt) # cv of observed indices varslt &lt;- index(idx[[1]]) varslt[] &lt;- 0.1 index.var(idx[[1]]) &lt;- varslt # run fit1 &lt;- sca(stk, idx, fmodel=fmod0, qmodel=qmod0, srmodel=srmod0, vmodel=vmod0, n1model=n1mod0) ## Note: Provided variances will be used to weight observations. ## Weighting assumes variances are on the log scale or equivalently log(CV^2 + 1). flqs &lt;- FLQuants(nowgt=stock.n(fit0), extwgt=stock.n(fit1)) xyplot(data~year|factor(age), groups=qname, data=flqs, type=&quot;l&quot;, scales=scl, auto.key=lgnd, par.settings=pset, ylab=&quot;&quot;) Figure 8.19: Stock summary of distinct likelihood weightings flsts &lt;- FLStocks(nowgt=ple4+fit0, wgt=ple4 + fit1) plot(flsts) Figure 8.20: Population estimates using two different variance models Note that by using a smaller CV for the index, one is increasing the contribution of the survey and penalizing catch at age, in relative terms. The ratio between likelihood scores of both fits show this effect with catch at age increasing by 2.3 while the index increases almost 8 fold. fit0 &lt;- sca(ple4, ple4.indices[1], fmodel=fmod0, qmodel=qmod0, srmodel=srmod0, vmodel=vmod0, n1model=n1mod0) (fitSumm(fit1)/fitSumm(fit0))[c(2,8,9),] ## nlogl nlogl_comp1 nlogl_comp2 ## 5.158773 2.577961 9.762901 8.7 Working with covariates In linear model one can use covariates to explain part of the variance observed on the data that the ‘core’ model does not explain. The same can be done in the a4a framework. The example below uses the North Atlantic Oscillation (NAO) index to model recruitment. nao &lt;- read.table(&quot;https://www.cpc.ncep.noaa.gov/products/precip/CWlink/pna/norm.nao.monthly.b5001.current.ascii.table&quot;, skip=1, fill=TRUE, na.strings=&quot;-99.90&quot;) dnms &lt;- list(quant=&quot;nao&quot;, year=1950:2024, unit=&quot;unique&quot;, season=1:12, area=&quot;unique&quot;) nao &lt;- FLQuant(unlist(nao[,-1]), dimnames=dnms, units=&quot;nao&quot;) nao &lt;- seasonMeans(trim(nao, year=dimnames(stock.n(ple4))$year)) First by simply assuming that the NAO index drives recruitment (Figure 8.21). srmod &lt;- ~ nao fit2 &lt;- sca(ple4, ple4.indices[1], qmodel=list(~s(age, k=4)), srmodel=srmod, covar=FLQuants(nao=nao)) flqs &lt;- FLQuants(simple=stock.n(fit)[1], covar=stock.n(fit2)[1]) Figure 8.21: Recruitment model with covariates. Using the NAO index as a recruitment index. In a second model we’re using the NAO index not to model recruitment directly but to model one of the parameters of the S/R function (Figure 8.22). srmod &lt;- ~ ricker(a=~nao, CV=0.25) fit3 &lt;- sca(ple4, ple4.indices[1], qmodel=list(~s(age, k=4)), srmodel=srmod, covar=FLQuants(nao=nao)) flqs &lt;- FLQuants(simple=stock.n(fit)[1], covar=stock.n(fit3)[1]) Figure 8.22: Recruitment model with covariates. Using the NAO index as a covariate for the stock-recruitment model parameters. Note that covariates can be added to any submodel using the linear model capabilities of R. 8.8 Assessing files The framework gives access to the files produced to run the ADMB fitting routine through the argument wkdir. When set up all the ADMB files will be left in the directory. Note that the ADMB tpl file is distributed with the FLa4a. One can get it from your R library, under the folder myRlib/FLa4a/admb/. fit1 &lt;- sca(ple4, ple4.indices, wkdir=&quot;fit1run&quot;) 8.9 Missing observations in the catch matrix or index %==================================================================== % COLIN TO CHECK THE SECTION %==================================================================== How are the data interpolated etc … "],["diagnostics.html", "Chapter 9 Diagnostics 9.1 Residuals 9.2 Predictive skill 9.3 Aggreagted catch in weight} 9.4 Fit summary, information and cross-validation metrics 9.5 The package a4adiags 9.6 Residuals and submodels misspecifiction", " Chapter 9 Diagnostics There’s a large number of diagnostics that can be computed for a stock assessment model, the framework implements several analysis of residuals, visualizations and statistics that can be used to evaluate the fit quality and chose across multiple fits. 9.1 Residuals Residuals are a ubiquos metrics to check quality of a fit. For sca() fits there are out-of-the-box methods to compute in the log scale, raw residuals (aka deviances), standardized residuals and pearson residuals. A set of plots to inspect residuals and evaluate fit quality and assumptions are implemented. Consider \\(x_{ay}\\) to be either a catch-at-age matrix (\\(C_{ay}\\)) or one abundance index (\\(I_{ay}\\)) and \\(d\\) to represent residuals. Raw residuals are compute by \\(d_{ay} = \\log{x_{ay}} - \\log{\\tilde{x}_{ay}}\\) and have distribution \\(N(0,\\upsilon^2_{a})\\). Standardized residuals will be compute with \\(d^s_{ay} = \\frac{d_{ay}}{\\hat{\\upsilon}^2_{a}}\\) where \\(\\hat{\\upsilon}^2_{a} = (n-1)^{-1} \\sum_y(d_{ay})^2\\). Pearson residuals scale raw residuals by the estimates of \\(\\sigma^2\\) or \\(\\tau^2\\), as such \\(d^p_{ay} = \\frac{d_{ay}}{\\tilde{\\upsilon}^2_{a}}\\) where \\(\\tilde{\\upsilon}^2_{a} = \\tilde{\\sigma}^2_{a}\\) for catches, or \\(\\tilde{\\upsilon}^2_{a} = \\tilde{\\tau}^2_{a}\\) for each index of abundance. The residuals() method will compute these residuals and generate a object which can be plotted using a set of packed methods. The argument type will allow the user to chose which residuals will be computed. By default the method computes standardized residuals. fit &lt;- sca(ple4, ple4.indices) d_s &lt;- residuals(fit, ple4, ple4.indices) Figure 9.1 shows a scatterplot of standardized residuals with a smoother to guide (or mis-guide …) your visual analysis. Note that the standardization should produce residuals with variance=1, which means that most residual values should be between \\(\\sim -2\\) and \\(\\sim 2\\). plot(d_s) Figure 9.1: Standardized residuals for abundance indices and catch numbers (catch.n). Each panel is coded by age class, dots represent standardized residuals and lines a simple smoother. When plotting residuals by default the auxiliar line is a smoother. However it’s possible to use other type of lines by setting the argument “auxline” in plot. The argument can take the values used by xyplot, which are (from panel.xyplot help page) one or more of the following: “p”, “l”, “h”, “b”, “o”, “s”, “S”, “g”, “r”, “a”, “smooth”, and “spline”. If type has more than one element, an attempt is made to combine the effect of each of the components. The behaviour if any of the first five are included is similar to the effect of the corresponding type in plot: “p” and “l” stand for points and lines respectively; “b” and “o” (for ‘overlay’) plot both; “h” draws vertical (or horizontal if horizontal = TRUE) line segments from the points to the origin. Types “s” and “S” are like “l” in the sense that they join consecutive points, but instead of being joined by a straight line, points are connected by a vertical and a horizontal segment forming a ‘step’, with the vertical segment coming first for “s”, and the horizontal segment coming first for “S”. “g” adds a reference grid. Type “r” adds a linear regression line, “smooth” adds a loess fit, “spline” adds a cubic smoothing spline fit, and “a” draws line segments joining the average y value for each distinct x value. Figure 9.2 shows a regression line over the residuals instead of the loess smooother. plot(d_s, auxline=&quot;r&quot;) Figure 9.2: Standardized residuals for abundance indices and catch numbers (catch.n). Each panel is coded by age class, dots represent standardized residuals and lines a simple smoother. The common bubble plot (bubble()) are shown in Figure 9.3. It shows the same information as Figure 9.1 but in a multivariate perspective. bubbles(d_s) Figure 9.3: Bubbles plot of standardized residuals for abundance indices and for catch numbers (catch.n). Figure 9.4 shows a quantile-quantile plot to assess how well standardized residuals match a normal distribution. qqmath(d_s) Figure 9.4: Quantile-quantile plot of standardized residuals for abundance indices and catch numbers (catch.n). Each panel is coded by age class, dots represent standardized residuals and lines the normal distribution quantiles. Pearson residuals can be computed and plotted the same way as standardized residuals by setting fit='pearson' (Figure 9.5). d_p &lt;- residuals(fit, ple4, ple4.indices, type=&#39;pearson&#39;) plot(d_p) Figure 9.5: Pearson residuals for abundance indices and catch numbers (catch.n). Each panel is coded by age class, dots represent standardized residuals and lines a simple smoother. Finally, the raw residuals are computed by setting fit='deviances' and plotted the same way as before (Figure 9.6). These residuals are usefull to identify which data points are not well modelled, showing a large dispersion of the residuals and requiring more attention from the analyst. d_r &lt;- residuals(fit, ple4, ple4.indices, type=&#39;deviances&#39;) plot(d_r) Figure 9.6: Raw residuals for abundance indices and catch numbers (catch.n). Each panel is coded by age class, dots represent standardized residuals and lines a simple smoother. 9.2 Predictive skill An important feature of stock assessment model fits is the capacity to predict, since one of the most important analysis done with these fits is forecasting future fishing opportunities under pre-defined conditions. The a4a framework implements a visualization of the fit’s predictive skill for both catch-at-age and abundance indices. These are generated by the method plot() with the fit object and a FLStock (Figure 9.7) or FLIndices (Figure ??) object as arguments. plot(fit, ple4) Figure 9.7: Predict and observed catch-at-age plot(fit, ple4.indices) Figure 9.8: Predict and observed abundance-at-age Figure 9.9: Predict and observed abundance-at-age Figure 9.10: Predict and observed abundance-at-age Figure 9.11: Predict and observed abundance-at-age Figure 9.12: Predict and observed abundance-at-age Figure 9.13: Predict and observed abundance-at-age 9.3 Aggreagted catch in weight} Although a statistical catch-at-age model assumes errors in catch-at-age and, as such, errors in the total catch in weight, there’s still interest to evaluate how close the model estimates are of the observed catch in weight1. The implementation of this diagnopstics is done through the method computeCatchDiagnostics(), which can be visualized with plot() (Figure @ref(c_d)). c_d &lt;- computeCatchDiagnostics(fit, ple4) plot(c_d) Figure 9.14: Diagnostics for age aggregated catch in weight 9.4 Fit summary, information and cross-validation metrics To get information about the likelihood fit the method fitSumm() can be used to report number of parameters (npar), negative log-likelkihood (nlogl), ADMB maximum gradient par (maxgrad), number of observations (nobs), generalized cross validation score (gcv), convergence flag (convergence) and acceptance rate (accrate) relevant for MCMC fits only. The second part refers to the likelihood value for each component. fitSumm(fit) ## iters ## 1 ## nopar 2.870000e+02 ## nlogl -3.823602e+02 ## maxgrad 2.060663e-04 ## nobs 1.728000e+03 ## gcv 1.185680e-01 ## convergence 0.000000e+00 ## accrate NA ## nlogl_comp1 -1.058450e+03 ## nlogl_comp2 6.695020e+01 ## nlogl_comp3 5.643150e+01 ## nlogl_comp4 4.025550e+02 ## nlogl_comp5 5.836470e+01 ## nlogl_comp6 6.057810e+01 ## nlogl_comp7 3.121530e+01 Information criteria based metrics are reported with the methods: AIC(fit) ## [1] -190.7205 BIC(fit) ## [1] 1374.784 9.5 The package a4adiags The package a4adiags contains some additional diagnostics based on the reference. Runs test checks weather the residuals are randomly distributed. A “run” is a sequence of the same sign residuals. Few runs indicate a trend or a correlation in the residuals while too many runs may suggest overfitting. The primary output of a runstest is a p-value where: a high p value \\((p\\leq 0.05)\\) suggests that the residuals are randomly distributed, a low p value indicates a non-random pattern in the residuals. library(a4adiags) ## ## Attaching package: &#39;a4adiags&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## fit theme_set(theme_bw()) fit &lt;- sca(mut09, mut09.idx, fmod = ~factor(age) + s(year, k = 8)) res &lt;- residuals(fit, mut09, mut09.idx) plotRunstest(fit, mut09.idx, combine = F) + theme_bw() + facet_wrap(~age) Figure 9.15: Runstest for the abundance index plotRunstest(catch.n(mut09), catch.n(mut09 + fit), combine = F) + theme_bw() + facet_wrap(~age) Figure 9.16: Runstest for the catch by age Green shading indicates no evidence \\((p &lt; 0.05)\\) and red shading evidence \\((p &gt;0.05)\\) to reject the hypothesis of a randomly distributed time-series of residuals, respectively. The shaded (green/red) area spans three residual standard deviations to either side from zero, and the red points outside of the shading violate the ‘\\(3\\sigma\\) limit’ for that series. 9.6 Residuals and submodels misspecifiction 9.6.1 The “mean” model To start the analysis we’ll fit a “mean” model, where all submodels will be set to an overall average, by using the \\(\\sim 1\\) formula. This will be our reference model to see how adding age and year effects will show up in the diagnostic tools, in particular in the residuals. data(hke1567) data(hke1567.idx) fit01 &lt;- sca(hke1567, hke1567.idx, fmod=~1, qmod=list(~1), srmod=~1, vmod=list(~1, ~1), n1mod=~1) res01 &lt;- residuals(fit01, hke1567, hke1567.idx) The common residuals plot clearly shows a trend across ages (Figure 9.18) for both datasets. plot(res01) Figure 9.17: Mean fit residuals by year) Which is even clearer when plotting the residuals by age across years. plot(res01, auxline=&quot;l&quot;, by=&quot;age&quot;) Figure 9.18: Mean fit residuals by age) 9.6.2 The age effects These models will introduce age effects in the fishing mortality submodel and catchability submodel. First in the fishinf mortality submodel. fit02 &lt;- sca(hke1567, hke1567.idx, fmod=~factor(age), qmod=list(~1), srmod=~1, vmod=list(~1, ~1), n1mod=~1) res02 &lt;- residuals(fit02, hke1567, hke1567.idx) The residuals plot now shows catch at age residuals less stagered, reflecting the modelling of the age effect. plot(res02) Figure 9.19: f age effect fit residuals by year) The residuals plot by age shows the same outcome. plot(res02, auxline=&quot;l&quot;, by=&quot;age&quot;) Figure 9.20: f age effect fit residuals by age) Follwed by the same addition to the catchability model. fit03 &lt;- sca(hke1567, hke1567.idx, fmod=~1, qmod=list(~factor(age)), srmod=~1, vmod=list(~1, ~1), n1mod=~1) res03 &lt;- residuals(fit03, hke1567, hke1567.idx) plot(res03) Figure 9.21: q age effect fit residuals by year) The residuals plot by age shows the same outcome. plot(res03, auxline=&quot;l&quot;, by=&quot;age&quot;) Figure 9.22: q age effect fit residuals by age) Finally both effects are brought together. fit04 &lt;- sca(hke1567, hke1567.idx, fmod=~factor(age), qmod=list(~factor(age)), srmod=~1, vmod=list(~1, ~1), n1mod=~1) res04 &lt;- residuals(fit04, hke1567, hke1567.idx) plot(res04) Figure 9.23: q age effect fit residuals by year) The residuals plot by age shows the same outcome. plot(res04, auxline=&quot;l&quot;, by=&quot;age&quot;) Figure 9.24: q age effect fit residuals by age) 9.6.3 The fishing mortality year model This model will introduce an year effect in the fishing mortality submodel on top of the F age effect added before. fit05 &lt;- sca(hke1567, hke1567.idx, fmod=~factor(age) + factor(year), qmod=list(~1), srmod=~1, vmod=list(~1, ~1), n1mod=~1) res05 &lt;- residuals(fit05, hke1567, hke1567.idx) The residuals plot now shows catch at age residuals stagered as before. The year trends are less pronounced although, because the data doesn’t have a very strong year effect, it’s less clear than when modelling the age effect. plot(res05) Figure 9.25: f year effect fit residuals by year) The residuals plot by age shows the same outcome. plot(res05, auxline=&quot;l&quot;, by=&quot;age&quot;) Figure 9.26: f year effect fit residuals by age) We can see now that the residuals show a lot less patterns than before. There’s still some issues, the survey catchability seems to have an year trend. However the model is not fully specified yet, stock recruitment is modelled as constant over time, the initial population abundance is also modelled as a constant as well as the variance models. 9.6.4 The initial year population abundance model, aka N1 This model will introduce an age effect in the population abundance in the first year of the time series. This model sets the n-at-age in the first year of the time series, which is needed due to the lack of previous data to reconstruct those cohorts. fit06 &lt;- sca(hke1567, hke1567.idx, fmod=~factor(age) + factor(year), qmod=list(~factor(age)), srmod=~1, vmod=list(~1, ~1), n1mod=~factor(age)) res06 &lt;- residuals(fit06, hke1567, hke1567.idx) The residuals plot now shows catch at age residuals stagered as before. The year trends are less pronounced although, because the data doesn’t have a very strong year effect, it’s less clear than when modelling the age effect. plot(res06) Figure 9.27: f year effect fit residuals by year) The residuals by age (Figure 9.28) the residuals’ improvement in the first year of the catch at age time series (bottom left plots). plot(res06, auxline=&quot;l&quot;, by=&quot;age&quot;) Figure 9.28: f year effect fit residuals by age) 9.6.5 The stock recruitment submodel In this example we’ll simply add a model to allow recruitment to vary over time and we’ll see how to track potential improvements in the residuals. fit07 &lt;- sca(hke1567, hke1567.idx, fmod=~factor(age) + factor(year), qmod=list(~factor(age)), srmod=~factor(year), vmod=list(~1, ~1), n1mod=~factor(age)) res07 &lt;- residuals(fit07, hke1567, hke1567.idx) The residuals plot by year are very useful to see the effect of adding a varying stock recruitment model. The year trends present in previous models are not absent. Recruitment variability when left unmodelled was being picked up by trends in the survey catchability and catch at age. And due to the cohort dynamics underlying the catch at age model, where propagating into other ages’ estimates. plot(res07) Figure 9.29: f year effect fit residuals by year) plot(res07, auxline=&quot;l&quot;, by=&quot;age&quot;) Figure 9.30: f year effect fit residuals by age) 9.6.6 The variance submodel Finally, we’re testing the variance submodel, specifically the catch at age variance model. We won’t dig into the catchability variance model though. It’s common to accept that a scientific survey following a well designed sampling protocol will have equal variance across ages since no preferential areas are sampled. fit10 &lt;- sca(hke1567, hke1567.idx, fmod=~factor(age) + s(year, k=10), qmod=list(~factor(age)), srmod=~s(year, k=10), vmod=list(~factor(age), ~1), n1mod=~factor(age)) res10 &lt;- residuals(fit10, hke1567, hke1567.idx) To see what’s happening with the variance model one can use predict to plot the different models fitted. flqs &lt;- FLQuants(mod10=predict(fit10)$vmodel$catch[,&quot;2022&quot;], mod07=predict(fit07)$vmodel$catch[,&quot;2022&quot;]) xyplot(data~age, data=flqs, group=qname, type=&quot;l&quot;, auto.key=T) Figure 9.31: Variance models for catch at age To see the effect these models have on the estimated quantities one can look at the variance of the estimates: flqs &lt;- FLQuants(mod10=catch.n(hke1567+simulate(fit10, nsim=500))[,&quot;2022&quot;], mod07=catch.n(hke1567+simulate(fit07, nsim=500))[,&quot;2022&quot;]) bwplot(data~qname|factor(age), data=as.data.frame(flqs), scales=&quot;free&quot;, auto.key=T) Figure 9.32: Estimates of population abundance with different variance models and the usual residuals plot(res10, auxline=&quot;r&quot;) Figure 9.33: f year effect fit residuals by year) plot(res10, auxline=&quot;l&quot;, by=&quot;age&quot;) Figure 9.34: f year effect fit residuals by age) Some analysts believe this is the most important diagnostic since total catch should be trusted. Needless to say we don’t agree and consider reported catch in weight one of the less reliable pieces of information available for stock assessment.↩︎ "],["predict-and-simulate.html", "Chapter 10 Predict and simulate 10.1 Basic functions} 10.2 submodels 10.3 Predict 10.4 Simulate", " Chapter 10 Predict and simulate To predict and simulate R uses the methods predict() and simulate(), which were implemented in FLa4a in the same fashion. fit0 &lt;- sca(ple4, ple4.index, fit=&quot;assessment&quot;) 10.1 Basic functions} Simulation and prediction in FLa4a is based on three functions: simulate and genFLQuant. 10.1.1 simulate() Unlike the stats function stats::simulate, FLa4a::simulate will return the same object as it was passed. For example, if you simulate from a FLa4a fit, you will get an FLa4a fit object back, in which the coefficients of the object are simulations from the model. Likewise, if you call simulate on a submodel object you will get back a submodel object inwhich the coefficients are simulations from the model. Simulations are always done by generating random draws from a multivariate normal distribution with mean given by the coeffients of the model, and variance matrix given by the estimated covariance matrix of the coefficients (in practice this is a submatrix of the inverse of the hessian matrix). Simulate works on several classes from full fits right down to the individual model compents, so if my\\_fit is a fitted a4a model, then simulate(my\\_fit, nsim = 100) will return a new fitted a4a model where the model coefficients now have 100 iters and are drawn from the full variance matrix of the fitted model. Similarly, simulate(fmodel(my\\_fit), nsim = 100) will return a submodel with the same formula as the fishing mortality model as in the a4a fit but where the coefficients are simulated from the variance matrix of the relavent parameters. 10.1.2 genFLQuant() This is a special function who’s purpose is to return an FLQuant or FLQuants. It essentially provides predictions from a model and provides in them as FLQuants of the correct dimensions. genFLQuant also has an argument nsim which if set to a value greater than zero produces simulated predictions from the model based on simulations of the model coefficients. 10.2 submodels In an sca fit individual submodel objects are often combined into a collection of submodels, for example the models for survey catchability are a collection of submodels qmod &lt;- qmodel(fit0) which contains a submodel for each survey catchability. Now (almost) the same code can be run as before to plot the estimates with confidence intervals, the difference with submodels is all the results will be FLQuants so, lapply must be used to do computations on the predictions for each submodel seperately. qmod_fit_sim &lt;- genFLQuant(qmod, nsim = 999) # reduce to quantiles qmod_fit_sim &lt;- lapply(qmod_fit_sim, &quot;[&quot;, j = &quot;2016&quot;) qmod_fit_sim &lt;- lapply(qmod_fit_sim, quantile, prob = c(0.025, 0.50, 0.975)) # reshape dat &lt;- reshape( as.data.frame(qmod_fit_sim, drop=TRUE), timevar = &quot;iter&quot;, idvar = c(&quot;age&quot;, &quot;qname&quot;), direction = &quot;wide&quot; ) # plot ggplot(data=dat, aes(x = age, y = `data.50%`)) + geom_ribbon(aes(ymin = `data.2.5%`, ymax = `data.97.5%`), fill = &quot;red&quot;, alpha = .15) + geom_point() + geom_line() + ylab(&quot;Estimated catchability at age&quot;) + facet_wrap(~ qname, scales = &quot;free_x&quot;) as before the data, coefficients and variance covariance are all available via coef(qmod) vcov(qmod) 10.3 Predict Predict simply computes the quantities of interest using the estimated coefficients and the design matrix of the model. fit.pred &lt;- predict(fit0) lapply(fit.pred, names) ## $stkmodel ## [1] &quot;harvest&quot; &quot;rec&quot; &quot;ny1&quot; ## ## $qmodel ## [1] &quot;BTS-Isis-early&quot; ## ## $vmodel ## [1] &quot;catch&quot; &quot;BTS-Isis-early&quot; 10.4 Simulate Simulate uses the variance-covariance matrix computed from the Hessian returned by ADMB and the fitted parameters, to parametrize a multivariate normal distribution. The simulations are carried out using the method mvrnorm() provided by the R package MASS. Figure 10.1 shows a comparison between the estimated values and the medians of the simulation, while Figure 10.2 presents the stock summary of the simulated and fitted data. fits &lt;- simulate(fit0, 100) flqs &lt;- FLQuants(sim=iterMedians(stock.n(fits)), det=stock.n(fit0)) xyplot(data~year|age, groups=qname, data=flqs, type=&quot;l&quot;, scales=list(y=list(relation=&quot;free&quot;, draw=FALSE)), auto.key=list(points=FALSE, lines=TRUE, columns=2), par.settings=list(superpose.line=list(col=c(&quot;gray35&quot;, &quot;black&quot;)), strip.background=list(col=&quot;gray90&quot;)), ylab=&quot;&quot;) Figure 10.1: Median simulations VS fit stks &lt;- ple4 + fits plot(stks) Figure 10.2: Stock summary of the simulated and fitted data "],["the-statistical-catch-at-age-stock-assessment-framework-with-mcmc.html", "Chapter 11 The statistical catch-at-age stock assessment framework with MCMC 11.1 Diagnostics with CODA", " Chapter 11 The statistical catch-at-age stock assessment framework with MCMC The previous methods were demonstrated using the maximum likelihood estimation method. However, ADMB can also use MCMC methods to fit the model. This section shows how the sca methods interface with ADMB to use the MCMC fits. For this section we’ll use the hake assessment in mediterranean areas (gsa) 1, 5, 6 and 7. The sca likelihood estimate is: # ll data(hke1567) data(hke1567.idx) fmod &lt;- ~s(age, k = 4) + s(year, k = 8) + s(year, k = 8, by = as.numeric(age == 0)) + s(year, k = 8, by = as.numeric(age == 4)) qmod &lt;- list(~I(1/(1 + exp(-age)))) fit &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod) fit &lt;- simulate(fit, 1000) To run the MCMC method, one needs to configure a set of arguments, which is done by creating a SCAMCMC object. For details on the MCMC configuration in ADMB visit the ADMB website. # mcmc mc &lt;- SCAMCMC() # check the default pars mc ## An object of class &quot;SCAMCMC&quot; ## Slot &quot;mcmc&quot;: ## [1] 10000 ## ## Slot &quot;mcsave&quot;: ## [1] 100 ## ## Slot &quot;mcscale&quot;: ## [1] NaN ## ## Slot &quot;mcmult&quot;: ## [1] NaN ## ## Slot &quot;mcrb&quot;: ## [1] NaN ## ## Slot &quot;mcprobe&quot;: ## [1] NaN ## ## Slot &quot;mcseed&quot;: ## [1] NaN ## ## Slot &quot;mcdiag&quot;: ## [1] FALSE ## ## Slot &quot;mcnoscale&quot;: ## [1] FALSE ## ## Slot &quot;mcu&quot;: ## [1] FALSE ## ## Slot &quot;hybrid&quot;: ## [1] FALSE ## ## Slot &quot;hynstep&quot;: ## [1] NaN ## ## Slot &quot;hyeps&quot;: ## [1] NaN Defaults for now are ok, so lets fit the model. Note that the argument fit must be set to MCMC and the argument mcmc takes the SCAMCMC object. A major check when running MCMC is the acceptance rate, which should be around 0.3. This is a rule of thumb, for more information read the (extensive) literature on MCMC. The slot fitSumm stores that information. # fit the model fitmc00 &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) # check acceptance rate fitSumm(fitmc00) ## iters ## 1 ## nopar 52.0000 ## nlogl NA ## maxgrad NA ## nobs 176.0000 ## gcv NA ## convergence NA ## accrate 0.3271 plot(hke1567 + fitmc00) As mentioned above ADMB has several options for MCMC. Here we demonstrate one of them, mcprobe which sets a fat-tailed proposal distribution, as an example of how to use the SCAMCMC objects. mc &lt;- SCAMCMC(mcprobe=0.45) fitmc01 &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) All fits together plot(FLStocks(ll=hke1567 + fit, mc=hke1567 + fitmc00, mc_alt=hke1567 + fitmc01)) 11.1 Diagnostics with CODA We use the package CODA to run the diagnostics on MCMC fits. One needs to convert the a4a output into a mcmc CODA object over which several diagostics can be ran. The mcmc object is a matrix with the parameters (row = iters, cols= pars). Common diagnostics for MCMC chains is to look at the burn-in period, auto-correlation and cross correlation2. The first can be dealt by droping an initial set of iterations, which is done using the function burnin. The second can be managed by thinning the chain, in ADMB this is done through the parameter mcsave N, which defines the iteration’s saving rate (the inverse of the thinning rate). This is the rate at which samples of the parameters are saved, such that thinning is effectively discarding draws. Next fit will run 1000 iterations and save every iter (mcsave=1). library(coda) ## ## Attaching package: &#39;coda&#39; ## The following object is masked from &#39;package:FLa4a&#39;: ## ## as.mcmc mc &lt;- SCAMCMC(mcmc=1000, mcsave=1) fitmc02 &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) fitmc02.mc &lt;- FLa4a::as.mcmc(fitmc02) The autocorrelation plots will show the very strong correlation across samples, which we want to avoid. Figure ?? shows autocorrelation for the first parameter. &lt;&gt;= acf(fitmc02.mc[,1]) Ploting the chain for the parameter clearly shows the autocorrelation but also the burnin phase, where there&#39;s no information about the parameter. These iterations must to be dropped. &lt;&lt;chain01&gt;&gt;= xyplot(fitmc02.mc[,1]) It’s also important to check if the distribution of the parameters is normal, which can be done with the densityplot: &lt;&gt;= densityplot(fitmc02.mc[,1]) Another interesting diagnostic is the Geweke-Brooks Z-score check. This diagnostic indicates if the first and last part of a sample from a Markov chain may not be drawn from the same distribution. It&#39;s useful to decide if the first few iterations should e discarded. &lt;&lt;gew01&gt;&gt;= geweke.plot(fitmc02.mc[,1]) It’s clear from the above diagnostics that a burnin phase of about 200 iterations should be considered. With relation to thining one needs to try several values until no autocorrelation exits. Next fit will run 10000 iterations and save every 10th iteration (mcsave=10), so that the same 1000 iters are generated by the method. mc &lt;- SCAMCMC(mcmc=10000, mcsave=10) fitmc03 &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) fitmc03.mc &lt;- FLa4a::as.mcmc(fitmc03) The autocorrelation plots still shows a strong correlation across samples, although less than in the previous model. acf(fitmc03.mc[,1]) Next fit will run 100000 iterations and save every 100th iteration (mcsave=100), so that the same 1000 iters are generated by the method. Autocorrelation is much weaker, could still be reduced by increasing mcsave. mc &lt;- SCAMCMC(mcmc=100000, mcsave=100) fitmc03 &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) fitmc03.mc &lt;- FLa4a::as.mcmc(fitmc03) Next fit will run 200000 iterations and save every 200th iteration (mcsave=200). mc &lt;- SCAMCMC(mcmc=200000, mcsave=200) fitmc03 &lt;- sca(hke1567, hke1567.idx, fmodel=fmod, qmodel=qmod, fit = &quot;MCMC&quot;, mcmc=mc) fitmc03.mc &lt;- FLa4a::as.mcmc(fitmc03) acf(fitmc03.mc[,1]) All diagnostics improved with the new thining rate although some other improvements can be done. Note this diagnostics should be checked for all parameters. For the sake of space the demonstration uses only on the first. xyplot(fitmc03.mc[,1]) densityplot(fitmc03.mc[,1]) geweke.plot(fitmc03.mc[,1]) Note: add correlation across parameters The manual “A Guide for Bayesian Analysis in AD Model Builder” by Cole C. Monnahan, Melissa L. Muradian and Peter T. Kuriyam describe and explain a larger group of arguments that can be set when running MCMC with ADMB, which the engine a4a uses. ToBe Added↩︎ "],["propagate-natural-mortality-uncertainty.html", "Chapter 12 Propagate natural mortality uncertainty", " Chapter 12 Propagate natural mortality uncertainty In this section we give an example of how uncertainty in natural mortality, set up using the m() method and the class a4aM (see chapter XX), is propagated through the stock assessment. We’ll start by fitting the default model to the data. fit &lt;- sca(ple4, ple4.indices) Using a4a methods we’ll model natural mortality using a negative exponential model by age, Jensen’s estimator for the level and a constant trend with time. We include multivariate normal uncertainty using the mvrnorm() method and create 10 iterations. nits &lt;- 10 shape &lt;- FLModelSim(model=~exp(-age-0.5)) level &lt;- FLModelSim(model=~k^0.66*t^0.57, params = FLPar(k=0.4, t=10), vcov=matrix(c(0.002, 0.01,0.01, 1), ncol=2)) trend &lt;- FLModelSim(model=~b, params=FLPar(b=0.5), vcov=matrix(0.02)) m4 &lt;- a4aM(shape=shape, level=level, trend=trend) m4 &lt;- mvrnorm(nits, m4) range(m4)[] &lt;- range(ple4)[] range(m4)[c(&quot;minmbar&quot;,&quot;maxmbar&quot;)]&lt;-c(1,1) flq &lt;- m(m4)[] quant(flq) &lt;- &quot;age&quot; stk &lt;- propagate(ple4, nits) m(stk) &lt;- flq We fit the same model to the new stock object which has uncertainty in the natural mortality. The assessment is performed for each of the 10 iterations. fit1 &lt;- sca(stk, ple4.indices) And compare the two results (Figure 12.1). It’s quite easy to run these kind of tests and a large part of our effort is to create the tools to do so. plot(FLStocks(&quot;Jensen M with uncertainty&quot;=ple4+fit1, &quot;M=0.2&quot;=ple4+fit), key=TRUE) Figure 12.1: Stock summary for two M models "],["modelling-spatial-effects.html", "Chapter 13 Modelling spatial effects", " Chapter 13 Modelling spatial effects ridx01 &lt;- stk0@stock.n[1]*0.7*0.001 ridx01 &lt;- log(ridx01*rlnorm(ridx01)) ridx02 &lt;- stk0@stock.n[1]*0.3*0.001 ridx02 &lt;- log(ridx02*rlnorm(ridx02)) srmod &lt;- ~ geomean(a~ridx01+ridx02, CV=0.5) cvar &lt;- FLQuants(ridx01 = ridx01, ridx02 = ridx02) fit01 &lt;- sca(stock,tun.sel[c(1)],fmodel=fmod,qmodel=qmod, srmodel=srmod, covar=cvar) coef(fit01) srmod &lt;- ~ geomean(CV=0.1) fit02 &lt;- sca(stock,tun.sel[c(1)],fmodel=fmod,qmodel=qmod, srmodel=srmod) coef(fit02) check situation where the two areas are negatively correlated the two covariates need to be at the same scale, in the sense of representing the same process other examples (ICES, ask in the plenary if we can have access to the data) dan ghotel ask for spatial workshop data "],["reference-points.html", "Chapter 14 Reference Points 14.1 Yield per recruit reference points 14.2 Stock recruitment relationship based reference points 14.3 Economics reference points 14.4 Computing user specific reference points", " Chapter 14 Reference Points One of the primary objectives of stock assessment is the estimation of reference points. These serve as benchmarks for evaluating the outputs of assessment models and determining the status of a fish stock. Reference points are critical for effective fisheries management, guiding decisions on sustainable exploitation. The most common classification of stock status is bidimensional, comparing exploitation levels and biomass sizes against target reference points. This framework allows for the assessment of whether a stock is overfished or experiencing overfishing: Exploitation Levels: Typically represented by fishing mortality (F), overfishing occurs when F exceeds the target reference point. Conversely, if F is below the reference, the stock is considered to be sustainably fished. Biomass Size: Commonly measured by spawning stock biomass (SSB), stocks are deemed overfished if the SSB falls below the reference point. These assessments often utilize tools like the Kobe plot, which visually represents stock status in relation to these metrics. In addition to target reference points, limit reference points (LRPs) are commonly included in stock assessments. These represent thresholds that should not be crossed, as they signal a high risk of stock collapse or significant uncertainty in population dynamics. Effective management aims to maintain fishing pressure and biomass levels within safe biological limits, ensuring long-term sustainability and reducing the risk of adverse outcomes. Advancements in stock assessment science continue to refine these reference points. For example, Maximum Sustainable Yield (MSY) and its proxies, such as B\\(_{MSY}\\) (biomass at MSY) and F\\(_{MSY}\\) (fishing mortality at MSY), remain widely used. These reference points are related with the stock’s productivity, which in itself is a complex interaction between recruitment, growth and mortality processes. Recruitment is the process of input to the population, it defines the number of fish that will enter the population and are vulnerable to fishing. It encompasses the process of spawning, which depends on the reproductive potential of the individuals, and the survivability of the laervae up to entering the fishery, which mostly depends on environmental conditions. Individual growth defines the time needed for an individual to gain weight, grow in length and eventualy mature and spawn. Mortality is commonly split between mortality caused by fishing and mortality caused by natural events. Natural mortality merges together all factors by which an individual may die and are not related to fishing, for example predation from other species. These processes, recruitment, individual growth and natural mortality depend on a mix of interactions between environmental conditions and species’ biology. Fishing mortality on the other hand is mostly dependent on the human factor, it’s related with the choice to fish and the way to fish. It’s the outcome of the effort the fleet deploys, the selectivity of the gear used and the availability of individuals. For example, the productivity of the stock will be different if the fleet fishes in an area with lots of young fish using a small mesh size, from a fleet fishing in an area where young fish are not common and using a large mesh size. For this section we’ll be using the package FLBRP (REF) from the FLR family of packages. library(FLBRP) library(FLa4a) data(ple4) data(ple4.indices) fit0 &lt;- sca(ple4, ple4.indices) stk0 &lt;- ple4 + fit0 To proceed with the computation of reference points we must start by creating an FLBRP object and afterwards run the fitting process with brp(). The FLBRP class has information on selection pattern, mass at age, and biological parameters. The information is stored in the object’s slots which can be accessed with the usual commands, respectively catch.sel(), discards.sel(), stock.wt(), catch.wt(), discards.wt(), m() and mat(). These quantities are computed by averaging the 3 most recent years of the relevant stock object slots. In the case of selection pattern the values are scaled so that \\(\\bar{F}=1\\) [IAGO TO CHECK]. FLBRP creates a harvest slot with 100 computations of fishing mortality at age scaled from \\(\\bar{F}=0\\) up to \\(\\bar{F}=4\\), which is later used in the fitting process. A number of parameters can be set by the user to create the FLBRP object: fbar = seq(0, 4, length.out = 101), nyears = 3, biol.nyears = nyears, fbar.nyears = nyears, sel.nyears = fbar.nyears, na.rm = TRUE, mean = “arithmetic”. [IAGO is this somewhere we can point to instead of adding it here?] 14.1 Yield per recruit reference points In the case where no stock recruitment relationship exists, or was fitted, brp() will return yield per recruit reference points. By default it computes biomasses in the absence of fishing, also know as virgin biomass, \\(F_{MAX}\\), \\(F_{0.1}\\) and 40% Spawning per recruit reference points. brp0 &lt;- FLBRP(stk0) brp0 &lt;- brp(brp0) summary(brp0) ## An object of class &quot;FLBRP&quot; ## ## Name: ## Description: ## Quant: age ## Dims: age year unit season area iter ## 10 101 1 1 1 1 ## ## Range: min max pgroup minfbar maxfbar ## 1 10 10 2 6 ## ## ## Model: rec ~ a ## params ## iter a ## 1 1 ## ## refpts: calculated The selection pattern and other quantities can be depicted by calling plot() on the specific FLBRP object’s slot. xyplot(data~age,data=catch.sel(brp0),type=c(&#39;l&#39;, &#39;p&#39;)) xyplot(data~age|qname, data=FLQuants(sel=catch.sel(brp0), dsel=discards.sel(brp0), swt=stock.wt(brp0), cwt =catch.wt(brp0), mat= mat(brp0), m = m(brp0)), type=&quot;l&quot;,scale=&quot;free&quot;) To extract a table with all reference points one uses the method refpts(). Note in this case \\(F_{msy}\\) is the same as \\(F_{max}\\), since the assumed stock recruitment is mean recruitment. refpts(brp0) ## An object of class &quot;FLPar&quot; ## quant ## refpt harvest yield rec ssb biomass revenue cost ## virgin 0.00e+00 0.00e+00 1.00e+00 3.42e+00 3.53e+00 NA NA ## msy 2.10e-01 7.08e-02 1.00e+00 9.44e-01 1.03e+00 NA NA ## crash 1.47e+01 6.02e-06 1.00e+00 4.38e-06 2.87e-02 NA NA ## f0.1 1.58e-01 6.85e-02 1.00e+00 1.28e+00 1.38e+00 NA NA ## fmax 2.10e-01 7.08e-02 1.00e+00 9.44e-01 1.03e+00 NA NA ## spr.30 1.96e-01 7.06e-02 1.00e+00 1.03e+00 1.12e+00 NA NA ## mey NA NA NA NA NA NA NA ## quant ## refpt profit ## virgin NA ## msy NA ## crash NA ## f0.1 NA ## fmax NA ## spr.30 NA ## mey NA ## units: NA refpts(brp0)[c(&#39;msy&#39;, (&#39;fmax&#39;)), ] ## An object of class &quot;FLPar&quot; ## quant ## refpt harvest yield rec ssb biomass revenue cost profit ## msy 0.2099 0.0708 1.0000 0.9436 1.0349 NA NA NA ## fmax 0.2099 0.0708 1.0000 0.9436 1.0349 NA NA NA ## units: NA The depiction of the reference points with the method plot() shows recruitment as constant over all levels of biomass and set to \\(1\\). plot(brp0) 14.2 Stock recruitment relationship based reference points An important way to improve reference points is to include stock recruitment dynamics. Yield per recruit, as in previous section, ignores this dynamics and assumes recruitment will be the same no matter SSB’s size, which is obviously wrong although in many cases due to unknown or very uncertain dynamics it’s the best one can do. The stock recruitment model must be fitted before computing reference points and the FLSR object has to be passed to the FLBRP call to create the object that brp() method will use. There’s two ways of fitting stock recruitment models: (i) after fitting the stock assessment model by using its outputs, SSB and recruitment, as data to fit the model; (ii) inside the stock assessment model together with all other quantities. There’s pros and cons on both approaches, we’re not going to dwell on those now though. 14.2.1 Stock recruitment after fitting the stock assessment model In the following example we’ll use a Beverton and Holt stock recruitment reltionship. There are several other relationships that can be used, see [IAGO CHECK THIS] sr0 &lt;- as.FLSR(stk0, model=bevholt) sr0 &lt;- fmle(sr0) ## Nelder-Mead direct search function minimizer ## function value for initial parameters = -18.229811 ## Scaled convergence tolerance is 2.71645e-07 ## Stepsize computed as 138047.472012 ## BUILD 3 -8.381087 -21.236493 ## REFLECTION 5 -18.229811 -23.986923 ## LO-REDUCTION 7 -21.236493 -23.986923 ## HI-REDUCTION 9 -23.670348 -23.986923 ## REFLECTION 11 -23.689833 -24.357493 ## LO-REDUCTION 13 -23.986923 -24.517332 ## LO-REDUCTION 15 -24.357493 -24.651322 ## LO-REDUCTION 17 -24.517332 -24.735311 ## LO-REDUCTION 19 -24.651322 -24.782483 ## LO-REDUCTION 21 -24.735311 -24.826934 ## LO-REDUCTION 23 -24.782483 -24.842111 ## LO-REDUCTION 25 -24.826934 -24.866395 ## HI-REDUCTION 27 -24.842111 -24.872880 ## HI-REDUCTION 29 -24.866395 -24.884011 ## LO-REDUCTION 31 -24.872880 -24.884011 ## HI-REDUCTION 33 -24.879745 -24.884011 ## EXTENSION 35 -24.882025 -24.888317 ## EXTENSION 37 -24.884011 -24.897425 ## EXTENSION 39 -24.888317 -24.904784 ## EXTENSION 41 -24.897425 -24.937270 ## EXTENSION 43 -24.904784 -24.940973 ## EXTENSION 45 -24.937270 -25.043544 ## LO-REDUCTION 47 -24.940973 -25.043544 ## EXTENSION 49 -25.008191 -25.186152 ## LO-REDUCTION 51 -25.043544 -25.186152 ## EXTENSION 53 -25.161825 -25.335939 ## EXTENSION 55 -25.186152 -25.540561 ## LO-REDUCTION 57 -25.335939 -25.540561 ## EXTENSION 59 -25.501589 -25.922665 ## EXTENSION 61 -25.540561 -26.001469 ## EXTENSION 63 -25.922665 -26.704604 ## LO-REDUCTION 65 -26.001469 -26.704604 ## LO-REDUCTION 67 -26.468110 -26.704604 ## LO-REDUCTION 69 -26.695594 -26.704604 ## HI-REDUCTION 71 -26.703662 -26.727696 ## HI-REDUCTION 73 -26.704604 -26.729632 ## HI-REDUCTION 75 -26.726736 -26.729632 ## HI-REDUCTION 77 -26.727696 -26.731255 ## HI-REDUCTION 79 -26.729632 -26.731255 ## HI-REDUCTION 81 -26.731204 -26.731582 ## HI-REDUCTION 83 -26.731255 -26.731744 ## HI-REDUCTION 85 -26.731582 -26.731772 ## HI-REDUCTION 87 -26.731744 -26.731820 ## HI-REDUCTION 89 -26.731772 -26.731823 ## HI-REDUCTION 91 -26.731820 -26.731835 ## HI-REDUCTION 93 -26.731823 -26.731844 ## LO-REDUCTION 95 -26.731835 -26.731844 ## Exiting from Nelder Mead minimizer ## 97 function evaluations used plot(sr0) We now need to provide the FLSR object, sr0, to the FLBRP call and refit the reference points. ## rec ~ a * ssb/(b + ssb) ## &lt;environment: 0x600e2cfddeb8&gt; ## An object of class &quot;FLPar&quot; ## params ## a b ## 1038832 9829 ## units: NA The new reference points can now be extracted using the refpts method with the FLBRP object as the main argument, and depict the relationships with plot(). Note this time by setting the flag obs to TRUE the plot will include the estimates of \\(SSB\\) and \\(R\\). refpts(brp0) ## An object of class &quot;FLPar&quot; ## quant ## refpt harvest yield rec ssb biomass revenue cost ## virgin 0.00e+00 0.00e+00 1.04e+06 3.54e+06 3.65e+06 NA NA ## msy 2.07e-01 7.28e+04 1.03e+06 9.87e+05 1.08e+06 NA NA ## crash 2.25e+00 1.11e-06 3.87e-04 3.66e-06 1.83e-05 NA NA ## f0.1 1.58e-01 7.06e+04 1.03e+06 1.32e+06 1.42e+06 NA NA ## fmax 2.10e-01 7.28e+04 1.03e+06 9.70e+05 1.06e+06 NA NA ## spr.30 1.96e-01 7.27e+04 1.03e+06 1.06e+06 1.15e+06 NA NA ## mey NA NA NA NA NA NA NA ## quant ## refpt profit ## virgin NA ## msy NA ## crash NA ## f0.1 NA ## fmax NA ## spr.30 NA ## mey NA ## units: NA Note \\(MSY\\) based reference points are no longer the same as \\(F_{MAX}\\), and recruitment is no longer constant over all \\(SSB\\) levels. plot(brp0, obs=TRUE) 14.2.2 Stock recruitment during fitting the stock assessment model [NEED COLIN’S HELP] fit1 &lt;- sca(ple4, ple4.indices, srmodel = ~ bevholt(CV = 0.5)) a4aflsr &lt;- as(stkmodel(fit1), &quot;FLSR&quot;) # or a4aflsr &lt;- as(fit1, &quot;FLSR&quot;) plot(a4aflsr, obs = TRUE) a4abrp &lt;- FLBRP(stk0, a4aflsr) 14.3 Economics reference points We can add economic data to the FLBRP object to calculate economic based reference points, like maximum economic yield (MEY). We need to provide information about price, variable costs and fixed costs. The first in value at age per weight of fish, the others in value per unit of fishing mortality. # price price(brp0) &lt;- c(rep(1,3),rep(1.5,2),rep(2,5)) price(brp0)@units &lt;- &quot;1000 euro per ton&quot; # variable costs per F vcost(brp0) &lt;- 100000 vcost(brp0)@units &lt;- &quot;1000 euro per F&quot; # fixed costs per F fcost(brp0) &lt;- 50000 fcost(brp0)@units &lt;- &quot;1000 euro per F&quot; # reference points brp0 &lt;- brp(brp0) refpts(brp0) ## An object of class &quot;FLPar&quot; ## quant ## refpt harvest yield rec ssb biomass revenue cost ## virgin 0.00e+00 0.00e+00 1.04e+06 3.54e+06 3.65e+06 0.00e+00 5.00e+04 ## msy 2.07e-01 7.28e+04 1.03e+06 9.87e+05 1.08e+06 1.21e+05 7.07e+04 ## crash 2.25e+00 1.11e-06 3.87e-04 3.66e-06 1.83e-05 1.15e-06 2.75e+05 ## f0.1 1.58e-01 7.06e+04 1.03e+06 1.32e+06 1.42e+06 1.20e+05 6.58e+04 ## fmax 2.10e-01 7.28e+04 1.03e+06 9.70e+05 1.06e+06 1.21e+05 7.10e+04 ## spr.30 1.96e-01 7.27e+04 1.03e+06 1.06e+06 1.15e+06 1.22e+05 6.96e+04 ## mey 2.19e-01 7.27e+04 1.03e+06 9.22e+05 1.02e+06 1.21e+05 7.19e+04 ## quant ## refpt profit ## virgin -5.00e+04 ## msy 5.07e+04 ## crash -2.75e+05 ## f0.1 5.38e+04 ## fmax 5.03e+04 ## spr.30 5.21e+04 ## mey 4.89e+04 ## units: NA The reference points table is now complete with values for revenue, costs and profit, as well as estimtes for \\(MEY\\) based reference points. The point where profits are maximized, instead of the point where catch is maximized as in the case of MSY. plot(brp0) 14.4 Computing user specific reference points [DANAI YOU ADD CODE TO DO THIS CAN YOU PLEASE SEND IT TO ME] One specific case is to compute \\(F_{MSY}\\) ranges according to Hilborn (2010) and Rindorf (2012) ideas. For this case there’s already the method msyRanges, which takes as argument a fitted FLBRP object and delivers a FLPar object, similar to refpts. rp.rngs &lt;- msyRange(brp0, range=0.05) rp.rngs ## An object of class &quot;FLPar&quot; ## quantity ## refpt harvest yield rec ssb biomass revenue cost profit ## msy 2.07e-01 7.28e+04 1.03e+06 9.87e+05 1.08e+06 1.21e+05 7.07e+04 5.07e+04 ## min 1.45e-01 6.92e+04 1.03e+06 1.43e+06 1.53e+06 1.18e+05 6.45e+04 5.31e+04 ## max 2.87e-01 6.92e+04 1.02e+06 6.26e+05 7.14e+05 1.12e+05 7.87e+04 3.36e+04 ## units: NA Another simple way, although it onl;y works for \\(SPR\\) based reference points, is to include other spr.## points in the refpts table. [IAGO NEED TO KNOW HOW TO EXTEND THE FLPAR TO HAVE MORE ROWS] "],["projections-and-harvest-control-rules.html", "Chapter 15 Projections and harvest control rules 15.1 Initial condition assumptions [CHECK fwdWindow] 15.2 Scenarios", " Chapter 15 Projections and harvest control rules Massad et.al has a nice definition “Prediction in general science can be divided into two components: forecasting and projections [2]. A forecast is an attempt to predict what will happen. A projection is an attempt to describe what would happen, given certain hypotheses [3].” (OECD) “Forecasting” and “prediction” are often used synonymously in the customary sense of assessing the magnitude which a quantity will assume at some future point of time . Medium-term projections or scenarios In the context of the OECD’s Economic Outlook, medium-term projections or scenarios look out five to six years, and are published as part of the OECD’s projection exercise. Projection This term is used in two connected senses. 1. In relation to a time series it means a future value calculated according to predetermined changes in the assumptions of the environment. 2. More recently, it has been used in probability theory to denote the conditional expectation of a variate. Since a regression equation gives the expectation of the dependent variate conditional upon values of the predicted (“independent”) variates and such equations are used for forecasting or prediction, the usages are connected. Considering the above definitions one can think of the process of advising future fishing opportunities as a mix of forecasting and projections. When using a HCR that is accepted by decision makers to predict the catches which can be taken from the stock, one’s in forecasting space. The process will retur what will happen having into account the known conditions of the system. On the other hand, when exploring potential future outcomes based on a set of scenarios previously defined, one’s much more in projection’s space. In fisheries science it’s not common to make the distinction between forecasts and predictions, and, in general, the approach tends to be much more within the realm of predictions. For example, it’s common to run scenarios when giving advice to fisheries managers or test different assumptions about future events. Strickly speaking projections are not part of the stock assessment process. The assessment of the status of the stock is completed when the analysts produces a comparison between the estimates of biomass and fishing mortality and the respective reference points. At that point it’s possible to make statements about the stock being overfished, or not, and being subject to overfishing, or not. Projections tend to come after having estimates of abundance or biomass and fishing mortality. Together with estimates, or assumptions, about the population’s dynamics: growth, reproduction, natural mortality, etc; one can predict catches, biomass, abundance and other relevant statistics, under spoecific conditions and with a certain level of uncertainty. For this section we’ll be using the package FLasher (REF) from the FLR family of packages. Starting by fitting a model including a stock recruitment model. For projections one needs to set future recruitment. ## Loading required package: FLFishery ## FLasher: Frighten their children by speaking our name The basic workflow to project with FLasher is to extend the FLStock object to store the predictions using the method fwdWindow, set the targets for the projection with method fwdControl and project the fishery with the method fwd with a FLStock and a FLSR. # model fit fit00 &lt;- sca(ple4, ple4.indices, srmodel=~geomean()) sr00 &lt;- as(fit00, &quot;FLSR&quot;) stk00 &lt;- ple4 + fit00 # set projection projy &lt;- 5 endpy &lt;- maxy + projy inipy &lt;- maxy + 1 stk00 &lt;- fwdWindow(stk00, end = endpy) trg00 &lt;- fwdControl(year = inipy:endpy, quant = &quot;f&quot;, value = 0.3) # project stk01 &lt;- fwd(stk00, control=trg00, sr=sr00) Figure 15.1: Projection of stock for 5 years with fixed fishing mortality and recruitment A natural addition to this forecast is to add uncertainty. We’ll do that by generating uncertainty in population numbers, catch numbers and fishing mortality, using simulate, and add stock recruitment uncertainty using the residuals of the fit. stk00 &lt;- ple4 + simulate(fit00, nsim) stk00 &lt;- fwdWindow(stk00, end = endpy) res00 &lt;- residuals(sr00) rec00 &lt;- window(rec(stk00), 2018, 2022) rec00 &lt;- rlnorm(rec00, mean(res00), sqrt(var(res00))) stk02 &lt;- fwd(stk00, control=trg00, sr=sr00, residuals=rec00) Figure 15.2: Stochastic projection of stock for 5 years with fixed fishing mortality and recruitment An alternative to the above workflow is to fit the stock recruitment model after the stock assessment model, using the outputs of the assessment as input to the stock recruitment fit. In which case stock recruitment estimation uncertainty can be added by fitting the stock recruitment model over stock assessment uncertainty, so that there will be stock-recruitment fit to each iteration generated from the stock assessment model. fit00 &lt;- sca(ple4, ple4.indices) stk00 &lt;- ple4 + simulate(fit00, nsim) sr00 &lt;- as.FLSR(stk00, model=&quot;geomean&quot;) sr00 &lt;- fmle(sr00, control = list(trace = 0)) stk00 &lt;- fwdWindow(stk00, end = endpy) res00 &lt;- residuals(sr00) rec00 &lt;- window(rec(stk00), inipy, endpy) rec00 &lt;- rlnorm(rec00, c(yearMeans(res00)), sqrt(c(yearVars(res00)))) stk03 &lt;- fwd(stk00, control=trg00, sr=sr00, residuals=rec00) [IAGO, check rlnorm, it doesn’t work if the meanlog and sdlog are FLQuants] Figure 15.3: Stochastic projection of stock for 5 years with fixed fishing mortality and recruitment These two methods don’t give very different result when the stock recruitment model is not having a large impact in the other parameters. However the second method is much slower due to all the fits needed to have the empirical distribution of the stock recruitment model parameters. Figure 15.4: Stochastic projection of stock for 5 years with fixed fishing mortality and recruitment 15.1 Initial condition assumptions [CHECK fwdWindow] When projecting the stock forward one needs to make a number of assumptions about initial conditions, the starting point from where projections will be made. The method fwdWindow has a set of options that allows the analyst to decide about those assumptions: - wt: Number of years to average over to get the future mean weights at age, default is 3 - mat: Number of years to average over to get the future proportion mature at age, default is 3 - m: Number of years to average over to get the future natural mortality at age, default is 3 - spwn: Number of years to average over to get the future fraction of mortality before spawning, default is 3 - discards.ratio: Number of years to average over to get the future mean proportion of discards at age. Default is 3 - catch.sel: Number of years to average over to get the future selection patern (fishing mortality at age which will be scaled based on canges in \\(\\bar{F}\\)). Default is 3 One can also define if those assumptions will be based on the mean value over the time period set, or randomly sampled from historical values, through setting the argument fun to mean or sample, respectively. For the next examples we’ll use the approach of fitting the stock recruitment within the assessment together with other parameters. We’ll set to 20 the number of years to compute mean weights at age, to 10 the number of years to average across and estimate the selection pattern in terms of fishing mortality at age. Finally, we’ll use a 10 year period to compute the average discard ratio. fit00 &lt;- sca(ple4, ple4.indices, srmodel=~geomean()) sr00 &lt;- as(fit00, &quot;FLSR&quot;) stk00 &lt;- ple4 + fit00 stk00 &lt;- fwdWindow(stk00, end = endpy, years = list(wt = 20, catch.sel = 10, discards.ratio = 10)) trg00 &lt;- fwdControl(year = inipy:endpy, quant = &quot;f&quot;, value = 0.3) stk04 &lt;- fwd(stk00, control=trg00, sr=sr00) Figure 15.5: Stochastic projections of stock for 5 years with fixed fishing mortality and recruitment. Two scenarios with different assumptions about initial conditions 15.2 Scenarios There’s a wide range of scenarios that can be of interest to project in order to give advice to policy makers, or to better understand the fitted stock assessment model. For example, forecasting the stock in the absence of fishing for a few generations, gives good insights about the dynamics of the population being modelled. fit00 &lt;- sca(ple4, ple4.indices, srmodel=~geomean()) sr00 &lt;- as(fit00, &quot;FLSR&quot;) stk00 &lt;- ple4 + simulate(fit00, nsim) # set projection projy &lt;- 25 endpy &lt;- maxy + projy inipy &lt;- maxy + 1 stk00 &lt;- fwdWindow(stk00, end = endpy) trg00 &lt;- fwdControl(year = inipy:endpy, quant = &quot;f&quot;, value = 0) # recruitment uncertainty res00 &lt;- residuals(sr00) rec00 &lt;- window(rec(stk00), inipy, endpy) rec00 &lt;- rlnorm(rec00, mean(res00), sqrt(var(res00))) # project stk05 &lt;- fwd(stk00, control=trg00, sr=sr00, residuals=rec00) Figure 15.6: Stochastic projection of stock for 25 years with fixed fishing mortality and recruitment These scenarios are defined by the target quantities one’s trying to achieve. In FLasher there are the following target quantities: - srp - ssb_end - biomass_end - ssb_spawn - biomass_spawn - ssb_flash - biomass_flash - inmb_end - indb - catch - landings - discards - f - fbar - revenue - effort [IAGO NEED YOUR HELP HERE] When projecting the stock under the conditions defined by the scenario one can mix several quantities. For example it may be interesting to project an initial situation of growing the stock followed by a higher exploitation to evaluate how catches would behave. trg00 &lt;- fwdControl(year = inipy:endpy, quant = c(rep(&quot;ssb_end&quot;, 15), rep(&quot;f&quot;, 10)), value = c(rep(2000000, 15), rep(0.3, 10))) stk06 &lt;- fwd(stk00, control=trg00, sr=sr00, residuals=rec00) Figure 15.7: Stochastic projection of stock for 25 years with fixed SSB for 15 years followed by fixed fishing mortality for 10 years and constant recruitment 15.2.1 Relative scenarios Another scenario that is very useful when advising decision makers is to have objectives which are relative to previous preformances. For example one could increase spwanwing stock biomass by 10% each year. This is done buy using the argument relYear and setting value in relative terms, \\(1.1\\). fit00 &lt;- sca(ple4, ple4.indices, srmodel=~geomean()) sr00 &lt;- as(fit00, &quot;FLSR&quot;) stk00 &lt;- ple4 + simulate(fit00, nsim) # set projection projy &lt;- 5 endpy &lt;- maxy + projy inipy &lt;- maxy + 1 stk00 &lt;- fwdWindow(stk00, end = endpy) trg00 &lt;- fwdControl(year = inipy:endpy, quant = &quot;ssb_end&quot;, value = 1.1, relYear = inipy:endpy-1) # recruitment uncertainty res00 &lt;- residuals(sr00) rec00 &lt;- window(rec(stk00), inipy, endpy) rec00 &lt;- rlnorm(rec00, mean(res00), sqrt(var(res00))) # project stk07 &lt;- fwd(stk00, control=trg00, sr=sr00, residuals=rec00) Similar scenarios can be set for all quantities and any years to use as reference. The next example sets a scenario where \\(SSB\\) levels are set in relation to the most recent estimate out of the assessmeent. trg00 &lt;- fwdControl(year = inipy:endpy, quant = &quot;ssb_end&quot;, value = 1.1, relYear = maxy) stk08 &lt;- fwd(stk00, control=trg00, sr=sr00, residuals=rec00) Figure 15.8: Stochastic projection of stock for 25 years with fixed SSB for 15 years followed by fixed fishing mortality for 10 years and constant recruitment 15.2.2 Limits An important element when projecting the stock forward is to keep the performance of the fishery within some boundaries. A common one requested by the industry is to keep catches within some stability. fwd can include those constraints using the min and max arguments. The next example sets the minimum future catches to half of mean historical catches. trg00 &lt;- fwdControl(year = inipy:endpy, quant = rep(c(&quot;ssb_end&quot;, &quot;catch&quot;), projy), value = rep(c(1500000, NA), projy), min=rep(c(NA, 0.5*mean(catch(stk00), na.rm=TRUE)), projy)) stk09 &lt;- fwd(stk00, control=trg00, sr=sr00, residuals=rec00) 15.2.3 Complex scenario "],["sections-to-be-added.html", "Chapter 16 Sections to be added!?", " Chapter 16 Sections to be added!? Confidence interval coverage and MCMC setup or more general Assessing the coverage of confidence intervals Probabilistic assessment (RH code) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
